# 데이터 로드 및 구조 점검

데이터 분석에 있어 가장 먼저 하는 작업이 데이터를 수집하는 것이다. 데이터 수집 방법에는 여러 가지가 있을 수 있고 대부분 로컬 파일이나 데이터베이스 또는 다른 시스템의 산출물에서 확보한다. 또한 데이터 원천에서 수집되는 데이터 형태는 시스템이나 상황에 따라 다양할 수 있지만 일반적으로 CSV나 Excel 형태로 처리하게 된다. 물론 API나 데이터베이스에 직접 Query해서 취합할 수도 있다.

```{python}
#| echo: false
#| output: false

import sys
sys.executable
```

## 데이터 로드

가장 일반적인 자료 형태인 CSV 파일과 Excel 파일을 메모리에 적재하는 방법을 알아 본다. 아래는 예제로 사용할 [`palmerpenguins`](https://pypi.org/project/palmerpenguins/) 데이터셋이다.

```{python}
#| warning: false
import pandas as pd
from palmerpenguins import load_penguins

df = load_penguins()

df.head()

df.to_csv("penguins.csv", index=False) # CSV 파일로 저장
df.to_excel("./penguins.xlsx", index=False) # Excel 파일로 저장

# sqlite DB
import sqlite3
conn = sqlite3.connect("penguins.db")
result = df.to_sql("penguins", conn, if_exists="replace", index=False)
```

### CSV 파일 로드

```{python}
import pandas as pd

df_from_csv = pd.read_csv("penguins.csv")

df_from_csv.head()
```


### Excel 파일 로드

```{python}
import pandas as pd

df_from_excel = pd.read_excel("penguins.xlsx")

df_from_excel.head()
```

### SQLite 데이터베이스 로드

```{python}
import sqlite3

conn = sqlite3.connect("penguins.db")

query = "SELECT * FROM penguins"
df_from_sql = pd.read_sql(query, conn)

df_from_sql.head()
```

## 데이터 구조 및 타입 확인

수집된 데이터에서 가장 먼저 확인하는 것은 데이터의 크기와 컬럼에 대한 정보이다.

### 데이터 크기와 기본 정보

```{python}
import pandas as pd

df = pd.read_csv("penguins.csv")

df.shape
```

```{python}
df.info()
```





