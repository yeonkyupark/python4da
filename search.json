[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "파이썬 데이터 분석",
    "section": "",
    "text": "들어가기\n기계학습으로 데이터를 분석한다. 분석 도구는 파이썬으로 데이터 전처리, 통계 분석, 기계학습 모델링 및 평가까지 코드 중심으로 구성된다. 실무에 필요한 내용을 정리하여 이론적, 학문적으로 미흡한 부분이 있다. 부족한 부분은 향후 성능 개선과 향상된 알고리즘 등으로 수정키로 한다.\n파이썬 버전은 3.12 기반으로 numpy, pandas, sklearn, scipy, stats와 같은 기본적인 라이브러리를 사용한다.\n전체 목차는 다음과 같다.",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-데이터-전처리-분포-이해",
    "href": "index.html#부.-데이터-전처리-분포-이해",
    "title": "파이썬 데이터 분석",
    "section": "1부. 데이터 전처리 & 분포 이해",
    "text": "1부. 데이터 전처리 & 분포 이해\n\n1.1 데이터 로드 및 구조 점검\n\n데이터 로드 (CSV, Excel, SQL)\n데이터 구조 및 타입 확인\n\n\n\n1.2 탐색적 데이터 분석(EDA)\n\n기술통계량 요약\n분포 시각화 (히스토그램, KDE, 박스플롯)\n상관관계 탐색\n\n\n\n1.3 데이터 분포 이해\n\n연속형·이산형 데이터 분포\n왜도와 첨도\n분포 해석을 통한 전처리 전략\n\n\n\n1.4 결측치 처리\n\n결측치 탐지 및 요약\n단순 대치 기법\n고급 대치 기법 (KNN, Iterative)\n\n\n\n1.5 이상치 탐지\n\n정규분포 기반 이상치\nIQR 기반 이상치\n밀도 기반 이상치 (LOF, DBSCAN)\n트리 기반 이상치 (IsolationForest)\n\n\n\n1.6 스케일링\n\n정규화 (Min-Max)\n표준화 (Standard, Robust, MaxAbs)\n\n\n\n1.7 데이터 분포 변환\n\n로그 변환\nBox-Cox 변환\nYeo-Johnson 변환\n분위수 변환\n변환 전·후 분포 비교\n\n\n\n1.8 범주형 데이터 처리\n\n명목형 인코딩\n순서형 인코딩\n\n\n\n1.9 연속형 데이터 범주화\n\n구간 분할 (cut, qcut, KBins)\n\n\n\n1.10 불균형 데이터 처리\n\n오버샘플링\n언더샘플링\n\n\n\n1.11 피처 엔지니어링\n\n다항 특성 생성\n집계 및 롤링 특성",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "href": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "title": "파이썬 데이터 분석",
    "section": "2부. 통계 기반 데이터 분석 & 가설검정",
    "text": "2부. 통계 기반 데이터 분석 & 가설검정\n\n2.1 확률분포와 표본\n\n연속형 확률분포\n이산형 확률분포\n표본 분포 개념\n\n\n\n2.2 정규성 검정\n\nShapiro-Wilk 검정\nKolmogorov-Smirnov 검정\nAnderson-Darling 검정\nQ-Q plot 해석\n\n\n\n2.3 등분산성 검정\n\nLevene 검정\nBartlett 검정\nFligner-Killeen 검정\n\n\n\n2.4 적합성 검정 & 독립성 검정\n\n카이제곱 적합성 검정\n분포 적합성 검정\n카이제곱 독립성 검정\nF 검정 (분산 비교)\n\n\n\n2.5 평균 비교 검정\n\n단일 표본 t-검정\n독립 표본 t-검정\n대응 표본 t-검정\n\n\n\n2.6 분산분석\n\n일원 분산 분석 (One-way ANOVA)\n이원 분산 분석 (Two-way ANOVA)\n사후 검정\n\n\n\n2.7 비모수 검정\n\nMann-Whitney U 검정\nWilcoxon 순위합 검정\nKruskal-Wallis 검정\nFriedman 검정\n\n\n\n2.8 상관 분석\n\nPearson 상관 분석\nSpearman 순위 상관",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-머신러닝-모델링-평가",
    "href": "index.html#부.-머신러닝-모델링-평가",
    "title": "파이썬 데이터 분석",
    "section": "3부. 머신러닝 모델링 & 평가",
    "text": "3부. 머신러닝 모델링 & 평가\n\n3.1 데이터 분할 및 검증\n\n학습·검증·테스트 분할\n교차 검증 기법\n\n\n\n3.2 특성 선택\n\n필터 방법\n래퍼 방법\n임베디드 방법\n\n\n\n3.3 차원 축소\n\n선형 차원 축소 (PCA)\n비선형 차원 축소 (t-SNE, UMAP)\n\n\n\n3.4 회귀 모델\n\n선형 회귀\n정규화 회귀 (Ridge, Lasso, ElasticNet)\n\n\n\n3.5 분류 모델\n\n선형 분류 모델\n거리 기반 모델\n트리 및 앙상블 모델\n\n\n\n3.6 서포트 벡터 머신\n\nSVM 분류\n\n\n\n3.7 군집 분석\n\n분할 기반 군집\n밀도 기반 군집\n혼합 모델 군집\n\n\n\n3.8 모델 성능 평가\n\n분류 성능 평가\n회귀 성능 평가\n\n\n\n3.9 파이프라인 & 자동화\n\n파이프라인 구성\n하이퍼파라미터 최적화\n\n\n\n3.10 모델 해석\n\n특성 중요도\nSHAP 기반 모델 해석",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html",
    "href": "part1/01. 데이터 로드 및 구조 점검.html",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1 데이터 로드\n데이터 분석에 있어 가장 먼저 하는 작업이 데이터를 수집하는 것이다. 데이터 수집 방법에는 여러 가지가 있을 수 있고 대부분 로컬 파일이나 데이터베이스 또는 다른 시스템의 산출물에서 확보한다. 또한 데이터 원천에서 수집되는 데이터 형태는 시스템이나 상황에 따라 다양할 수 있지만 일반적으로 CSV나 Excel 형태로 처리하게 된다. 물론 API나 데이터베이스에 직접 Query해서 취합할 수도 있다.\n가장 일반적인 자료 형태인 CSV 파일과 Excel 파일을 메모리에 적재하는 방법을 알아 본다. 아래는 예제로 사용할 palmerpenguins 데이터셋이다.\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\ndf = load_penguins()\n\ndf.head()\n\ndf.to_csv(\"penguins.csv\", index=False) # CSV 파일로 저장\ndf.to_excel(\"./penguins.xlsx\", index=False) # Excel 파일로 저장\n\n# sqlite DB\nimport sqlite3\nconn = sqlite3.connect(\"penguins.db\")\nresult = df.to_sql(\"penguins\", conn, if_exists=\"replace\", index=False)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1.1 CSV 파일 로드\n\nimport pandas as pd\n\ndf_from_csv = pd.read_csv(\"penguins.csv\")\n\ndf_from_csv.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.2 Excel 파일 로드\n\nimport pandas as pd\n\ndf_from_excel = pd.read_excel(\"penguins.xlsx\")\n\ndf_from_excel.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.3 SQLite 데이터베이스 로드\n\nimport sqlite3\n\nconn = sqlite3.connect(\"penguins.db\")\n\nquery = \"SELECT * FROM penguins\"\ndf_from_sql = pd.read_sql(query, conn)\n\ndf_from_sql.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "1.2 데이터 구조 및 타입 확인",
    "text": "1.2 데이터 구조 및 타입 확인\n수집된 데이터에서 가장 먼저 확인하는 것은 데이터의 크기와 컬럼에 대한 정보이다.\n\n1.2.1 데이터 크기와 기본 정보\n\nimport pandas as pd\n\ndf = pd.read_csv(\"penguins.csv\")\n\ndf.shape\n\n(344, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "",
    "text": "2.1 라이브러리 로드 & 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# seaborn 내장 penguins 데이터셋 로드\ndf = sns.load_dataset(\"penguins\")\n\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.2 데이터 구조 확인",
    "text": "2.2 데이터 구조 확인\n\ndf.shape\n\n(344, 7)\n\n\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \ndtypes: float64(4), str(3)\nmemory usage: 18.9 KB\n\n\n확인 포인트\n\n행(row), 열(column) 개수\n변수 타입 (수치형 / 범주형)\n결측치 여부",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.3 기초 통계량 확인",
    "text": "2.3 기초 통계량 확인\n\n2.3.1 수치형 변수 요약\n\ndf.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n\n\n2.3.2 범주형 변수 요약\n\ndf.describe(include=\"object\")\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\n\n\n\n\ncount\n344\n344\n333\n\n\nunique\n3\n3\n2\n\n\ntop\nAdelie\nBiscoe\nMale\n\n\nfreq\n152\n168\n168",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.4 결측치 탐색",
    "text": "2.4 결측치 탐색\n\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\n\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.5 범주형 변수 탐색",
    "text": "2.5 범주형 변수 탐색\n\n2.5.1 종(species) 분포\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\nsns.countplot(data=df, x=\"species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.5.2 성별(sex) 분포\n\nsns.countplot(data=df, x=\"sex\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.6 수치형 변수 분포 확인",
    "text": "2.6 수치형 변수 분포 확인\n\n2.6.1 히스토그램\n\ndf.hist(bins=20, figsize=(10, 8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.6.2 박스플롯 (이상치 탐색)\n\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=df, x=\"species\", y=\"body_mass_g\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.7 변수 간 관계 탐색",
    "text": "2.7 변수 간 관계 탐색\n\n2.7.1 두 수치형 변수 관계\n\nsns.scatterplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.7.2 페어플롯\n\nsns.pairplot(\n    df,\n    hue=\"species\",\n    diag_kind=\"hist\"\n)\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.8 그룹별 통계 확인",
    "text": "2.8 그룹별 통계 확인\n\ndf.groupby(\"species\")[[\"bill_length_mm\", \"body_mass_g\"]].mean()\n\n\n\n\n\n\n\n\nbill_length_mm\nbody_mass_g\n\n\nspecies\n\n\n\n\n\n\nAdelie\n38.791391\n3700.662252\n\n\nChinstrap\n48.833824\n3733.088235\n\n\nGentoo\n47.504878\n5076.016260",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#간단한-eda-결론-예시",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#간단한-eda-결론-예시",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.9 간단한 EDA 결론 예시",
    "text": "2.9 간단한 EDA 결론 예시\n\n종(species)에 따라 부리 길이와 몸무게 분포가 뚜렷하게 구분된다.\nAdelie 종은 상대적으로 몸무게와 부리 길이가 작다.\n일부 변수에 결측치가 존재하므로 분석 전 처리 전략이 필요하다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#eda-기본-흐름-요약",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#eda-기본-흐름-요약",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.10 EDA 기본 흐름 요약",
    "text": "2.10 EDA 기본 흐름 요약\n\n데이터 구조 확인 → info(), shape\n기초 통계 → describe()\n결측치 → isna()\n분포 → 히스토그램, 박스플롯\n관계 → 산점도, pairplot\n그룹 비교 → groupby()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html",
    "href": "part1/03. 결측치 처리.html",
    "title": "3  결측치 처리",
    "section": "",
    "text": "3.1 데이터 로드 & 결측치 현황 재확인\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")\n\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "href": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "title": "3  결측치 처리",
    "section": "3.2 결측치 비율 확인",
    "text": "3.2 결측치 비율 확인\n\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "href": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "title": "3  결측치 처리",
    "section": "3.3 결측치 제거 (행 삭제)",
    "text": "3.3 결측치 제거 (행 삭제)\n\n3.3.1 전체 결측치가 있는 행 제거\n\ndf_drop_all = df.dropna()\ndf_drop_all.shape\n\n(333, 7)\n\n\n\n\n3.3.2 결측치가 3개 이상인 행 제거\n\ndf[\"na_count\"] = df.isna().sum(axis=1)\ndf_row_filtered = df[df[\"na_count\"] &lt; 3].drop(columns=\"na_count\")\ndf_row_filtered.shape\n\n(342, 7)\n\n\n\n# 결측치가 전체 컬럼에서 2개까지만 허용()\ndf_thresh = df.dropna(thresh=len(df.columns)-2)\n\n\n\n3.3.3 특정 컬럼 기준 제거\n\ndf_drop_sex = df.dropna(subset=[\"sex\"])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.4 수치형 변수 결측치 대체",
    "text": "3.4 수치형 변수 결측치 대체\n\n3.4.1 평균(mean) 대체\n특정 컬럼에 대한 평균 대체\n\ndf_mean = df.copy()\ndf_mean[\"bill_length_mm\"] = df_mean[\"bill_length_mm\"].fillna(\n    df_mean[\"bill_length_mm\"].mean()\n)\n\n다중 컬럼에 대한 처리 - 1\n\ndf_cols = df.copy()\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\n\ndf_cols[num_cols] = df_cols[num_cols].fillna(df[num_cols].mean())\n\n다중 컬럼에 대한 처리 - 2\n\nfor col in num_cols:\n    mean_value = df[col].mean()\n    df[col] = df[col].fillna(mean_value)\n\n범주별 다중 컬럼 처리\n\ndf_group = df.copy()\n\ndf_group[num_cols] = df_group[num_cols].fillna(\n    df_group.groupby(\"species\")[num_cols].transform(\"mean\")\n)\n\n\n\n3.4.2 중앙값(median) 대체\n\ndf_median = df.copy()\ndf_median[\"bill_depth_mm\"] = df_median[\"bill_depth_mm\"].fillna(\n    df_median[\"bill_depth_mm\"].median()\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.5 범주형 변수 결측치 대체",
    "text": "3.5 범주형 변수 결측치 대체\n\n3.5.1 최빈값(mode) 대체\n\ndf_mode = df.copy()\nmode_sex = df_mode[\"sex\"].mode()[0]\ndf_mode[\"sex\"] = df_mode[\"sex\"].fillna(mode_sex)\n\n\n\n3.5.2 명시적 범주 추가\n\ndf_category = df.copy()\ndf_category[\"sex\"] = df_category[\"sex\"].fillna(\"Unknown\")\n\n\ndf_mode = df.copy()\n\ncat_cols = [\"sex\", \"island\"]\n\nfor col in cat_cols:\n    mode_value = df_mode[col].mode()[0]\n    df_mode[col] = df_mode[col].fillna(mode_value)\n\n\ndf_mode = df.copy()\n\ndf_mode[\"sex\"] = df_mode[\"sex\"].fillna(\n    df_mode.groupby(\"species\")[\"sex\"].transform(\n        lambda x: x.mode()[0]\n    )\n)\n\n\ndf_mode = df.copy()\n\ncat_cols = [\"sex\", \"island\"]\n\nfor col in cat_cols:\n    df_mode[col] = df_mode[col].fillna(\n        df_mode.groupby(\"species\")[col].transform(\n            lambda x: x.mode()[0]\n        )\n    )",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "href": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "title": "3  결측치 처리",
    "section": "3.6 결측치 처리 후 검증",
    "text": "3.6 결측치 처리 후 검증\n\ndf_group.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        0\nbill_depth_mm         0\nflipper_length_mm     0\nbody_mass_g           0\nsex                  11\nna_count              0\ndtype: int64\n\n\n\ndf_group.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     344 non-null    float64\n 3   bill_depth_mm      344 non-null    float64\n 4   flipper_length_mm  344 non-null    float64\n 5   body_mass_g        344 non-null    float64\n 6   sex                333 non-null    str    \n 7   na_count           344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-처리-권장-순서",
    "href": "part1/03. 결측치 처리.html#결측치-처리-권장-순서",
    "title": "3  결측치 처리",
    "section": "3.7 결측치 처리 권장 순서",
    "text": "3.7 결측치 처리 권장 순서\n\n결측치 현황 파악\n제거 가능한 행/열 판단\n수치형 → 그룹 기반 대체\n범주형 → mode 또는 Unknown\n최종 검증",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html",
    "href": "part1/04. 이상치 탐지.html",
    "title": "4  이상치 탐지",
    "section": "",
    "text": "4.1 분포 기반 이상치 탐지\n분석 대상은 수치형 변수만 사용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "",
    "text": "4.1.1 박스플롯으로 이상치 확인\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\ndf_num_scaled = pd.DataFrame(scale.fit_transform(df_num[num_cols]),\n                              columns=df_num.columns)\n\ndf_num_scaled.boxplot(figsize=(10, 5))\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.2 IQR 기반 이상치 탐지",
    "text": "4.2 IQR 기반 이상치 탐지\n\n4.2.1 IQR 계산\n\nQ1 = df_num.quantile(0.25)\nQ3 = df_num.quantile(0.75)\nIQR = Q3 - Q1\n\n\n\n4.2.2 이상치 조건 정의\n\noutlier_condition = (df_num &lt; (Q1 - 1.5 * IQR)) | (df_num &gt; (Q3 + 1.5 * IQR))\n\n\n\n4.2.3 이상치가 있는 행 확인\n\noutlier_rows = outlier_condition.any(axis=1)\ndf[outlier_rows]\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n\n\n\n\n\n컬럼 중 하나라도 이상치면 해당 행 전체를 이상치로 판단\n\n\n4.2.4 이상치 제거\n\ndf_iqr_clean = df[~outlier_rows]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.3 Z-score 기반 이상치 탐지",
    "text": "4.3 Z-score 기반 이상치 탐지\n\nfrom scipy.stats import zscore\n\n\nz_scores = df_num.apply(zscore)\n\n\n4.3.1 임계값 기준 이상치 탐지 (|z| &gt; 3)\n\noutlier_z = (np.abs(z_scores) &gt; 3).any(axis=1)\ndf_z_clean = df[~outlier_z]\n\n계산이 간단한 장점이 있으나 분포 왜곡에 취약한 단점 존재",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.4 범주별 이상치 탐지",
    "text": "4.4 범주별 이상치 탐지\n\n4.4.1 species별 IQR 이상치 탐지\n\ndef iqr_outlier_by_group(group):\n    Q1 = group[num_cols].quantile(0.25)\n    Q3 = group[num_cols].quantile(0.75)\n    IQR = Q3 - Q1\n\n    condition = (group[num_cols] &lt; (Q1 - 1.5 * IQR)) | \\\n                (group[num_cols] &gt; (Q3 + 1.5 * IQR))\n    return group[~condition.any(axis=1)]\n\n\ndf_group_iqr_clean = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(iqr_outlier_by_group)\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "href": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "title": "4  이상치 탐지",
    "section": "4.5 밀도 기반 이상치 탐지 (LOF)",
    "text": "4.5 밀도 기반 이상치 탐지 (LOF)\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n\ndf_lof = df_num.dropna()\n\nlof = LocalOutlierFactor(n_neighbors=20)\noutlier_lof = lof.fit_predict(df_lof)\n\n\ndf_lof_clean = df.loc[df_lof.index][outlier_lof == 1]\n\n주변 데이터와 너무 다른 밀도를 가지면 이상치로 판단하는 방식으로 비선형 데이터에 강함",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "href": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "title": "4  이상치 탐지",
    "section": "4.6 트리 기반 이상치 탐지 (Isolation Forest)",
    "text": "4.6 트리 기반 이상치 탐지 (Isolation Forest)\n\nfrom sklearn.ensemble import IsolationForest\n\n\niso = IsolationForest(contamination=0.05, random_state=42)\noutlier_if = iso.fit_predict(df_lof)\n\n\ndf_if_clean = df.loc[df_lof.index][outlier_if == 1]\n\n이상치는 트리 분류 기준으로 쉽게 분리가 된다는 가정하에 이상치를 탐지하는 방식으로 고차원 데이터에 강함하고 실무에서 자주 사용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#단계별-선택-가이드",
    "href": "part1/04. 이상치 탐지.html#단계별-선택-가이드",
    "title": "4  이상치 탐지",
    "section": "4.7 단계별 선택 가이드",
    "text": "4.7 단계별 선택 가이드\n\n\n\n단계\n방법\n언제 쓰나\n\n\n\n\n1\n시각화\n이해, 설명\n\n\n2\nIQR\n기본, 안전\n\n\n3\nZ-score\n정규분포 가정\n\n\n4\n그룹별 IQR\n가장 실무적\n\n\n5\nLOF\n비선형, 밀도\n\n\n6\nIsolationForest\n대규모, 고급",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html",
    "href": "part1/05. 스케일링.html",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1 스케일링 이유\n예제 데이터셋 적재",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#스케일링-이유",
    "href": "part1/05. 스케일링.html#스케일링-이유",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1.1 스케일 차이 확인\n\ndf_num.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n변수별 측정 단위가 다름(상호 비교가 어려움)\n- 거리 기반 모델(KNN, SVM, K-means)에서는 큰 값이 모델을 지배",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "href": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "title": "5  스케일링",
    "section": "5.2 Min-Max 스케일링 (정규화)",
    "text": "5.2 Min-Max 스케일링 (정규화)\n모든 값을 0~1 범위로 변환\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler_minmax = MinMaxScaler()\ndf_minmax = scaler_minmax.fit_transform(df_num)\n\n\ndf_minmax = pd.DataFrame(df_minmax, columns=num_cols)\nprint(df_minmax.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.429888       0.482282           0.490088     0.417154\nstd          0.198530       0.235094           0.238334     0.222765\nmin          0.000000       0.000000           0.000000     0.000000\n25%          0.259091       0.297619           0.305085     0.236111\n50%          0.449091       0.500000           0.423729     0.375000\n75%          0.596364       0.666667           0.694915     0.569444\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n변수 범위가 고정(0~1) 되며 이상치에 매우 민감한 특징\n참고. 아래와 같이 수식을 통해 직접 구현\n\\[\nx' = \\frac{x-x_{min}}{x_{max} - x_{min}}\n\\]\n\ndf_minmax_manual = df_num.copy()\n\nfor col in num_cols:\n    col_min = df_minmax_manual[col].min()\n    col_max = df_minmax_manual[col].max()\n\n    df_minmax_manual[col] = (df_minmax_manual[col] - col_min) \\\n                            / (col_max - col_min)\n\n(a, b) 구간으로 일반화한 수식\n\\[\nx' = a + \\frac{x-x_{min}}{x_{max} - x_{min}}\\times(b-a)\n\\]\n\ndef minmax_scale_ab(series, a, b):\n    x_min = series.min()\n    x_max = series.max()\n\n    return a + (series - x_min) * (b - a) / (x_max - x_min)\n\n\nscaler_genernal = MinMaxScaler(feature_range=(-1, 1))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#표준화-standard-scaling",
    "href": "part1/05. 스케일링.html#표준화-standard-scaling",
    "title": "5  스케일링",
    "section": "5.3 표준화 (Standard Scaling)",
    "text": "5.3 표준화 (Standard Scaling)\n변수 변위를 평균 0, 표준편차 1로 변환\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler_std = StandardScaler()\ndf_std = scaler_std.fit_transform(df_num)\ndf_std = pd.DataFrame(df_std, columns=num_cols)\n\nprint(df_std.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm   body_mass_g\ncount    3.420000e+02   3.420000e+02       3.420000e+02  3.420000e+02\nmean     8.310441e-17  -1.412775e-15      -8.310441e-16  4.155221e-17\nstd      1.001465e+00   1.001465e+00       1.001465e+00  1.001465e+00\nmin     -2.168526e+00  -2.054446e+00      -2.059320e+00 -1.875362e+00\n25%     -8.615697e-01  -7.866355e-01      -7.773731e-01 -8.138982e-01\n50%      9.686524e-02   7.547549e-02      -2.788381e-01 -1.895079e-01\n75%      8.397670e-01   7.854492e-01       8.606705e-01  6.846384e-01\nmax      2.875868e+00   2.205397e+00       2.142618e+00  2.620248e+00\n\n\n가장 널리 사용되며 변수가 정규분포 가정(완전하지 않아도 OK)하에 수행\n참고. 아래와 같이 수식을 통해 직접 구현\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\ndf_std_manual = df_num.copy()\n\nfor col in num_cols:\n    mean = df_std_manual[col].mean()\n    std = df_std_manual[col].std()\n\n    df_std_manual[col] = (df_std_manual[col] - mean) \\\n                         / std",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#robust-scaling-이상치-대응",
    "href": "part1/05. 스케일링.html#robust-scaling-이상치-대응",
    "title": "5  스케일링",
    "section": "5.4 Robust Scaling (이상치 대응)",
    "text": "5.4 Robust Scaling (이상치 대응)\n중앙값과 IQR 기준으로 변환을 수행하여 이상치에 대응\n\nfrom sklearn.preprocessing import RobustScaler\n\n\nscaler_robust = RobustScaler()\ndf_robust = scaler_robust.fit_transform(df_num)\ndf_robust = pd.DataFrame(df_robust, columns=num_cols)\n\nprint(df_robust.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount    3.420000e+02     342.000000         342.000000   342.000000\nmean    -5.693479e-02      -0.048010           0.170226     0.126462\nstd      5.886344e-01       0.637030           0.611379     0.668295\nmin     -1.331536e+00      -1.354839          -1.086957    -1.125000\n25%     -5.633423e-01      -0.548387          -0.304348    -0.416667\n50%     -3.833739e-16       0.000000           0.000000     0.000000\n75%      4.366577e-01       0.451613           0.695652     0.583333\nmax      1.633423e+00       1.354839           1.478261     1.875000\n\n\n이상치 영향 최소화",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#maxabs-scaling",
    "href": "part1/05. 스케일링.html#maxabs-scaling",
    "title": "5  스케일링",
    "section": "5.5 MaxAbs Scaling",
    "text": "5.5 MaxAbs Scaling\n최대 절댓값으로 나눔\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\n\nscaler_maxabs = MaxAbsScaler()\ndf_maxabs = scaler_maxabs.fit_transform(df_num)\ndf_maxabs = pd.DataFrame(df_maxabs, columns=num_cols)\n\nprint(df_maxabs.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.736945       0.797729           0.869763     0.666945\nstd          0.091604       0.091851           0.060873     0.127294\nmin          0.538591       0.609302           0.744589     0.428571\n25%          0.658138       0.725581           0.822511     0.563492\n50%          0.745805       0.804651           0.852814     0.642857\n75%          0.813758       0.869767           0.922078     0.753968\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n희소 행렬에 유리하며 값 부호 유지하는 특징",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#범주별-스케일링",
    "href": "part1/05. 스케일링.html#범주별-스케일링",
    "title": "5  스케일링",
    "section": "5.6 범주별 스케일링",
    "text": "5.6 범주별 스케일링\n\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef scale_by_species(group):\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(group[num_cols])\n    return pd.DataFrame(scaled, columns=num_cols, index=group.index)\n\n\ndf_scaled_group = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(scale_by_species)\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#파이프라인-적용",
    "href": "part1/05. 스케일링.html#파이프라인-적용",
    "title": "5  스케일링",
    "section": "5.7 파이프라인 적용",
    "text": "5.7 파이프라인 적용\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression())\n])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#스케일링-방법-선택-가이드",
    "href": "part1/05. 스케일링.html#스케일링-방법-선택-가이드",
    "title": "5  스케일링",
    "section": "5.8 스케일링 방법 선택 가이드",
    "text": "5.8 스케일링 방법 선택 가이드\n\n\n\n방법\n특징\n추천 상황\n\n\n\n\nMin-Max\n0~1\n신경망, 시각화\n\n\nStandard\n평균 0\n대부분의 모델\n\n\nRobust\n이상치 강함\n실무 기본\n\n\nMaxAbs\n부호 유지\n희소 데이터\n\n\n그룹별\n분포 반영\n도메인 지식 있을 때",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html",
    "href": "part1/06. 데이터 분포 변환.html",
    "title": "6  데이터 분포 변환",
    "section": "",
    "text": "6.1 분포 변환 이유\n예제 데이터셋 적재\ndf_num.skew()\n\nbill_length_mm       0.053118\nbill_depth_mm       -0.143465\nflipper_length_mm    0.345682\nbody_mass_g          0.470329\ndtype: float64\n대부분 데이터는 양의 왜도(skewness) 형태를 보이며 평균·분산 기반 모델(선형 회귀, PCA 등)에 불리\n분포를 대칭에 가깝게 변환함으로써 극단값 영향 완화, 모델 가정 충족이 목표",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "href": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.2 로그 변환 (Log Transform)",
    "text": "6.2 로그 변환 (Log Transform)\n오른쪽 꼬리가 긴 분포(양의 왜도, Positive)에 효과적\n\n6.2.1 수식\n\\[\nx' = \\log(x)\n\\]\n0 이하 값 변환 불가로 보정 필요\n\n\n6.2.2 코드\n\ndf_log = df_num.copy()\n\nfor col in num_cols:\n    df_log[col] = np.log(df_log[col])\n\nprint(df_log)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          3.666122       2.928524           5.198497     8.229511\n1          3.676301       2.856470           5.225747     8.242756\n2          3.696351       2.890372           5.273000     8.086410\n3               NaN            NaN                NaN          NaN\n4          3.602777       2.960105           5.262690     8.146130\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        3.845883       2.660260           5.370638     8.486734\n341        3.919991       2.753661           5.402677     8.656955\n342        3.811097       2.694627           5.356586     8.556414\n343        3.910021       2.778819           5.361292     8.594154\n\n[344 rows x 4 columns]\n\n\n\n\n6.2.3 0 포함 변수 안전 처리\n\ndf_log_safe = np.log1p(df_num)\n\nprint(df_log_safe)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          3.691376       2.980619           5.204007     8.229778\n1          3.701302       2.912351           5.231109     8.243019\n2          3.720862       2.944439           5.278115     8.086718\n3               NaN            NaN                NaN          NaN\n4          3.629660       3.010621           5.267858     8.146419\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        3.867026       2.727853           5.375278     8.486940\n341        3.939638       2.815409           5.407172     8.657129\n342        3.832980       2.760010           5.361292     8.556606\n343        3.929863       2.839078           5.365976     8.594339\n\n[344 rows x 4 columns]\n\n\n\n\n6.2.4 왜도 변화 확인\n\ndf_log_safe.skew()\n\nbill_length_mm      -0.143463\nbill_depth_mm       -0.315791\nflipper_length_mm    0.251965\nbody_mass_g          0.170721\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "href": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.3 제곱근 변환 (Square Root Transform)",
    "text": "6.3 제곱근 변환 (Square Root Transform)\n로그보다 완만한 변환\n\n6.3.1 수식\n\\[\nx' = \\sqrt{x}\n\\]\n\n\n6.3.2 코드\n\ndf_sqrt = np.sqrt(df_num)\n\nprint(df_sqrt)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          6.252999       4.324350          13.453624    61.237244\n1          6.284903       4.171331          13.638182    61.644140\n2          6.348228       4.242641          13.964240    57.008771\n3               NaN            NaN                NaN          NaN\n4          6.058052       4.393177          13.892444    58.736701\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        6.841053       3.781534          14.662878    69.641941\n341        7.099296       3.962323          14.899664    75.828754\n342        6.723095       3.847077          14.560220    72.111026\n343        7.063993       4.012481          14.594520    73.484692\n\n[344 rows x 4 columns]\n\n\n\n\n6.3.3 제곱근 변환과 비교\n제곱근이 적합한 경우\n\n왜도가 심하지 않음\n데이터 범위가 크지 않음\n분포를 살짝만 정리하고 싶을 때\n카운트 데이터 (포아송 계열)\n\n로그가 적합한 경우\n\n왜도가 매우 큼\n극단값(outlier)이 많음\n선형 회귀, PCA, 거리 기반 모델\n“비율 변화”가 중요한 문제",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#boxcox-변환",
    "href": "part1/06. 데이터 분포 변환.html#boxcox-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.4 Box–Cox 변환",
    "text": "6.4 Box–Cox 변환\n로그의 일반화 버전으로 λ(람다)를 자동 추정 (단, 양수 전용)\n\n6.4.1 수식\n\\[\nx' =\n\\begin{cases}\n\\frac{x^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\n\\log(x), & \\lambda = 0\n\\end{cases}\n\\]\n\n\n6.4.2 코드\n\nfrom scipy.stats import boxcox\n\n\ndf_boxcox = df_num.copy()\nlambdas = {}\n\nfor col in num_cols:\n    data = df_boxcox[col].dropna()\n    transformed, lam = boxcox(data)\n    df_boxcox.loc[data.index, col] = transformed\n    lambdas[col] = lam\n\n\nlambdas\n\n{'bill_length_mm': np.float64(0.6201739671602454),\n 'bill_depth_mm': np.float64(1.4747606585324315),\n 'flipper_length_mm': np.float64(-2.0632096556190813),\n 'body_mass_g': np.float64(-0.4656520321320674)}",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#yeojohnson-변환",
    "href": "part1/06. 데이터 분포 변환.html#yeojohnson-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.5 Yeo–Johnson 변환",
    "text": "6.5 Yeo–Johnson 변환\nBox–Cox의 확장 방식으로 0, 음수에 대해서도 변환 가능\n\n6.5.1 sklearn 사용\n\nfrom sklearn.preprocessing import PowerTransformer\n\n\npt = PowerTransformer(method=\"yeo-johnson\")\n\ndf_yeojohnson = pd.DataFrame(\n    pt.fit_transform(df_num),\n    columns=num_cols\n)\n\n0, 음수 허용를 허용하며 결측치 제외 후 자동 처리",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "href": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.6 분위수 변환 (Quantile Transform)",
    "text": "6.6 분위수 변환 (Quantile Transform)\n분포 모양 자체를 바꿈\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n\nqt = QuantileTransformer(\n    output_distribution=\"normal\",\n    random_state=42\n)\n\ndf_quantile = pd.DataFrame(\n    qt.fit_transform(df_num),\n    columns=num_cols\n)\n\nprint(df_quantile)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0         -0.704794       0.690633          -1.663512    -0.366106\n1         -0.649469       0.033085          -1.127632    -0.269985\n2         -0.506872       0.288983          -0.176300    -1.347428\n3               NaN            NaN                NaN          NaN\n4         -1.237926       1.107167          -0.335021    -0.890537\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        0.449514      -1.286551           0.796647     0.737791\n341        1.120759      -0.627072           1.413753     1.848596\n342        0.091477      -0.963539           0.596232     1.042077\n343        0.965467      -0.465743           0.653988     1.245827\n\n[344 rows x 4 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "href": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "title": "6  데이터 분포 변환",
    "section": "6.7 변환 전·후 왜도 비교",
    "text": "6.7 변환 전·후 왜도 비교\n\nskew_compare = pd.DataFrame({\n    \"original\": df_num.skew(),\n    \"log\": df_log_safe.skew(),\n    \"yeo-johnson\": df_yeojohnson.skew()\n})\n\nskew_compare\n\n\n\n\n\n\n\n\noriginal\nlog\nyeo-johnson\n\n\n\n\nbill_length_mm\n0.053118\n-0.143463\n-0.023577\n\n\nbill_depth_mm\n-0.143465\n-0.315791\n-0.052609\n\n\nflipper_length_mm\n0.345682\n0.251965\n0.050271\n\n\nbody_mass_g\n0.470329\n0.170721\n0.026153",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#선택-가이드-교안-요약",
    "href": "part1/06. 데이터 분포 변환.html#선택-가이드-교안-요약",
    "title": "6  데이터 분포 변환",
    "section": "6.8 선택 가이드 (교안 요약)",
    "text": "6.8 선택 가이드 (교안 요약)\n\n\n\n방법\n특징\n추천\n\n\n\n\nLog\n단순\n기초\n\n\nSqrt\n완만\n보조\n\n\nBox–Cox\n최적 λ\n양수 데이터\n\n\nYeo–Johnson\n범용\n실무 기본\n\n\nQuantile\n분포 강제\n비정상 분포",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#스케일링과의-관계",
    "href": "part1/06. 데이터 분포 변환.html#스케일링과의-관계",
    "title": "6  데이터 분포 변환",
    "section": "6.9 스케일링과의 관계",
    "text": "6.9 스케일링과의 관계\n\n분포 변환 → 모양 변경\n스케일링 → 크기 조정\n일반 순서 분포 변환 → 스케일링",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html",
    "href": "part1/07. 범주형 데이터 인코딩.html",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "7.1 Label Encoding\n예제 데이터셋 적재\n범주를 정수 값으로 매핑하는 방식으로 순서가 없는데 숫자로 바뀌므로 주의 필요",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "7.1.1 예제 코드\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_label = df.copy()\ndf_label[\"species_enc\"] = le.fit_transform(df_label[\"species\"])\n\nprint(df_label)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  species_enc  \n0         3750.0    Male            0  \n1         3800.0  Female            0  \n2         3250.0  Female            0  \n3            NaN     NaN            0  \n4         3450.0  Female            0  \n..           ...     ...          ...  \n339          NaN     NaN            2  \n340       4850.0  Female            2  \n341       5750.0    Male            2  \n342       5200.0  Female            2  \n343       5400.0    Male            2  \n\n[344 rows x 8 columns]\n\n\n\n\n7.1.2 사용 시점\n트리 기반 모델 (Decision Tree, RandomForest, XGBoost) 사용 시 순서 의미가 없어도 되는 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.2 One-Hot Encoding",
    "text": "7.2 One-Hot Encoding\n각 범주를 이진 컬럼으로 분리, 거리 기반, 선형 모델에서 가장 안전\n\n7.2.1 pandas 방식\n\ndf_ohe = pd.get_dummies(\n    df,\n    columns=[\"species\", \"island\"],\n    drop_first=False\n)\n\nprint(df_ohe)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  \\\n0              39.1           18.7              181.0       3750.0    Male   \n1              39.5           17.4              186.0       3800.0  Female   \n2              40.3           18.0              195.0       3250.0  Female   \n3               NaN            NaN                NaN          NaN     NaN   \n4              36.7           19.3              193.0       3450.0  Female   \n..              ...            ...                ...          ...     ...   \n339             NaN            NaN                NaN          NaN     NaN   \n340            46.8           14.3              215.0       4850.0  Female   \n341            50.4           15.7              222.0       5750.0    Male   \n342            45.2           14.8              212.0       5200.0  Female   \n343            49.9           16.1              213.0       5400.0    Male   \n\n     species_Adelie  species_Chinstrap  species_Gentoo  island_Biscoe  \\\n0              True              False           False          False   \n1              True              False           False          False   \n2              True              False           False          False   \n3              True              False           False          False   \n4              True              False           False          False   \n..              ...                ...             ...            ...   \n339           False              False            True           True   \n340           False              False            True           True   \n341           False              False            True           True   \n342           False              False            True           True   \n343           False              False            True           True   \n\n     island_Dream  island_Torgersen  \n0           False              True  \n1           False              True  \n2           False              True  \n3           False              True  \n4           False              True  \n..            ...               ...  \n339         False             False  \n340         False             False  \n341         False             False  \n342         False             False  \n343         False             False  \n\n[344 rows x 11 columns]\n\n\ndrop_first 옵션으로 첫번째 더미 변수 제거, 다중공선성 방지 (회귀 모델에서 중요)\n\ndf_ohe = pd.get_dummies(\n    df,\n    columns=[\"species\"],\n    drop_first=True\n)\n\nprint(df_ohe)\n\n        island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n0    Torgersen            39.1           18.7              181.0       3750.0   \n1    Torgersen            39.5           17.4              186.0       3800.0   \n2    Torgersen            40.3           18.0              195.0       3250.0   \n3    Torgersen             NaN            NaN                NaN          NaN   \n4    Torgersen            36.7           19.3              193.0       3450.0   \n..         ...             ...            ...                ...          ...   \n339     Biscoe             NaN            NaN                NaN          NaN   \n340     Biscoe            46.8           14.3              215.0       4850.0   \n341     Biscoe            50.4           15.7              222.0       5750.0   \n342     Biscoe            45.2           14.8              212.0       5200.0   \n343     Biscoe            49.9           16.1              213.0       5400.0   \n\n        sex  species_Chinstrap  species_Gentoo  \n0      Male              False           False  \n1    Female              False           False  \n2    Female              False           False  \n3       NaN              False           False  \n4    Female              False           False  \n..      ...                ...             ...  \n339     NaN              False            True  \n340  Female              False            True  \n341    Male              False            True  \n342  Female              False            True  \n343    Male              False            True  \n\n[344 rows x 8 columns]\n\n\n선형 회귀, 로지스틱 회귀, KNN, SVM, PCA 등의 모델 사용 시 적용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.3 Ordinal Encoding",
    "text": "7.3 Ordinal Encoding\n순서가 있는 범주형 변수에 사용, 사용자가 순서를 직접 정의\n\n7.3.1 예제 코드\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\noe = OrdinalEncoder(\n    categories=[[\"small\", \"medium\", \"large\"]]\n)\n\ndf_ord = pd.DataFrame(\n    oe.fit_transform(\n        pd.DataFrame({\"size\": [\"small\", \"large\", \"medium\"]})\n    ),\n    columns=[\"size_enc\"]\n)\n\nprint(df_ord)\n\n   size_enc\n0       0.0\n1       2.0\n2       1.0\n\n\n명확한 순서 존재하는 등급, 크기, 단계 표현 등에 적용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#targetmean-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#targetmean-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.4 Target(Mean) Encoding",
    "text": "7.4 Target(Mean) Encoding\n범주를 타깃 변수의 평균값으로 치환, 정보량은 크지만 데이터 누수 위험 존재\n\n7.4.1 예제 코드\n\ndf_te = df.copy()\n\ntarget = \"body_mass_g\"\ncol = \"species\"\n\nmean_map = df_te.groupby(col)[target].mean()\ndf_te[\"species_enc\"] = df_te[col].map(mean_map)\n\nprint(df_te)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  species_enc  \n0         3750.0    Male  3700.662252  \n1         3800.0  Female  3700.662252  \n2         3250.0  Female  3700.662252  \n3            NaN     NaN  3700.662252  \n4         3450.0  Female  3700.662252  \n..           ...     ...          ...  \n339          NaN     NaN  5076.016260  \n340       4850.0  Female  5076.016260  \n341       5750.0    Male  5076.016260  \n342       5200.0  Female  5076.016260  \n343       5400.0    Male  5076.016260  \n\n[344 rows x 8 columns]\n\n\n반드시 train 데이터 기준으로만 계산, CV 기반 smoothing 권장",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#빈도frequency-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#빈도frequency-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.5 빈도(Frequency) 인코딩",
    "text": "7.5 빈도(Frequency) 인코딩\n범주의 출현 빈도로 인코딩, 희귀 범주 처리에 유용\n\n7.5.1 예제 코드\n\nfreq_map = df[\"island\"].value_counts(normalize=True)\ndf_freq = df.copy()\ndf_freq[\"island_enc\"] = df_freq[\"island\"].map(freq_map)\n\nprint(df_freq)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  island_enc  \n0         3750.0    Male    0.151163  \n1         3800.0  Female    0.151163  \n2         3250.0  Female    0.151163  \n3            NaN     NaN    0.151163  \n4         3450.0  Female    0.151163  \n..           ...     ...         ...  \n339          NaN     NaN    0.488372  \n340       4850.0  Female    0.488372  \n341       5750.0    Male    0.488372  \n342       5200.0  Female    0.488372  \n343       5400.0    Male    0.488372  \n\n[344 rows x 8 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.6 여러 컬럼을 한 번에 인코딩",
    "text": "7.6 여러 컬럼을 한 번에 인코딩\n\n7.6.1 One-Hot (pandas)\n\ncat_cols = [\"species\", \"island\", \"sex\"]\n\ndf_ohe = pd.get_dummies(df, columns=cat_cols)\n\n\n\n7.6.2 Label Encoding (반복 처리)\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_label = df.copy()\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_label[col] = le.fit_transform(df_label[col].astype(str))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#인코딩-방식-요약",
    "href": "part1/07. 범주형 데이터 인코딩.html#인코딩-방식-요약",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.7 인코딩 방식 요약",
    "text": "7.7 인코딩 방식 요약\n\n\n\n인코딩\n순서 필요\n컬럼 수 증가\n추천 모델\n\n\n\n\nLabel\nX\nX\n트리 모델\n\n\nOne-Hot\nX\nO\n선형, 거리 기반\n\n\nOrdinal\nO\nX\n순서형 데이터\n\n\nTarget\nX\nX\n고급 모델\n\n\nFrequency\nX\nX\n희귀 범주 많을 때",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html",
    "href": "part1/08. 연속형 데이터 범주화.html",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "8.1 연속형 데이터 범주화 (Discretization / Binning)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#연속형-데이터-범주화-discretization-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#연속형-데이터-범주화-discretization-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "8.1.1 개념\n\n연속형 변수를 구간(범주)으로 나누어 범주형 변수로 변환\n정보는 줄지만 해석력 증가, 노이즈 감소, 비선형 관계 단순화 효과",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.2 등간격 구간화 (Equal-width binning)",
    "text": "8.2 등간격 구간화 (Equal-width binning)\n\n8.2.1 개념\n전체 값 범위를 동일한 간격으로 분할\n\\[\n\\text{bin width} = \\frac{\\max(x) - \\min(x)}{k}\n\\]\n\n\n8.2.2 예제\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")\n\ndf[\"bill_length_bin\"] = pd.cut(df[\"bill_length_mm\"], bins=4)\n\nprint(df[\"bill_length_bin\"])\n\n0       (38.975, 45.85]\n1       (38.975, 45.85]\n2       (38.975, 45.85]\n3                   NaN\n4      (32.072, 38.975]\n             ...       \n339                 NaN\n340     (45.85, 52.725]\n341     (45.85, 52.725]\n342     (38.975, 45.85]\n343     (45.85, 52.725]\nName: bill_length_bin, Length: 344, dtype: category\nCategories (4, interval[float64, right]): [(32.072, 38.975] &lt; (38.975, 45.85] &lt; (45.85, 52.725] &lt; (52.725, 59.6]]\n\n\n\n\n8.2.3 범주 이름 지정\n\ndf[\"bill_length_bin\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=4,\n    labels=[\"짧음\", \"보통\", \"김\", \"매우 김\"]\n)\n\nprint(df[\"bill_length_bin\"])\n\n0       보통\n1       보통\n2       보통\n3      NaN\n4       짧음\n      ... \n339    NaN\n340      김\n341      김\n342     보통\n343      김\nName: bill_length_bin, Length: 344, dtype: category\nCategories (4, str): ['짧음' &lt; '보통' &lt; '김' &lt; '매우 김']\n\n\n구현 간단하지만 이상치에 민감, 데이터 밀도 불균형 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-quantile-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-quantile-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.3 등빈도 구간화 (Equal-frequency, Quantile binning)",
    "text": "8.3 등빈도 구간화 (Equal-frequency, Quantile binning)\n\n8.3.1 개념\n각 구간에 데이터 개수가 동일하도록 분할\n\n\n8.3.2 예제\n\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4\n)\n\nprint(df[\"bill_length_qbin\"])\n\n0      (32.099000000000004, 39.225]\n1                   (39.225, 44.45]\n2                   (39.225, 44.45]\n3                               NaN\n4      (32.099000000000004, 39.225]\n                   ...             \n339                             NaN\n340                   (44.45, 48.5]\n341                    (48.5, 59.6]\n342                   (44.45, 48.5]\n343                    (48.5, 59.6]\nName: bill_length_qbin, Length: 344, dtype: category\nCategories (4, interval[float64, right]): [(32.099000000000004, 39.225] &lt; (39.225, 44.45] &lt; (44.45, 48.5] &lt; (48.5, 59.6]]\n\n\n\n\n8.3.3 라벨 지정\n\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4,\n    labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n)\n\ndf[\"bill_length_qbin\"]\n\n0       Q1\n1       Q2\n2       Q2\n3      NaN\n4       Q1\n      ... \n339    NaN\n340     Q3\n341     Q4\n342     Q3\n343     Q4\nName: bill_length_qbin, Length: 344, dtype: category\nCategories (4, str): ['Q1' &lt; 'Q2' &lt; 'Q3' &lt; 'Q4']\n\n\n데이터 불균형 완화, 분포 반영, 동일값 많으면 에러 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.4 사용자 정의 구간화",
    "text": "8.4 사용자 정의 구간화\n\n8.4.1 개념\n도메인 지식 기반으로 직접 구간 설정\n\n\n8.4.2 예제\n\nbins = [30, 40, 45, 50, 60]\nlabels = [\"매우 짧음\", \"짧음\", \"보통\", \"김\"]\n\ndf[\"bill_length_custom\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=bins,\n    labels=labels\n)\n\nprint(df[\"bill_length_custom\"])\n\n0      매우 짧음\n1      매우 짧음\n2         짧음\n3        NaN\n4      매우 짧음\n       ...  \n339      NaN\n340       보통\n341        김\n342       보통\n343       보통\nName: bill_length_custom, Length: 344, dtype: category\nCategories (4, str): ['매우 짧음' &lt; '짧음' &lt; '보통' &lt; '김']\n\n\n해석력 최고, 주관 개입, 기준 설명 필요",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.5 k-means 기반 구간화",
    "text": "8.5 k-means 기반 구간화\n\n8.5.1 개념\n값의 분포를 고려해 군집 중심 기준으로 구간화, 비선형 구조 반영 가능\n\n\n8.5.2 예제\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nx = df[[\"bill_length_mm\"]].dropna()\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nx[\"cluster\"] = kmeans.fit_predict(x)\n\ndf.loc[x.index, \"bill_length_kbin\"] = x[\"cluster\"]\n\nprint(df[\"bill_length_kbin\"])\n\n0      2.0\n1      2.0\n2      2.0\n3      NaN\n4      2.0\n      ... \n339    NaN\n340    0.0\n341    1.0\n342    0.0\n343    1.0\nName: bill_length_kbin, Length: 344, dtype: float64\n\n\n\n\n8.5.3 특징\n\n데이터 기반\n해석은 상대적으로 어려움",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.6 Decision Tree 기반 구간화",
    "text": "8.6 Decision Tree 기반 구간화\n\n8.6.1 개념\n타깃 변수를 기준으로 정보이득 최대화, 지도학습 기반 binning\n\n\n8.6.2 예제 (체중 기준)\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nx = df[[\"bill_length_mm\"]].dropna()\ny = df.loc[x.index, \"body_mass_g\"]\n\ntree = DecisionTreeRegressor(\n    max_leaf_nodes=4\n)\ntree.fit(x, y)\n\ndf.loc[x.index, \"bill_length_treebin\"] = tree.apply(x)\n\n타깃 정보 반영, 데이터 누수 주의",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩까지-연결",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩까지-연결",
    "title": "8  연속형 데이터 범주화",
    "section": "8.7 범주화 후 인코딩까지 연결",
    "text": "8.7 범주화 후 인코딩까지 연결\n\ndf_bin = pd.get_dummies(\n    df,\n    columns=[\"bill_length_bin\"],\n    drop_first=True\n)\n\nprint(df_bin)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex bill_length_qbin bill_length_custom  \\\n0         3750.0    Male               Q1              매우 짧음   \n1         3800.0  Female               Q2              매우 짧음   \n2         3250.0  Female               Q2                 짧음   \n3            NaN     NaN              NaN                NaN   \n4         3450.0  Female               Q1              매우 짧음   \n..           ...     ...              ...                ...   \n339          NaN     NaN              NaN                NaN   \n340       4850.0  Female               Q3                 보통   \n341       5750.0    Male               Q4                  김   \n342       5200.0  Female               Q3                 보통   \n343       5400.0    Male               Q4                 보통   \n\n     bill_length_kbin  bill_length_treebin  bill_length_bin_보통  \\\n0                 2.0                  4.0                True   \n1                 2.0                  4.0                True   \n2                 2.0                  4.0                True   \n3                 NaN                  NaN               False   \n4                 2.0                  3.0               False   \n..                ...                  ...                 ...   \n339               NaN                  NaN               False   \n340               0.0                  5.0               False   \n341               1.0                  6.0               False   \n342               0.0                  5.0                True   \n343               1.0                  6.0               False   \n\n     bill_length_bin_김  bill_length_bin_매우 김  \n0                False                 False  \n1                False                 False  \n2                False                 False  \n3                False                 False  \n4                False                 False  \n..                 ...                   ...  \n339              False                 False  \n340               True                 False  \n341               True                 False  \n342              False                 False  \n343               True                 False  \n\n[344 rows x 14 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-상황",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-상황",
    "title": "8  연속형 데이터 범주화",
    "section": "8.8 범주화 상황",
    "text": "8.8 범주화 상황\n\n\n\n상황\n효과\n\n\n\n\n모델 해석이 중요할 때\nO\n\n\n비선형 관계 단순화\nO\n\n\n데이터 노이즈 큼\nO\n\n\n데이터 충분히 많음\nX\n\n\n딥러닝\n거의 사용 안 함",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#스케일링-vs-범주화",
    "href": "part1/08. 연속형 데이터 범주화.html#스케일링-vs-범주화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.9 스케일링 vs 범주화",
    "text": "8.9 스케일링 vs 범주화\n\n\n\n항목\n스케일링\n범주화\n\n\n\n\n정보 손실\n없음\n있음\n\n\n해석력\n낮음\n높음\n\n\n모델 안정성\n보통\n높음\n\n\n주 용도\n거리 계산\n규칙 기반\n\n\n\n범주화는 정보 손실을 감수하고 해석력을 얻는 선택, EDA 결과 보고 결정하는 단계",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html",
    "href": "part1/09. 불균형 데이터 처리.html",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "9.1 불균형 데이터의 문제점\n타깃 클래스 간 샘플 수 차이가 큰 경우, 다수 클래스에 치우친 모델이 생성됨 → 정확도는 높아 보이지만 실제 성능은 나쁨",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "9.1.1 예시\n\n전체 정확도 95%\n소수 클래스 재현율 10%\n\n“잘 맞춘 것처럼 보이지만 중요한 건 다 틀림”",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "title": "9  불균형 데이터 처리",
    "section": "9.2 불균형 여부 진단",
    "text": "9.2 불균형 여부 진단\n\n9.2.1 클래스 분포 확인\n\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\").dropna(subset=[\"species\"])\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\n\n9.2.2 비율로 확인\n\ndf[\"species\"].value_counts(normalize=True)\n\nspecies\nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\nName: proportion, dtype: float64\n\n\n\n\n9.2.3 시각화\n\nimport matplotlib.pyplot as plt\n\ndf[\"species\"].value_counts().plot(kind=\"bar\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "href": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "title": "9  불균형 데이터 처리",
    "section": "9.3 평가 지표 변경",
    "text": "9.3 평가 지표 변경\n불균형 데이터에서는 Accuracy 사용 금물\n\n9.3.1 대안 지표\n\nPrecision\nRecall\nF1-score\nROC-AUC\nPR-AUC\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true, y_pred))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.4 데이터 수준 처리 방법",
    "text": "9.4 데이터 수준 처리 방법\n\n9.4.1 언더샘플링 (Under-sampling)\n\n9.4.1.1 개념\n다수 클래스 일부 제거\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf_dropna = df[['species']+num_cols].dropna()\nX = df_dropna.drop(\"species\", axis=1)\ny = df_dropna[\"species\"]\n\nrus = RandomUnderSampler(random_state=42)\nX_under, y_under = rus.fit_resample(X, y)\nprint(\"Before\\n\", y.value_counts())\nprint(\"After\\n\", y_under.value_counts())\n\nBefore\n species\nAdelie       151\nGentoo       123\nChinstrap     68\nName: count, dtype: int64\nAfter\n species\nAdelie       68\nChinstrap    68\nGentoo       68\nName: count, dtype: int64\n\n\n빠르다는 장점이 있으나 정보가 손신된다는 단점 존재\n\n\n\n9.4.2 오버샘플링 (Over-sampling)\n\n9.4.2.1 단순 복제\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\nX_over, y_over = ros.fit_resample(X, y)\n\nprint(y_over.value_counts())\n\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n\n\n\n\n9.4.2.2 문제점\n과적합 위험\n\n\n\n9.4.3 SMOTE (Synthetic Minority Over-sampling)\n\n9.4.3.1 개념\n소수 클래스 사이에 가상 샘플 생성\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X, y)\n\nprint(y_smote.value_counts())\n\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n\n\n\n\n\n9.4.4 SMOTE 변형\n\n\n\n기법\n특징\n\n\n\n\nBorderline-SMOTE\n경계 근처만 증강\n\n\nSMOTE-NC\n수치 + 범주 혼합\n\n\nADASYN\n학습 어려운 영역 집중",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.5 모델 수준 처리 방법",
    "text": "9.5 모델 수준 처리 방법\n\n9.5.1 클래스 가중치 조정\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n    class_weight=\"balanced\"\n)\n\n\n\n9.5.2 수동 가중치 설정\n\nweights = {\n    \"Adelie\": 1,\n    \"Gentoo\": 2,\n    \"Chinstrap\": 3\n}\n\nmodel = LogisticRegression(class_weight=weights)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "href": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "title": "9  불균형 데이터 처리",
    "section": "9.6 앙상블 + 불균형 대응",
    "text": "9.6 앙상블 + 불균형 대응\n\n9.6.1 대표 기법\n\nBalanced Random Forest\nEasyEnsemble\n\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\nmodel = BalancedRandomForestClassifier(\n    n_estimators=100,\n    random_state=42\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "href": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "title": "9  불균형 데이터 처리",
    "section": "9.7 Pipeline으로 안전하게 적용",
    "text": "9.7 Pipeline으로 안전하게 적용\n샘플링은 반드시 학습 데이터에만!\n\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\n\npipeline = Pipeline([\n    (\"smote\", SMOTE(random_state=42)),\n    (\"model\", LogisticRegression())\n])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#권장-상황",
    "href": "part1/09. 불균형 데이터 처리.html#권장-상황",
    "title": "9  불균형 데이터 처리",
    "section": "9.8 권장 상황",
    "text": "9.8 권장 상황\n\n\n\n상황\n권장\n\n\n\n\n데이터 작음\nSMOTE\n\n\n데이터 큼\n클래스 가중치\n\n\n노이즈 많음\n언더샘플링\n\n\n범주형 포함\nSMOTE-NC\n\n\n트리 모델\n가중치 or Balanced RF",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html",
    "href": "part1/10. 피처 엔지니어링.html",
    "title": "10  피처 엔지니어링",
    "section": "",
    "text": "10.1 피처 엔지니어링",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#피처-엔지니어링",
    "href": "part1/10. 피처 엔지니어링.html#피처-엔지니어링",
    "title": "10  피처 엔지니어링",
    "section": "",
    "text": "10.1.1 정의\n기존 데이터를 변형·조합하여 모델이 더 잘 학습할 수 있는 입력 변수(feature)를 만드는 과정\n\n모델 성능의 상당 부분은 알고리즘이 아니라 피처에서 결정됨",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#왜-중요한가",
    "href": "part1/10. 피처 엔지니어링.html#왜-중요한가",
    "title": "10  피처 엔지니어링",
    "section": "10.2 왜 중요한가?",
    "text": "10.2 왜 중요한가?\n\n같은 모델, 다른 성능의 가장 큰 이유\n복잡한 모델보다 좋은 피처 + 단순 모델이 더 강력한 경우 많음\n도메인 지식이 가장 잘 반영되는 단계",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#피처-엔지니어링의-주요-유형",
    "href": "part1/10. 피처 엔지니어링.html#피처-엔지니어링의-주요-유형",
    "title": "10  피처 엔지니어링",
    "section": "10.3 피처 엔지니어링의 주요 유형",
    "text": "10.3 피처 엔지니어링의 주요 유형\n예제 데이터셋 적재\n\nimport seaborn as sns\nimport pandas as pd\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\n사용 컬럼 예시\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nspecies\nsex",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#파생-변수-생성",
    "href": "part1/10. 피처 엔지니어링.html#파생-변수-생성",
    "title": "10  피처 엔지니어링",
    "section": "10.4 파생 변수 생성",
    "text": "10.4 파생 변수 생성\n\n10.4.1 (1) 비율(Ratio) 피처\n\ndf[\"bill_ratio\"] = df[\"bill_length_mm\"] / df[\"bill_depth_mm\"]\n\nprint(df[\"bill_ratio\"])\n\n0      2.090909\n1      2.270115\n2      2.238889\n4      1.901554\n5      1.907767\n         ...   \n338    3.445255\n340    3.272727\n341    3.210191\n342    3.054054\n343    3.099379\nName: bill_ratio, Length: 333, dtype: float64\n\n\n\n\n10.4.2 (2) 조합 피처 (곱, 합)\n\ndf[\"body_flipper_interaction\"] = (df[\"body_mass_g\"] * df[\"flipper_length_mm\"])\n\nprint(df[\"body_flipper_interaction\"])\n\n0       678750.0\n1       706800.0\n2       633750.0\n4       665850.0\n5       693500.0\n         ...    \n338    1053950.0\n340    1042750.0\n341    1276500.0\n342    1102400.0\n343    1150200.0\nName: body_flipper_interaction, Length: 333, dtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#로그제곱근-변환-후-파생",
    "href": "part1/10. 피처 엔지니어링.html#로그제곱근-변환-후-파생",
    "title": "10  피처 엔지니어링",
    "section": "10.5 로그·제곱근 변환 후 파생",
    "text": "10.5 로그·제곱근 변환 후 파생\n\nimport numpy as np\n\ndf[\"log_body_mass\"] = np.log(df[\"body_mass_g\"])\n\nprint(df[\"log_body_mass\"])\n\n0      8.229511\n1      8.242756\n2      8.086410\n4      8.146130\n5      8.202482\n         ...   \n338    8.502080\n340    8.486734\n341    8.656955\n342    8.556414\n343    8.594154\nName: log_body_mass, Length: 333, dtype: float64\n\n\n\n스케일 안정화\n거리 기반 모델(KNN, SVM)에 특히 중요",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#구간-기반-피처",
    "href": "part1/10. 피처 엔지니어링.html#구간-기반-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.6 구간 기반 피처",
    "text": "10.6 구간 기반 피처\n\ndf[\"body_mass_group\"] = pd.cut(\n    df[\"body_mass_g\"],\n    bins=[0, 3500, 4500, 7000],\n    labels=[\"small\", \"medium\", \"large\"]\n)\n\nprint(df[\"body_mass_group\"])\n\n0      medium\n1      medium\n2       small\n4       small\n5      medium\n        ...  \n338     large\n340     large\n341     large\n342     large\n343     large\nName: body_mass_group, Length: 333, dtype: category\nCategories (3, str): ['small' &lt; 'medium' &lt; 'large']\n\n\n\n연속값 → 해석 가능한 범주\n트리 모델에서 성능 개선 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#집계-기반-피처",
    "href": "part1/10. 피처 엔지니어링.html#집계-기반-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.7 집계 기반 피처",
    "text": "10.7 집계 기반 피처\n\n10.7.1 species 기준 평균 대비 차이\n\nspecies_mean = df.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")\n\ndf[\"body_mass_diff_species\"] = (df[\"body_mass_g\"] - species_mean)\n\nprint(df[\"body_mass_diff_species\"])\n\n0       43.835616\n1       93.835616\n2     -456.164384\n4     -256.164384\n5      -56.164384\n          ...    \n338   -167.436975\n340   -242.436975\n341    657.563025\n342    107.563025\n343    307.563025\nName: body_mass_diff_species, Length: 333, dtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#통계-요약-피처",
    "href": "part1/10. 피처 엔지니어링.html#통계-요약-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.8 통계 요약 피처",
    "text": "10.8 통계 요약 피처\n\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf[\"num_mean\"] = df[num_cols].mean(axis=1)\ndf[\"num_std\"] = df[num_cols].std(axis=1)\n\nprint(df[\"num_mean\"])\nprint(df[\"num_std\"])\n\n0       997.200\n1      1010.725\n2       875.825\n4       924.750\n5       974.975\n         ...   \n338    1299.975\n340    1281.525\n341    1509.525\n342    1368.000\n343    1419.750\nName: num_mean, Length: 333, dtype: float64\n0      1836.619008\n1      1861.021127\n2      1584.739239\n4      1685.310864\n5      1784.961610\n          ...     \n338    2418.270493\n340    2380.608776\n341    2828.421689\n342    2556.137065\n343    2654.892212\nName: num_std, Length: 333, dtype: float64\n\n\n\n개체의 전체적인 신체 특성 요약",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#범주형-조합-피처",
    "href": "part1/10. 피처 엔지니어링.html#범주형-조합-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.9 범주형 조합 피처",
    "text": "10.9 범주형 조합 피처\n\ndf[\"species_sex\"] = (\n    df[\"species\"] + \"_\" + df[\"sex\"]\n)\n\n\n단일 범주로는 부족할 때 유용\n빈도가 너무 낮은 조합은 주의",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#시간순서형-데이터",
    "href": "part1/10. 피처 엔지니어링.html#시간순서형-데이터",
    "title": "10  피처 엔지니어링",
    "section": "10.10 시간·순서형 데이터",
    "text": "10.10 시간·순서형 데이터\n\n날짜 → 연/월/요일\n시간 → 주기(sin, cos)\n누적합, 이동평균",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#피처-선택과의-연결",
    "href": "part1/10. 피처 엔지니어링.html#피처-선택과의-연결",
    "title": "10  피처 엔지니어링",
    "section": "10.11 피처 선택과의 연결",
    "text": "10.11 피처 선택과의 연결\n모든 피처를 쓰는 게 정답은 아님\n\n10.11.1 간단한 상관 확인\n\ndf[num_cols + [\"bill_ratio\"]].corr()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nbill_ratio\n\n\n\n\nbill_length_mm\n1.000000\n-0.228626\n0.653096\n0.589451\n0.779230\n\n\nbill_depth_mm\n-0.228626\n1.000000\n-0.577792\n-0.472016\n-0.781246\n\n\nflipper_length_mm\n0.653096\n-0.577792\n1.000000\n0.872979\n0.801980\n\n\nbody_mass_g\n0.589451\n-0.472016\n0.872979\n1.000000\n0.702179\n\n\nbill_ratio\n0.779230\n-0.781246\n0.801980\n0.702179\n1.000000\n\n\n\n\n\n\n\n\n중복 정보 제거\n다중공선성 완화",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#모델별-피처-엔지니어링-전략",
    "href": "part1/10. 피처 엔지니어링.html#모델별-피처-엔지니어링-전략",
    "title": "10  피처 엔지니어링",
    "section": "10.12 모델별 피처 엔지니어링 전략",
    "text": "10.12 모델별 피처 엔지니어링 전략\n\n\n\n모델\n전략\n\n\n\n\n선형 모델\n스케일링, 상호작용\n\n\nKNN / SVM\n스케일·분포 중요\n\n\n트리 계열\n비선형 파생, binning\n\n\n딥러닝\n단순 + 양 많게",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#전처리-전체-흐름에서-위치",
    "href": "part1/10. 피처 엔지니어링.html#전처리-전체-흐름에서-위치",
    "title": "10  피처 엔지니어링",
    "section": "10.13 전처리 전체 흐름에서 위치",
    "text": "10.13 전처리 전체 흐름에서 위치\n데이터 로드\n → 결측치 처리\n → 이상치 탐지\n → 스케일링\n → 분포 변환\n → 인코딩\n → 불균형 처리\n → 피처 엔지니어링\n → 모델링",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html",
    "href": "part2/01. 확률분포와 표본.html",
    "title": "11  확률분포와 표본",
    "section": "",
    "text": "11.1 확률분포란?\n통계적 추론의 출발점은 “데이터가 어떤 분포에서 나왔는가?”를 이해하는 것이다.\n확률변수(Random Variable) 가 취할 수 있는 값과 그 값이 나타날 확률의 구조\n분류",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#확률분포란",
    "href": "part2/01. 확률분포와 표본.html#확률분포란",
    "title": "11  확률분포와 표본",
    "section": "",
    "text": "이산형 확률분포\n연속형 확률분포",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#연속형-확률분포-continuous-distributions",
    "href": "part2/01. 확률분포와 표본.html#연속형-확률분포-continuous-distributions",
    "title": "11  확률분포와 표본",
    "section": "11.2 연속형 확률분포 (Continuous Distributions)",
    "text": "11.2 연속형 확률분포 (Continuous Distributions)\n\n11.2.1 대표적인 연속형 분포\n\n\n\n분포\n특징\n사용 예\n\n\n\n\n정규분포\n종 모양, 평균 중심 대칭\n신체 측정값\n\n\n지수분포\n오른쪽 꼬리\n대기 시간\n\n\n감마분포\n비대칭 양수\n생존 시간\n\n\n로그정규\n로그 후 정규\n소득, 크기\n\n\n\n\n\n11.2.2 예제 – 연속형 변수\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\n\nsns.histplot(\n    df[\"body_mass_g\"],\n    kde=True\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n11.2.3 정규분포 수식\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#이산형-확률분포-discrete-distributions",
    "href": "part2/01. 확률분포와 표본.html#이산형-확률분포-discrete-distributions",
    "title": "11  확률분포와 표본",
    "section": "11.3 이산형 확률분포 (Discrete Distributions)",
    "text": "11.3 이산형 확률분포 (Discrete Distributions)\n\n11.3.1 대표적인 이산형 분포\n\n\n\n분포\n특징\n사용 예\n\n\n\n\n베르누이\n성공/실패\n합격 여부\n\n\n이항분포\nn번 시행\n불량 개수\n\n\n포아송\n단위 시간 발생 횟수\n고객 유입\n\n\n\n\n\n11.3.2 예제 – 이산형 변수\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64\n\n\n\nsns.countplot(\n    x=\"species\",\n    data=df\n)\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#표본과-모집단",
    "href": "part2/01. 확률분포와 표본.html#표본과-모집단",
    "title": "11  확률분포와 표본",
    "section": "11.4 표본과 모집단",
    "text": "11.4 표본과 모집단\n\n모집단(Population) 전체 관심 대상\n표본(Sample) 모집단에서 일부 추출한 데이터\n\n\n실제 분석은 거의 항상 표본 기반",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#표본-분포-sampling-distribution",
    "href": "part2/01. 확률분포와 표본.html#표본-분포-sampling-distribution",
    "title": "11  확률분포와 표본",
    "section": "11.5 표본 분포 (Sampling Distribution)",
    "text": "11.5 표본 분포 (Sampling Distribution)\n\n11.5.1 개념\n\n“표본 통계량(평균, 분산 등)”의 분포\n데이터의 분포 ≠ 표본 평균의 분포\n\n\n\n11.5.2 예제 – 표본 평균 분포\n\nimport numpy as np\n\nbody_mass = df[\"body_mass_g\"].values\n\n\nsample_means = []\n\nfor _ in range(1000):\n    sample = np.random.choice(body_mass, size=30, replace=True)\n    sample_means.append(sample.mean())\n\n\nsns.histplot(sample_means, kde=True)\nplt.show()\n\n\n\n\n\n\n\n\n\n원 데이터는 비대칭\n표본 평균 분포는 정규에 가까워짐 &gt; 중심극한정리(CLT)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#중심극한정리-central-limit-theorem",
    "href": "part2/01. 확률분포와 표본.html#중심극한정리-central-limit-theorem",
    "title": "11  확률분포와 표본",
    "section": "11.6 중심극한정리 (Central Limit Theorem)",
    "text": "11.6 중심극한정리 (Central Limit Theorem)\n\n표본 크기가 충분히 크면 표본 평균의 분포는 원래 분포와 관계없이 정규분포에 수렴\n\n조건\n\n표본 크기 ↑\n독립성 가정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#중심극한-정리가-중요한-이유",
    "href": "part2/01. 확률분포와 표본.html#중심극한-정리가-중요한-이유",
    "title": "11  확률분포와 표본",
    "section": "11.7 중심극한 정리가 중요한 이유",
    "text": "11.7 중심극한 정리가 중요한 이유\n\nt 검정\nANOVA\n신뢰구간\n회귀 분석\n\n모두 “표본 평균 분포” 가정 위에서 동작",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html",
    "href": "part2/02. 정규성 검정.html",
    "title": "12  정규성 검정",
    "section": "",
    "text": "12.1 정규성 검정\n많은 통계 기법은 다음을 가정한다.\n적용 대상\n정규성 검정(Normality Test)은 “데이터가 정규인가?” 가 아니라 “정규 가정을 써도 되는가?” 를 판단하는 과정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성-검정",
    "href": "part2/02. 정규성 검정.html#정규성-검정",
    "title": "12  정규성 검정",
    "section": "",
    "text": "표본이 정규분포를 따른다\n또는 표본 평균이 정규분포를 따른다\n\n\n\nt 검정\nANOVA\n선형회귀 (잔차)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성을-검사-필요-여부",
    "href": "part2/02. 정규성 검정.html#정규성을-검사-필요-여부",
    "title": "12  정규성 검정",
    "section": "12.2 정규성을 검사 필요 여부",
    "text": "12.2 정규성을 검사 필요 여부\n\n\n\n상황\n정규성 필요 여부\n\n\n\n\n원 데이터\n경우에 따라\n\n\n표본 평균\nCLT로 완화\n\n\n소표본 (n &lt; 30)\n매우 중요\n\n\n잔차(residual)\n중요",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#시각적-정규성-검정",
    "href": "part2/02. 정규성 검정.html#시각적-정규성-검정",
    "title": "12  정규성 검정",
    "section": "12.3 시각적 정규성 검정",
    "text": "12.3 시각적 정규성 검정\n\n12.3.1 (1) 히스토그램 + KDE\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\n\nsns.histplot(\n    df[\"body_mass_g\"],\n    kde=True\n)\nplt.show()\n\n\n\n\n\n\n\n\n정규성 판단을 위한 점검 사항\n\n종 모양 여부\n대칭성\n꼬리 방향\n\n\n\n12.3.2 (2) Q–Q Plot\n\nimport scipy.stats as stats\n\nstats.probplot(df[\"body_mass_g\"], plot=plt)\nplt.show()\n\n\n\n\n\n\n\n\n해석\n\n점들이 직선에 가까울수록 정규성 만족\n양끝 이탈 → 꼬리 문제",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#통계적-정규성-검정",
    "href": "part2/02. 정규성 검정.html#통계적-정규성-검정",
    "title": "12  정규성 검정",
    "section": "12.4 통계적 정규성 검정",
    "text": "12.4 통계적 정규성 검정\n\n12.4.1 대표적인 정규성 검정 방법\n\n\n\n검정\n특징\n권장 상황\n\n\n\n\nShapiro–Wilk\n가장 보편적\nn ≤ 5000\n\n\nKolmogorov–Smirnov\n기준 분포 필요\n이론 검증\n\n\nAnderson–Darling\n꼬리 민감\n분포 비교\n\n\nD’Agostino\n왜도·첨도\n중대형 표본\n\n\n\n\n\n12.4.2 (1) Shapiro–Wilk 검정\n\nfrom scipy.stats import shapiro\n\nstat, p_value = shapiro(df[\"body_mass_g\"])\n\nstat, p_value\n\n(np.float64(0.9580123688127105), np.float64(3.567711408268874e-08))\n\n\n판단\n\np ≥ 0.05 → 정규성 가정 가능\np &lt; 0.05 → 정규성 위반\n\n\n\n12.4.3 (2) Anderson–Darling 검정\n\nfrom scipy.stats import anderson\n\nresult = anderson(df[\"body_mass_g\"], dist=\"norm\")\nresult\n\nAndersonResult(statistic=np.float64(4.614584474585229), critical_values=array([0.56 , 0.63 , 0.75 , 0.871, 1.033]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=np.float64(4207.057057057057), scale=np.float64(805.2158019428965))\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n\n\n해석\n\nstatistic &lt; critical value → 정규성 만족\n유의수준별 임계값 제공",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#표본-크기와-p-value의-함정",
    "href": "part2/02. 정규성 검정.html#표본-크기와-p-value의-함정",
    "title": "12  정규성 검정",
    "section": "12.5 표본 크기와 p-value의 함정",
    "text": "12.5 표본 크기와 p-value의 함정\n\n표본 수가 크면 거의 항상 p &lt; 0.05\n\n\nlarge_sample = df[\"body_mass_g\"].sample(300, random_state=42)\nshapiro(large_sample)\n\nShapiroResult(statistic=np.float64(0.95918974304124), pvalue=np.float64(1.9213489184341196e-07))\n\n\n의미\n\n통계적으로는 비정규\n실무적으로는 문제 없음\n\n시각적 판단 + 목적 기반 판단이 중요",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성이-깨졌을-때의-대응-전략",
    "href": "part2/02. 정규성 검정.html#정규성이-깨졌을-때의-대응-전략",
    "title": "12  정규성 검정",
    "section": "12.6 정규성이 깨졌을 때의 대응 전략",
    "text": "12.6 정규성이 깨졌을 때의 대응 전략\n\n12.6.1 분포 변환\n\n로그 변환\n제곱근 변환\nBox–Cox\nYeo–Johnson\n\n\nimport numpy as np\n\nlog_mass = np.log(df[\"body_mass_g\"])\nshapiro(log_mass)\n\nShapiroResult(statistic=np.float64(0.9755498857335912), pvalue=np.float64(1.9605691846856507e-05))\n\n\n\n\n12.6.2 비모수 검정 사용\n\n\n\n상황\n대안\n\n\n\n\n평균 비교\nMann–Whitney\n\n\n다집단\nKruskal–Wallis\n\n\n상관\nSpearman\n\n\n\n\n\n12.6.3 표본 평균 활용 (CLT)\n\n충분한 표본 크기\n독립성 만족",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성-검정-유의사항",
    "href": "part2/02. 정규성 검정.html#정규성-검정-유의사항",
    "title": "12  정규성 검정",
    "section": "12.7 정규성 검정 유의사항",
    "text": "12.7 정규성 검정 유의사항\n\n정규성 ≠ 절대 조건\np-value 하나로 결정\n시각 + 표본 크기 + 분석 목적 종합 판단",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html",
    "href": "part2/03. 등분산 검정.html",
    "title": "13  등분산 검정",
    "section": "",
    "text": "13.1 등분산성\n여러 집단의 분산이 동일하다는 가정(Homoscedasticity)\n\\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2\n\\]\n반대 개념",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성",
    "href": "part2/03. 등분산 검정.html#등분산성",
    "title": "13  등분산 검정",
    "section": "",
    "text": "이분산성 (Heteroscedasticity)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성-검사-필요성",
    "href": "part2/03. 등분산 검정.html#등분산성-검사-필요성",
    "title": "13  등분산 검정",
    "section": "13.2 등분산성 검사 필요성",
    "text": "13.2 등분산성 검사 필요성\n다음 기법들의 전제 조건으로 등분산성 검사((Test for Homogeneity of Variance) 진행\n\n\n\n분석\n등분산 필요 여부\n\n\n\n\nStudent t-test\n필요\n\n\nANOVA\n필요\n\n\n선형회귀(잔차)\n중요\n\n\nWelch t-test\n불필요\n\n\n비모수 검정\n불필요\n\n\n\n정규성보다 등분산성 위반이 더 위험한 경우도 많음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성-검사-시점",
    "href": "part2/03. 등분산 검정.html#등분산성-검사-시점",
    "title": "13  등분산 검정",
    "section": "13.3 등분산성 검사 시점",
    "text": "13.3 등분산성 검사 시점\n\n집단 간 평균 비교 전\nANOVA 적용 전\n회귀 분석에서 잔차 진단 시",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#시각적-등분산성-점검",
    "href": "part2/03. 등분산 검정.html#시각적-등분산성-점검",
    "title": "13  등분산 검정",
    "section": "13.4 시각적 등분산성 점검",
    "text": "13.4 시각적 등분산성 점검\n\n13.4.1 (1) 박스플롯\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\n\nsns.boxplot(\n    x=\"species\",\n    y=\"body_mass_g\",\n    data=df\n)\nplt.show()\n\n\n\n\n\n\n\n\n관찰 포인트\n\n박스 높이\nwhisker 길이\n집단 간 퍼짐 차이\n\n\n\n\n13.4.2 (2) 분산 비교 히스토그램\n\nsns.histplot(\n    data=df,\n    x=\"body_mass_g\",\n    hue=\"species\",\n    kde=True\n)\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#통계적-등분산-검정-방법",
    "href": "part2/03. 등분산 검정.html#통계적-등분산-검정-방법",
    "title": "13  등분산 검정",
    "section": "13.5 통계적 등분산 검정 방법",
    "text": "13.5 통계적 등분산 검정 방법\n\n13.5.1 주요 등분산 검정 비교\n\n\n\n검정\n특징\n전제\n\n\n\n\nBartlett\n민감\n정규성 필요\n\n\nLevene\n안정적\n정규성 불필요\n\n\nBrown–Forsythe\n중앙값 기반\n이상치 강건\n\n\nFligner–Killeen\n비모수\n매우 강건",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#levene-검정",
    "href": "part2/03. 등분산 검정.html#levene-검정",
    "title": "13  등분산 검정",
    "section": "13.6 Levene 검정",
    "text": "13.6 Levene 검정\n\n13.6.1 원리\n\n각 관측값과 집단 평균(또는 중앙값) 의 절대편차 비교 (가장 권장)\n\n\n\n13.6.2 예제\n\nfrom scipy.stats import levene\n\ngroups = [\n    df[df[\"species\"] == sp][\"body_mass_g\"]\n    for sp in df[\"species\"].unique()\n]\n\nstat, p_value = levene(*groups)\nstat, p_value\n\n(np.float64(5.134899089832661), np.float64(0.0063670535459093534))\n\n\n판단\n\np ≥ 0.05 → 등분산 가정 가능\np &lt; 0.05 → 이분산",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#bartlett-검정",
    "href": "part2/03. 등분산 검정.html#bartlett-검정",
    "title": "13  등분산 검정",
    "section": "13.7 Bartlett 검정",
    "text": "13.7 Bartlett 검정\n\nfrom scipy.stats import bartlett\n\nstat, p_value = bartlett(*groups)\nstat, p_value\n\n(np.float64(5.692031757944016), np.float64(0.0580752393481068))\n\n\n주의\n\n정규성 위반 시 잘못된 경고\n실제 실무에서는 거의 비권장",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#flignerkilleen-검정",
    "href": "part2/03. 등분산 검정.html#flignerkilleen-검정",
    "title": "13  등분산 검정",
    "section": "13.8 Fligner–Killeen 검정",
    "text": "13.8 Fligner–Killeen 검정\n\nfrom scipy.stats import fligner\n\nstat, p_value = fligner(*groups)\nstat, p_value\n\n(np.float64(9.250672076098088), np.float64(0.009800361188170159))\n\n\n특징\n\n비모수\n꼬리 두꺼운 분포에도 안정적",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성이-깨졌을-때의-선택지",
    "href": "part2/03. 등분산 검정.html#등분산성이-깨졌을-때의-선택지",
    "title": "13  등분산 검정",
    "section": "13.9 등분산성이 깨졌을 때의 선택지",
    "text": "13.9 등분산성이 깨졌을 때의 선택지\n\n13.9.1 대안 검정 사용\n\n\n\n상황\n대안\n\n\n\n\n두 집단 평균\nWelch t-test\n\n\n다집단 평균\nWelch ANOVA\n\n\n전반\n비모수 검정\n\n\n\n\n\n13.9.2 분포 변환\n\n로그\nBox-Cox\nYeo-Johnson\n\n\n\n13.9.3 그룹별 분산 고려 모델\n\n가중 회귀\n일반화 선형 모델",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#정규성-등분산성-관계",
    "href": "part2/03. 등분산 검정.html#정규성-등분산성-관계",
    "title": "13  등분산 검정",
    "section": "13.10 정규성 ↔︎ 등분산성 관계",
    "text": "13.10 정규성 ↔︎ 등분산성 관계\n\n정규성 통과 ≠ 등분산성 통과\nBartlett는 정규성에 의존\nLevene/Fligner는 독립\n\n실무 기본 조합 Q–Q plot + Levene",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html",
    "href": "part2/04. 적합성 및 독립성 검정.html",
    "title": "14  적합성 및 독립성 검정",
    "section": "",
    "text": "14.1 적합성 및 독립성 검정 시점\n평균이 아니라 빈도와 구조를 파악하기 위해 수행한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#적합성-및-독립성-검정-시점",
    "href": "part2/04. 적합성 및 독립성 검정.html#적합성-및-독립성-검정-시점",
    "title": "14  적합성 및 독립성 검정",
    "section": "",
    "text": "이 데이터는 내가 기대한 분포를 따르는가?\n두 범주형 변수는 서로 관련이 있는가?",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#카이제곱χ²-검정-공통-아이디어",
    "href": "part2/04. 적합성 및 독립성 검정.html#카이제곱χ²-검정-공통-아이디어",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.2 카이제곱(χ²) 검정 공통 아이디어",
    "text": "14.2 카이제곱(χ²) 검정 공통 아이디어\n카이제곱 검정 논리:\n\\[\n\\chi^2 = \\sum \\frac{(관측값 - 기대값)^2}{기대값}\n\\]\n핵심 비교\n\n관측 빈도 (Observed)\n기대 빈도 (Expected)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#적합성-검정-goodness-of-fit-test",
    "href": "part2/04. 적합성 및 독립성 검정.html#적합성-검정-goodness-of-fit-test",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.3 적합성 검정 (Goodness-of-Fit Test)",
    "text": "14.3 적합성 검정 (Goodness-of-Fit Test)\n\n14.3.1 개념\n관측된 범주 빈도가 미리 정해진 이론적 분포와 일치하는가?\n\n\n14.3.2 가설 설정\n\n귀무가설 (H_0): 관측 분포 = 기대 분포\n대립가설 (H_1): 관측 분포 ≠ 기대 분포\n\n\n\n14.3.3 예제\nspecies 분포가 균등 분포라고 볼 수 있을까?\n\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import chisquare\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\n\nobserved = df[\"species\"].value_counts().sort_index()\nobserved\n\nspecies\nAdelie       146\nChinstrap     68\nGentoo       119\nName: count, dtype: int64\n\n\n기대 빈도 (균등 가정)\n\nexpected = [observed.sum() / len(observed)] * len(observed)\n\n\nstat, p_value = chisquare(f_obs=observed, f_exp=expected)\nstat, p_value\n\n(np.float64(28.270270270270274), np.float64(7.264217011785265e-07))\n\n\n판단\n\np ≥ 0.05 → 기대 분포와 크게 다르지 않음\np &lt; 0.05 → 분포가 다름\n\n\n\n14.3.4 적합성 검정 예시\n\n주사위 공정성\n범주 비율 검증\n샘플링 결과 검증",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#독립성-검정-test-of-independence",
    "href": "part2/04. 적합성 및 독립성 검정.html#독립성-검정-test-of-independence",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.4 독립성 검정 (Test of Independence)",
    "text": "14.4 독립성 검정 (Test of Independence)\n\n14.4.1 개념\n두 범주형 변수가 서로 독립적인가?\n즉,\n\n하나를 알면\n다른 하나의 분포가 바뀌는가?\n\n\n\n14.4.2 가설 설정\n\n(H_0): 두 변수는 독립\n(H_1): 두 변수는 독립이 아님\n\n\n\n14.4.3 예제\nspecies와 sex는 독립일까?\n\n\n14.4.4 교차표 생성\n\nimport pandas as pd\ncontingency_table = pd.crosstab(\n    df[\"species\"],\n    df[\"sex\"]\n)\ncontingency_table\n\n\n\n\n\n\n\nsex\nFemale\nMale\n\n\nspecies\n\n\n\n\n\n\nAdelie\n73\n73\n\n\nChinstrap\n34\n34\n\n\nGentoo\n58\n61\n\n\n\n\n\n\n\n\n\n14.4.5 카이제곱 독립성 검정\n\nfrom scipy.stats import chi2_contingency\n\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\nchi2, p_value\n\n(np.float64(0.04860717014078318), np.float64(0.9759893689765846))\n\n\n판단\n\np ≥ 0.05 → 독립\np &lt; 0.05 → 관련 있음\n\n\n\n14.4.6 기대 빈도 확인\n\nexpected\n\narray([[72.34234234, 73.65765766],\n       [33.69369369, 34.30630631],\n       [58.96396396, 60.03603604]])\n\n\n주의\n\n기대 빈도 &lt; 5 인 셀이 많으면 → 카이제곱 신뢰도 ↓",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#f-검정",
    "href": "part2/04. 적합성 및 독립성 검정.html#f-검정",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.5 F 검정",
    "text": "14.5 F 검정\n\n14.5.1 F 검정의 의미\n두 집단의 분산이 같은가?\n\\[\nF = \\frac{s_1^2}{s_2^2}\n\\]\n\n등분산성의 가장 기본적 형태\n하지만 정규성에 매우 민감\n\n\n\n14.5.2 예제\n\nfrom scipy.stats import f\n\ngroup1 = df[df[\"species\"] == \"Adelie\"][\"body_mass_g\"]\ngroup2 = df[df[\"species\"] == \"Chinstrap\"][\"body_mass_g\"]\n\nf_stat = group1.var() / group2.var()\nf_stat\n\nnp.float64(1.4239219323005283)\n\n\np-value 계산\n\ndf1 = len(group1) - 1\ndf2 = len(group2) - 1\n\np_value = 1 - f.cdf(f_stat, df1, df2)\np_value\n\nnp.float64(0.052342183408140563)\n\n\n주의\n\nLevene 검정이 훨씬 안전\nF 검정은 이론 설명용",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#검정별-예시",
    "href": "part2/04. 적합성 및 독립성 검정.html#검정별-예시",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.6 검정별 예시",
    "text": "14.6 검정별 예시\n\n\n\n목적\n데이터 유형\n검정\n\n\n\n\n분포 적합성\n범주 1개\nχ² 적합성\n\n\n변수 관계\n범주 × 범주\nχ² 독립성\n\n\n분산 비교\n연속\nF 검정\n\n\n실무 등분산\n연속\nLevene",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#자주-하는-오해",
    "href": "part2/04. 적합성 및 독립성 검정.html#자주-하는-오해",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.7 자주 하는 오해",
    "text": "14.7 자주 하는 오해\n\n“p &lt; 0.05 → 강한 관계” &gt; 관계 존재 여부만 판단\n“카이제곱은 평균 비교” &gt; 빈도 기반 검정\n“카이제곱은 다 쓸 수 있다” &gt; 표본 크기·기대빈도 조건 중요",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#정리",
    "href": "part2/04. 적합성 및 독립성 검정.html#정리",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.8 정리",
    "text": "14.8 정리\n\n적합성 검정 → 분포 자체를 비교\n독립성 검정 → 범주 간 관계\n카이제곱은 빈도의 언어 → 분산 비교\n평균 비교 전에 구조부터 확인하자",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html",
    "href": "part2/05. 평균 비교 검정.html",
    "title": "15  평균 비교 검정",
    "section": "",
    "text": "15.1 평균 비교 검정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#평균-비교-검정",
    "href": "part2/05. 평균 비교 검정.html#평균-비교-검정",
    "title": "15  평균 비교 검정",
    "section": "",
    "text": "15.1.1 평균 비교 검정\n두 개 이상 집단간 평균 차이가 단순한 우연인지 통계적으로 유의한 차이인지를 검정하는 방법이다. 범주형 변수로 집단을 나누고, 연속형 변수의 평균을 비교한다.\n예시\n\n성별에 따라 몸무게 평균이 다른가?\n종(species)에 따라 부리 길이 평균이 다른가?\n처리 전·후 평균이 달라졌는가?\n\n\n\n15.1.2 평균 비교 검정의 종류\n\n\n\n상황\n사용 검정\n\n\n\n\n두 집단 평균 비교\nt-test\n\n\n세 집단 이상 평균 비교\nANOVA\n\n\n동일 대상 전·후 비교\n대응표본 t-test\n\n\n\n\n\n15.1.3 평균 비교 전 반드시 확인할 가정\n\n정규성\n\n각 집단의 데이터가 정규분포를 따른다고 가정\n\n등분산성\n\n집단 간 분산이 동일하다고 가정\n\n독립성\n\n각 관측치는 서로 독립\n\n\n현실에서는 가정이 완벽하지 않아도 표본 수가 충분하면 비교적 강건하게 동작한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#두-집단-평균-비교",
    "href": "part2/05. 평균 비교 검정.html#두-집단-평균-비교",
    "title": "15  평균 비교 검정",
    "section": "15.2 두 집단 평균 비교",
    "text": "15.2 두 집단 평균 비교\n\n15.2.1 독립표본 t-검정\n서로 독립된 두 집단의 평균을 비교한다.\n\n예\n\n남성 vs 여성\n처리군 vs 대조군\n\n\n\n\n15.2.2 가설 설정\n\n귀무가설(H₀)\n\n두 집단의 평균은 같다\n\n대립가설(H₁)\n\n두 집단의 평균은 다르다\n\n\n\n\n15.2.3 예제 코드\n\nimport seaborn as sns\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\n# 데이터 로드\ndf = sns.load_dataset(\"penguins\")\n\n# 필요한 열만 선택 및 결측치 제거\ndf_t = df[[\"sex\", \"body_mass_g\"]].dropna()\n\n# 집단 분리\nmale = df_t[df_t[\"sex\"] == \"Male\"][\"body_mass_g\"]\nfemale = df_t[df_t[\"sex\"] == \"Female\"][\"body_mass_g\"]\n\n# 독립표본 t-test\nt_stat, p_value = ttest_ind(male, female, equal_var=True)\n\nt_stat, p_value\n\n(np.float64(8.541720337994516), np.float64(4.897246751596277e-16))\n\n\n\n\n15.2.4 결과 해석\n\np-value &lt; 유의수준\n\n성별에 따라 평균 몸무게 차이가 있다\n\np-value ≥ 유의수준\n\n평균 차이가 있다고 보기 어렵다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#등분산-가정이-깨진-경우",
    "href": "part2/05. 평균 비교 검정.html#등분산-가정이-깨진-경우",
    "title": "15  평균 비교 검정",
    "section": "15.3 등분산 가정이 깨진 경우",
    "text": "15.3 등분산 가정이 깨진 경우\n\n15.3.1 Welch’s t-test\n\n등분산이 의심되면 equal_var=False 사용\n\n\nt_stat, p_value = ttest_ind(male, female, equal_var=False)\n\nt_stat, p_value\n\n(np.float64(8.554537231165762), np.float64(4.793891255051508e-16))",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#세-집단-이상-평균-비교",
    "href": "part2/05. 평균 비교 검정.html#세-집단-이상-평균-비교",
    "title": "15  평균 비교 검정",
    "section": "15.4 세 집단 이상 평균 비교",
    "text": "15.4 세 집단 이상 평균 비교\n\n15.4.1 일원분산분석(One-way ANOVA)\n세 개 이상의 집단 평균을 동시에 비교한다. “어느 집단이 다른지”가 아니라 적어도 하나의 평균이 다른지를 먼저 검정한다.\n\n\n15.4.2 가설 설정\n\n귀무가설(H₀)\n\n모든 집단의 평균은 같다\n\n대립가설(H₁)\n\n적어도 하나의 집단 평균은 다르다\n\n\n\n\n15.4.3 예제 코드\n\nfrom scipy.stats import f_oneway\n\n# 필요한 열 선택\ndf_a = df[[\"species\", \"bill_length_mm\"]].dropna()\n\n# 집단별 데이터 분리\nadelie = df_a[df_a[\"species\"] == \"Adelie\"][\"bill_length_mm\"]\nchinstrap = df_a[df_a[\"species\"] == \"Chinstrap\"][\"bill_length_mm\"]\ngentoo = df_a[df_a[\"species\"] == \"Gentoo\"][\"bill_length_mm\"]\n\n# ANOVA\nf_stat, p_value = f_oneway(adelie, chinstrap, gentoo)\n\nf_stat, p_value\n\n(np.float64(410.6002550405077), np.float64(2.6946137388895495e-91))\n\n\n\n\n15.4.4 결과 해석\n\np-value &lt; 유의수준\n\n종에 따라 평균 부리 길이에 차이가 있다\n\np-value ≥ 유의수준\n\n평균 차이가 있다고 보기 어렵다\n\n\n\n\n15.4.5 주의점\n\nANOVA 결과는 “어디가 다른지”를 알려주지 않는다. 이후 단계 사후검정(Post-hoc test)이 필요하다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#대응표본-t-검정",
    "href": "part2/05. 평균 비교 검정.html#대응표본-t-검정",
    "title": "15  평균 비교 검정",
    "section": "15.5 대응표본 t-검정",
    "text": "15.5 대응표본 t-검정\n동일한 대상의 전·후 변화 비교하여 차이값의 평균이 0인지 검정한다.\n\n15.5.1 가설 설정\n\n귀무가설(H₀)\n\n전·후 평균 차이는 0이다\n\n대립가설(H₁)\n\n전·후 평균 차이는 0이 아니다\n\n\n\n\n15.5.2 예시 코드\nfrom scipy.stats import ttest_rel\n\nt_stat, p_value = ttest_rel(before, after)\n\npenguins 데이터셋에는 전후 비교가 불가하여 코드 구조만 제시함",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#평균-비교-검정-요약",
    "href": "part2/05. 평균 비교 검정.html#평균-비교-검정-요약",
    "title": "15  평균 비교 검정",
    "section": "15.6 평균 비교 검정 요약",
    "text": "15.6 평균 비교 검정 요약\n\n\n\n상황\n검정 방법\n\n\n\n\n두 독립 집단\n독립표본 t-test\n\n\n등분산 가정 불확실\nWelch t-test\n\n\n세 집단 이상\nANOVA\n\n\n동일 대상 전·후\n대응표본 t-test",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#핵심-포인트",
    "href": "part2/05. 평균 비교 검정.html#핵심-포인트",
    "title": "15  평균 비교 검정",
    "section": "15.7 핵심 포인트",
    "text": "15.7 핵심 포인트\n\n평균 차이 ≠ 의미 있는 차이\np-value는\n\n차이 존재 여부\n크기나 중요도를 직접 말해주지는 않는다.\n\n항상\n\n시각화\n기초 통계량\n가정 점검 을 함께 본다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#anova-이후-사후검정post-hoc-test",
    "href": "part2/05. 평균 비교 검정.html#anova-이후-사후검정post-hoc-test",
    "title": "15  평균 비교 검정",
    "section": "15.8 ANOVA 이후 사후검정(Post-hoc Test)",
    "text": "15.8 ANOVA 이후 사후검정(Post-hoc Test)\n\n15.8.1 사후검정 필요 이유\n\nANOVA 결과\n\np-value &lt; 유의수준\n→ 집단 간 평균 차이가 존재함\n\n하지만\n\n어떤 집단과 어떤 집단 사이가 다른지는 알려주지 않는다.\n\n\n예시\n\nspecies에 따라 부리 길이가 다르다. 하지만\nAdelie vs Chinstrap?\nChinstrap vs Gentoo?\nAdelie vs Gentoo? → 사후검정 필요\n\n\n\n15.8.2 사후검정의 기본 아이디어\n\n모든 집단 쌍(pairwise comparison)을 비교\n단순 t-test를 여러 번 하면 문제 발생\n\n다중 비교 문제 (Multiple Testing Problem)\n우연에 의한 유의 결과가 늘어난다\n\n사후검정은\n\n유의수준을 보정하여 신뢰도를 유지한다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#대표적인-사후검정-방법",
    "href": "part2/05. 평균 비교 검정.html#대표적인-사후검정-방법",
    "title": "15  평균 비교 검정",
    "section": "15.9 대표적인 사후검정 방법",
    "text": "15.9 대표적인 사후검정 방법\n\n\n\n방법\n특징\n사용 상황\n\n\n\n\nTukey HSD\n가장 많이 사용\n등분산 가정 만족\n\n\nBonferroni\n매우 보수적\n비교 횟수가 적을 때\n\n\nScheffé\n매우 보수적\n모든 선형 조합 비교\n\n\nGames-Howell\n등분산 가정 불필요\n분산이 다를 때",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#tukey-hsd-honestly-significant-difference",
    "href": "part2/05. 평균 비교 검정.html#tukey-hsd-honestly-significant-difference",
    "title": "15  평균 비교 검정",
    "section": "15.10 Tukey HSD (Honestly Significant Difference)",
    "text": "15.10 Tukey HSD (Honestly Significant Difference)\n\n15.10.1 개념\n\n모든 집단 쌍의 평균 차이를 비교\n전체 유의수준을 유지하면서 검정\nANOVA 이후 가장 표준적인 선택\n\n\n\n15.10.2 가설 구조\n각 집단 쌍에 대해\n\n귀무가설(H₀)\n\n두 집단의 평균은 같다\n\n대립가설(H₁)\n\n두 집단의 평균은 다르다\n\n\n\n\n15.10.3 분석 목표\n종(species)에 따라 부리 길이(bill_length_mm)의 평균 차이가 어떤 종 간에 존재하는지 확인한다.\n\nimport seaborn as sns\nimport pandas as pd\n\ndf = sns.load_dataset(\"penguins\")\ndf_post = df[[\"species\", \"bill_length_mm\"]].dropna()\n\n\nfrom scipy.stats import f_oneway\n\ngroups = [\n    df_post[df_post[\"species\"] == sp][\"bill_length_mm\"]\n    for sp in df_post[\"species\"].unique()\n]\n\nf_stat, p_value = f_oneway(*groups)\nf_stat, p_value\n\n(np.float64(410.6002550405077), np.float64(2.6946137388895495e-91))\n\n\n\np-value &lt; 유의수준 → 사후검정 진행\n\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\ntukey = pairwise_tukeyhsd(\n    endog=df_post[\"bill_length_mm\"],   # 비교할 수치형 변수\n    groups=df_post[\"species\"],         # 집단 변수\n    alpha=0.05\n)\n\nprint(tukey)\n\n   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n=========================================================\n  group1    group2  meandiff p-adj   lower  upper  reject\n---------------------------------------------------------\n   Adelie Chinstrap  10.0424    0.0  9.0249  11.06   True\n   Adelie    Gentoo   8.7135    0.0  7.8672 9.5598   True\nChinstrap    Gentoo  -1.3289 0.0089 -2.3819 -0.276   True\n---------------------------------------------------------\n\n\n\n\n15.10.4 출력 결과 해석\n출력 테이블 주요 열 의미\n\n\n\n열\n의미\n\n\n\n\ngroup1, group2\n비교되는 집단\n\n\nmeandiff\n평균 차이\n\n\np-adj\n보정된 p-value\n\n\nlower, upper\n평균 차이 신뢰구간\n\n\nreject\n귀무가설 기각 여부\n\n\n\n\n\n15.10.5 해석 예시\n\nreject = True\n\n두 종 간 평균 부리 길이에 유의한 차이 존재\n\nreject = False\n\n평균 차이가 있다고 보기 어렵다\n\n\nANOVA 결과를 구체적인 집단 비교로 분해한 것이 사후검정\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(\n    data=df_post,\n    x=\"species\",\n    y=\"bill_length_mm\"\n)\n\nplt.title(\"Bill Length by Species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n사후검정 결과와 박스플롯 분포를 함께 보면 통계적 결과 + 직관적 이해가 동시에 가능",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#등분산-가정이-깨진-경우-1",
    "href": "part2/05. 평균 비교 검정.html#등분산-가정이-깨진-경우-1",
    "title": "15  평균 비교 검정",
    "section": "15.11 등분산 가정이 깨진 경우",
    "text": "15.11 등분산 가정이 깨진 경우\n\nANOVA 전에 등분산 검정에서 문제가 있었다면\n\nTukey 대신 Games-Howell 권장\n\nstatsmodels 기본 API에는 직접 구현이 없어서\n\npingouin 라이브러리를 자주 사용",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html",
    "href": "part2/06. 분산 분석.html",
    "title": "16  분산 분석",
    "section": "",
    "text": "16.1 분산분석\n분산분석(ANOVA: Analysis of Variance)은 세 개 이상 집단의 평균을 동시에 비교하기 위한 통계적 방법이다. 이름은 분산분석이지만, 관심의 대상은 평균 차이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#일원-분산-분석-one-way-anova",
    "href": "part2/06. 분산 분석.html#일원-분산-분석-one-way-anova",
    "title": "16  분산 분석",
    "section": "16.2 일원 분산 분석 (One-way ANOVA)",
    "text": "16.2 일원 분산 분석 (One-way ANOVA)\n\n16.2.1 개념\n\n하나의 범주형 독립변수(요인)\n하나의 연속형 종속변수\n집단 간 평균 차이가 존재하는지 검정\n\n예시\n\n종(species)에 따라 부리 길이(bill_length_mm)의 평균이 다른가?\n\n\n\n16.2.2 가설 설정\n\n귀무가설(H₀)\n\n모든 집단의 평균은 같다\n\n대립가설(H₁)\n\n적어도 하나의 집단 평균은 다르다\n\n\n\n\n16.2.3 분산분석의 핵심 아이디어\n\n전체 변동 = 집단 간 변동 + 집단 내 변동\n집단 간 변동이 충분히 크다면\n\n평균 차이가 있다고 판단\n\n\n\nimport seaborn as sns\nimport pandas as pd\n\ndf = sns.load_dataset(\"penguins\")\ndf_anova = df[[\"species\", \"bill_length_mm\"]].dropna()\n\n\ndf_anova.groupby(\"species\")[\"bill_length_mm\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n151.0\n38.791391\n2.663405\n32.1\n36.75\n38.80\n40.750\n46.0\n\n\nChinstrap\n68.0\n48.833824\n3.339256\n40.9\n46.35\n49.55\n51.075\n58.0\n\n\nGentoo\n123.0\n47.504878\n3.081857\n40.9\n45.30\n47.30\n49.550\n59.6\n\n\n\n\n\n\n\n\n16.2.3.1 일원 ANOVA 수행\n\nfrom scipy.stats import f_oneway\n\ngroups = [\n    df_anova[df_anova[\"species\"] == sp][\"bill_length_mm\"]\n    for sp in df_anova[\"species\"].unique()\n]\n\nf_stat, p_value = f_oneway(*groups)\nf_stat, p_value\n\n(np.float64(410.6002550405077), np.float64(2.6946137388895495e-91))\n\n\n\n\n\n16.2.4 결과 해석\n\np-value &lt; 유의수준(0.05)\n\n종에 따라 평균 부리 길이에 차이가 있다\n\np-value ≥ 유의수준\n\n평균 차이가 있다고 보기 어렵다\n\n\n중요: 어느 종 간 차이인지는 아직 모른다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#이원-분산-분석-two-way-anova",
    "href": "part2/06. 분산 분석.html#이원-분산-분석-two-way-anova",
    "title": "16  분산 분석",
    "section": "16.3 이원 분산 분석 (Two-way ANOVA)",
    "text": "16.3 이원 분산 분석 (Two-way ANOVA)\n\n16.3.1 개념\n\n두 개의 범주형 독립변수\n하나의 연속형 종속변수\n세 가지 효과를 동시에 검정\n\n요인 A의 주효과\n요인 B의 주효과\nA × B 상호작용 효과\n\n\n\n\n16.3.2 예시\n\nspecies(종)\nsex(성별)\n→ 부리 길이에 어떤 영향을 주는가?\n\n\n\n16.3.3 가설 구조\n\nspecies 효과\n\n종에 따른 평균 차이가 있는가?\n\nsex 효과\n\n성별에 따른 평균 차이가 있는가?\n\n상호작용 효과\n\n종에 따른 성별 효과가 달라지는가?\n\n\n\ndf_two = df[[\"species\", \"sex\", \"bill_length_mm\"]].dropna()\n\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nmodel = ols(\n    \"bill_length_mm ~ C(species) + C(sex) + C(species):C(sex)\",\n    data=df_two\n).fit()\n\nanova_table = sm.stats.anova_lm(model, typ=2)\nanova_table\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(species)\n6975.591607\n2.0\n650.478579\n1.059087e-114\n\n\nC(sex)\n1135.683888\n1.0\n211.806563\n2.422971e-37\n\n\nC(species):C(sex)\n24.494427\n2.0\n2.284122\n1.034865e-01\n\n\nResidual\n1753.338642\n327.0\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n16.3.4 결과 해석 포인트\n\nC(species)\n\n종의 주효과\n\nC(sex)\n\n성별의 주효과\n\nC(species):C(sex)\n\n상호작용 효과\n\n\n상호작용이 유의하면 → “성별 차이가 종마다 다르게 나타난다”",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#사후-검정-post-hoc-test",
    "href": "part2/06. 분산 분석.html#사후-검정-post-hoc-test",
    "title": "16  분산 분석",
    "section": "16.4 사후 검정 (Post-hoc Test)",
    "text": "16.4 사후 검정 (Post-hoc Test)\n\n16.4.1 사후 검정 필요 이유\n\nANOVA 결과\n\n평균 차이가 존재함은 알 수 있음\n\n하지만\n\n어느 집단 간 차이인지 알 수 없음\n\n\n\n\n16.4.2 대표적인 사후검정 방법\n\nTukey HSD (가장 일반적)\nBonferroni\nScheffé\nGames-Howell (등분산 가정이 약할 때)\n\n\n\n16.4.3 Tukey HSD\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\ntukey = pairwise_tukeyhsd(\n    endog=df_anova[\"bill_length_mm\"],\n    groups=df_anova[\"species\"],\n    alpha=0.05\n)\n\nprint(tukey)\n\n   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n=========================================================\n  group1    group2  meandiff p-adj   lower  upper  reject\n---------------------------------------------------------\n   Adelie Chinstrap  10.0424    0.0  9.0249  11.06   True\n   Adelie    Gentoo   8.7135    0.0  7.8672 9.5598   True\nChinstrap    Gentoo  -1.3289 0.0089 -2.3819 -0.276   True\n---------------------------------------------------------\n\n\n\n16.4.3.1 결과 해석\n\nreject = True\n\n두 집단 간 평균 차이가 유의함\n\nreject = False\n\n평균 차이가 통계적으로 유의하지 않음\n\n\n\n\n16.4.3.2 시각화와 함께 보기\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.boxplot(\n    data=df_anova,\n    x=\"species\",\n    y=\"bill_length_mm\"\n)\n\nplt.title(\"Bill Length by Species\")\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#분산분석-사용-시-주의사항",
    "href": "part2/06. 분산 분석.html#분산분석-사용-시-주의사항",
    "title": "16  분산 분석",
    "section": "16.5 분산분석 사용 시 주의사항",
    "text": "16.5 분산분석 사용 시 주의사항\n\n독립성 가정\n정규성 가정\n등분산성 가정\n가정이 깨질 경우\n\n비모수 방법(Kruskal–Wallis)\n또는 변환(log, Box-Cox 등) 고려",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html",
    "href": "part2/07. 비모수 검정.html",
    "title": "17  비모수 검정",
    "section": "",
    "text": "17.1 비모수 검정\n비모수 검정(Non-parametric Tests)은\n순위(rank) 정보를 이용해 집단 간 차이를 검정한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#비모수-검정",
    "href": "part2/07. 비모수 검정.html#비모수-검정",
    "title": "17  비모수 검정",
    "section": "",
    "text": "정규성 가정이 만족되지 않거나\n표본 수가 작거나\n이상치의 영향이 큰 경우",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#비모수-검정이-필요한-상황",
    "href": "part2/07. 비모수 검정.html#비모수-검정이-필요한-상황",
    "title": "17  비모수 검정",
    "section": "17.2 비모수 검정이 필요한 상황",
    "text": "17.2 비모수 검정이 필요한 상황\n\n정규성 검정에서 귀무가설 기각\n박스플롯에서 극단값 다수 존재\n평균보다 중앙값 비교가 더 적절\n측정 단위가 순서형(ordinal)\n\n데이터 분포에 덜 민감하지만 해석은 평균이 아닌 “분포의 위치 차이”",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#mannwhitney-u-검정",
    "href": "part2/07. 비모수 검정.html#mannwhitney-u-검정",
    "title": "17  비모수 검정",
    "section": "17.3 Mann–Whitney U 검정",
    "text": "17.3 Mann–Whitney U 검정\n\n17.3.1 개념\n\n두 독립 집단 비교\nt-test의 비모수 대안\n두 집단의 분포 위치가 같은지 검정\n\n\n\n17.3.2 가설 설정\n\nH₀: 두 집단의 분포는 동일하다\nH₁: 두 집단의 분포는 다르다\n\n\n17.3.2.1 분석 목표\n\nAdelie vs Gentoo\n부리 길이(bill_length_mm) 차이가 있는가?\n\n\nimport seaborn as sns\nfrom scipy.stats import mannwhitneyu\n\ndf = sns.load_dataset(\"penguins\")\n\ng1 = df[df[\"species\"] == \"Adelie\"][\"bill_length_mm\"].dropna()\ng2 = df[df[\"species\"] == \"Gentoo\"][\"bill_length_mm\"].dropna()\n\n\nu_stat, p_value = mannwhitneyu(g1, g2, alternative=\"two-sided\")\nu_stat, p_value\n\n(np.float64(224.5), np.float64(7.279301009021372e-44))\n\n\n\np-value &lt; 0.05\n\n두 종의 부리 길이 분포는 다르다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#wilcoxon-순위합-검정-wilcoxon-signed-rank-test",
    "href": "part2/07. 비모수 검정.html#wilcoxon-순위합-검정-wilcoxon-signed-rank-test",
    "title": "17  비모수 검정",
    "section": "17.4 Wilcoxon 순위합 검정 (Wilcoxon Signed-Rank Test)",
    "text": "17.4 Wilcoxon 순위합 검정 (Wilcoxon Signed-Rank Test)\n\n17.4.1 개념\n\n대응 표본 비교\npaired t-test의 비모수 대안\n동일 대상의 전·후 비교에 사용\n\n\n\n17.4.2 가설 설정\n\nH₀: 차이의 중앙값은 0이다\nH₁: 차이의 중앙값은 0이 아니다\n\n\n\n17.4.3 예제 (가상 전·후 데이터)\n\npalmerpenguins에는 자연스러운 전후 데이터가 없으므로 예제용으로 생성\n\n\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\nnp.random.seed(0)\n\nbefore = df[df[\"species\"] == \"Adelie\"][\"bill_length_mm\"].dropna().sample(30)\nafter = before + np.random.normal(0.5, 1.0, size=len(before))\n\n\nw_stat, p_value = wilcoxon(before, after)\nw_stat, p_value\n\n(np.float64(99.0), np.float64(0.0050126127898693085))\n\n\n\np-value &lt; 0.05\n\n전·후 차이가 유의하다",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#kruskalwallis-검정",
    "href": "part2/07. 비모수 검정.html#kruskalwallis-검정",
    "title": "17  비모수 검정",
    "section": "17.5 Kruskal–Wallis 검정",
    "text": "17.5 Kruskal–Wallis 검정\n\n17.5.1 개념\n\n세 개 이상 독립 집단\n일원 ANOVA의 비모수 대안\n집단 간 분포 차이 검정\n\n\n\n17.5.2 가설 설정\n\nH₀: 모든 집단의 분포는 동일하다\nH₁: 적어도 하나의 집단 분포가 다르다\n\n\nfrom scipy.stats import kruskal\n\ngroups = [\n    df[df[\"species\"] == sp][\"bill_length_mm\"].dropna()\n    for sp in df[\"species\"].unique()\n]\n\nh_stat, p_value = kruskal(*groups)\nh_stat, p_value\n\n(np.float64(244.13671803364164), np.float64(9.691371997194331e-54))\n\n\n\np-value &lt; 0.05\n\n종 간 부리 길이 분포에 차이가 있다\n\n\n어느 집단 간 차이인지는 추가 검정 필요 → 사후 검정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#friedman-검정",
    "href": "part2/07. 비모수 검정.html#friedman-검정",
    "title": "17  비모수 검정",
    "section": "17.6 Friedman 검정",
    "text": "17.6 Friedman 검정\n\n17.6.1 개념\n\n세 개 이상 대응 집단\n반복 측정 ANOVA의 비모수 대안\n동일 대상에 대해 여러 조건 비교\n\n\n\n17.6.2 예제 (가상 반복 측정 데이터)\n\nfrom scipy.stats import friedmanchisquare\n\nnp.random.seed(1)\n\ncond1 = before\ncond2 = before + np.random.normal(0.3, 1.0, size=len(before))\ncond3 = before + np.random.normal(0.8, 1.0, size=len(before))\n\n\nf_stat, p_value = friedmanchisquare(cond1, cond2, cond3)\nf_stat, p_value\n\n(np.float64(15.800000000000011), np.float64(0.0003707435404590862))\n\n\n\n\n17.6.3 해석\n\np-value &lt; 0.05\n\n조건 간 분포 차이가 존재",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#모수-vs-비모수-검정-정리",
    "href": "part2/07. 비모수 검정.html#모수-vs-비모수-검정-정리",
    "title": "17  비모수 검정",
    "section": "17.7 모수 vs 비모수 검정 정리",
    "text": "17.7 모수 vs 비모수 검정 정리\n\n\n\n상황\n모수 검정\n비모수 검정\n\n\n\n\n두 독립 집단\nt-test\nMann–Whitney U\n\n\n두 대응 집단\npaired t-test\nWilcoxon\n\n\n세 집단 이상\nANOVA\nKruskal–Wallis\n\n\n반복 측정\nRM ANOVA\nFriedman\n\n\n\n\n비모수 검정은\n\n평균 비교가 아니라 순위 기반 분포 비교\n\n가정이 깨졌을 때의 안전한 대안\n시각화 → 정규성/등분산 검정 → 비모수 여부 판단",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html",
    "href": "part2/08. 상관 분석.html",
    "title": "18  상관 분석",
    "section": "",
    "text": "18.1 상관 분석\n상관 분석(Correlation Analysis)은 두 연속형 변수 간의 관계의 방향과 강도를 수치로 표현하는 방법이다. 인과관계를 의미하지는 않는다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#상관계수-기본-개념",
    "href": "part2/08. 상관 분석.html#상관계수-기본-개념",
    "title": "18  상관 분석",
    "section": "18.2 상관계수 기본 개념",
    "text": "18.2 상관계수 기본 개념\n\n값의 범위: -1 ~ +1\n해석\n\n+1: 완전한 양의 상관\n0: 선형 관계 없음\n-1: 완전한 음의 상관\n\n\n\n18.2.1 주의사항\n\n상관관계 ≠ 인과관계\n이상치에 민감할 수 있음\n관계가 비선형이면 값이 작게 나올 수 있음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#pearson-상관-분석",
    "href": "part2/08. 상관 분석.html#pearson-상관-분석",
    "title": "18  상관 분석",
    "section": "18.3 Pearson 상관 분석",
    "text": "18.3 Pearson 상관 분석\n\n18.3.1 개념\n\n선형(linear) 관계 측정\n평균과 분산을 기반으로 계산\n정규성 가정이 비교적 중요\n\n\n\n18.3.2 사용 조건\n\n연속형 변수\n선형 관계\n극단값이 많지 않음\n\n\n\n18.3.3 가설 설정\n\nH₀: 두 변수 간 선형 상관이 없다 (ρ = 0)\nH₁: 두 변수 간 선형 상관이 있다 (ρ ≠ 0)\n\n\n\n18.3.4 예제\n\n18.3.4.1 분석 목표\n\n부리 길이(bill_length_mm)와 부리 깊이(bill_depth_mm) 사이의 관계\n\n\nimport seaborn as sns\nfrom scipy.stats import pearsonr\n\ndf = sns.load_dataset(\"penguins\")\n\nx = df[\"bill_length_mm\"]\ny = df[\"bill_depth_mm\"]\n\ndf_corr = df[[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n\n\n\n18.3.4.2 Pearson 상관계수 계산\n\nr, p_value = pearsonr(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\nr, p_value\n\n(np.float64(-0.2350528703555327), np.float64(1.1196621961373343e-05))\n\n\n\n\n\n18.3.5 결과 해석\n\nr &gt; 0\n\n양의 선형 관계\n\n|r|가 클수록\n\n관계가 강함\n\np-value &lt; 0.05\n\n상관관계가 통계적으로 유의함",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#spearman-순위-상관-분석",
    "href": "part2/08. 상관 분석.html#spearman-순위-상관-분석",
    "title": "18  상관 분석",
    "section": "18.4 Spearman 순위 상관 분석",
    "text": "18.4 Spearman 순위 상관 분석\n\n18.4.1 개념\n\n순위(rank) 기반 상관 분석\n비선형이지만 단조(monotonic) 관계면 탐지 가능\n비모수적 방법\n\n\n\n18.4.2 언제 사용하는가?\n\n정규성 가정이 깨졌을 때\n이상치가 많은 경우\n값의 크기보다 순서가 중요할 때\n\n\n\n18.4.3 가설 설정\n\nH₀: 두 변수 간 순위 상관이 없다\nH₁: 두 변수 간 순위 상관이 있다\n\n\n\n18.4.4 예제\n\nfrom scipy.stats import spearmanr\n\nrho, p_value = spearmanr(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\nrho, p_value\n\n(np.float64(-0.22174915179457863), np.float64(3.511539739648999e-05))\n\n\n\n\n18.4.5 Pearson vs Spearman 비교\n\n\n\n항목\nPearson\nSpearman\n\n\n\n\n기반\n실제 값\n순위\n\n\n관계 형태\n선형\n단조\n\n\n이상치 영향\n큼\n작음\n\n\n정규성\n중요\n덜 중요",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#상관-행렬correlation-matrix",
    "href": "part2/08. 상관 분석.html#상관-행렬correlation-matrix",
    "title": "18  상관 분석",
    "section": "18.5 상관 행렬(Correlation Matrix)",
    "text": "18.5 상관 행렬(Correlation Matrix)\n\n18.5.1 여러 변수 간 관계 한 번에 보기\n\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ncorr_matrix = df[num_cols].corr(method=\"pearson\")\ncorr_matrix\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\nbill_length_mm\n1.000000\n-0.235053\n0.656181\n0.595110\n\n\nbill_depth_mm\n-0.235053\n1.000000\n-0.583851\n-0.471916\n\n\nflipper_length_mm\n0.656181\n-0.583851\n1.000000\n0.871202\n\n\nbody_mass_g\n0.595110\n-0.471916\n0.871202\n1.000000\n\n\n\n\n\n\n\n\n\n18.5.2 시각화\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    cmap=\"coolwarm\",\n    fmt=\".2f\"\n)\n\nplt.title(\"Correlation Matrix of Penguins\")\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#해석-팁",
    "href": "part2/08. 상관 분석.html#해석-팁",
    "title": "18  상관 분석",
    "section": "18.6 해석 팁",
    "text": "18.6 해석 팁\n\n상관계수 절대값 기준(경험적)\n\n0.1 ~ 0.3: 약함\n0.3 ~ 0.5: 중간\n0.5 이상: 강함\n\n항상 산점도와 함께 확인\n상관이 높다고 변수 제거를 바로 결정하지 말 것",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#정리",
    "href": "part2/08. 상관 분석.html#정리",
    "title": "18  상관 분석",
    "section": "18.7 정리",
    "text": "18.7 정리\n\nPearson\n\n선형 관계 분석의 기본\n\nSpearman\n\n분포·이상치에 강한 대안\n\n상관 분석은\n\nEDA\n피처 선택\n회귀 모델링의 출발점",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#kendall-순위-상관",
    "href": "part2/08. 상관 분석.html#kendall-순위-상관",
    "title": "18  상관 분석",
    "section": "18.8 Kendall 순위 상관",
    "text": "18.8 Kendall 순위 상관\nKendall 순위 상관(Kendall’s τ)은 순위 기반 상관 분석 방법이다.\n\n18.8.1 개념\n\n순위 기반 상관 분석\nSpearman과 유사하지만 계산 방식이 다름\n두 변수의 순위 쌍이 얼마나 일관되게 증가/감소하는지 측정\n\n\n\n18.8.2 직관적 설명\n\n관측치 쌍 (xᵢ, yᵢ), (xⱼ, yⱼ)에 대해\n\nx와 y가 같은 방향으로 증가/감소 → concordant\n방향이 다르면 → discordant\n\nKendall τ는\n\n(일치 쌍 − 불일치 쌍) / 전체 쌍\n\n\n“순서가 얼마나 자주 같은 방향으로 움직이는가?”\n\n\n18.8.3 값의 범위\n\n-1 ~ +1\nSpearman보다 절댓값이 작게 나오는 경향\n대신 해석은 더 보수적이고 안정적\n\n\n\n18.8.4 언제 사용하는가?\n\n표본 수가 작을 때\n동일 값(ties)이 많을 때\n순서 정보가 매우 중요한 경우\n비선형 + 단조 관계\n\n\n\n18.8.5 예제\n\nfrom scipy.stats import kendalltau\n\ntau, p_value = kendalltau(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\ntau, p_value\n\n(np.float64(-0.12285019226399843), np.float64(0.0007864168450192424))\n\n\n\n\n18.8.6 해석\n\nτ &gt; 0\n\n순위가 함께 증가하는 경향\n\np-value &lt; 0.05\n\n순위 상관이 통계적으로 유의함",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#pearson-spearman-kendall-비교-정리",
    "href": "part2/08. 상관 분석.html#pearson-spearman-kendall-비교-정리",
    "title": "18  상관 분석",
    "section": "18.9 Pearson · Spearman · Kendall 비교 정리",
    "text": "18.9 Pearson · Spearman · Kendall 비교 정리\n\n\n\n구분\nPearson\nSpearman\nKendall\n\n\n\n\n기반\n실제 값\n순위\n순위 쌍\n\n\n관계 형태\n선형\n단조\n단조\n\n\n이상치 영향\n큼\n중간\n작음\n\n\n정규성 필요\n높음\n낮음\n낮음\n\n\n값 크기\n가장 큼\n중간\n가장 작음\n\n\n해석 안정성\n낮음\n중간\n높음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html",
    "href": "part3/01. 데이터 분할 및 검정.html",
    "title": "19  데이터 분할 및 검정",
    "section": "",
    "text": "19.1 데이터 분할 및 검증\n머신러닝 모델의 성능은 모델 자체보다 데이터를 어떻게 나누고 검증했는지에 더 크게 좌우된다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#왜-데이터-분할이-필요한가",
    "href": "part3/01. 데이터 분할 및 검정.html#왜-데이터-분할이-필요한가",
    "title": "19  데이터 분할 및 검정",
    "section": "19.2 왜 데이터 분할이 필요한가?",
    "text": "19.2 왜 데이터 분할이 필요한가?\n모델은 학습 데이터에 최적화된다. 따라서 동일 데이터를 평가에 사용하면\n\n성능이 과대평가(overfitting)\n보지 않은 데이터에서의 성능을 추정해야 한다\n\n즉, 일반화 성능(generalization performance) 평가가 목적이다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#학습-검증-테스트-분할",
    "href": "part3/01. 데이터 분할 및 검정.html#학습-검증-테스트-분할",
    "title": "19  데이터 분할 및 검정",
    "section": "19.3 학습 · 검증 · 테스트 분할",
    "text": "19.3 학습 · 검증 · 테스트 분할\n\n19.3.1 기본 개념\n\n\n\n구분\n역할\n\n\n\n\n학습(train)\n모델 학습\n\n\n검증(validation)\n하이퍼파라미터 튜닝\n\n\n테스트(test)\n최종 성능 평가\n\n\n\n\n\n19.3.2 일반적인 분할 비율\n\n60 / 20 / 20\n70 / 15 / 15\n80 / 20 (검증을 교차 검증으로 대체하는 경우)\n\n\n\n19.3.3 예제\n\n19.3.3.1 분석 목표\n\nspecies 분류\n수치형 변수를 사용한 기본 모델링\n\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\ndf = sns.load_dataset(\"penguins\")\ndf_ml = df.dropna()\n\nX = df_ml[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n]\ny = df_ml[\"species\"]\n\n\n\n\n19.3.4 학습 / 테스트 분할\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n\nstratify\n\n클래스 비율 유지\n분류 문제에서 매우 중요\n\n\n\n\n19.3.5 학습 / 검증 분할\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_train, y_train,\n    test_size=0.25,  # 전체 기준 0.2\n    random_state=42,\n    stratify=y_train\n)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#교차-검증-cross-validation",
    "href": "part3/01. 데이터 분할 및 검정.html#교차-검증-cross-validation",
    "title": "19  데이터 분할 및 검정",
    "section": "19.4 교차 검증 (Cross-Validation)",
    "text": "19.4 교차 검증 (Cross-Validation)\n\n19.4.1 개념\n\n데이터를 여러 번 나누어\n학습과 검증을 반복\n성능의 평균과 분산을 함께 평가\n\n단일 분할의 운(luck)에 의존하지 않음",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#k-fold-교차-검증",
    "href": "part3/01. 데이터 분할 및 검정.html#k-fold-교차-검증",
    "title": "19  데이터 분할 및 검정",
    "section": "19.5 K-Fold 교차 검증",
    "text": "19.5 K-Fold 교차 검증\n\n19.5.1 원리\n\n데이터를 K개 폴드로 분할\nK-1개로 학습, 1개로 검증\nK번 반복\n\n\n\n19.5.2 시각적 개념\nFold 1 | V T T ... T T\nFold 2 | T V T ... T T\nFold 3 | T T V ... T T\n...\nFold K | T T T ... T V\n\n\n19.5.3 기본 K-Fold 예제\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(max_iter=1000)\n\nkf = KFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=42\n)\n\nscores = cross_val_score(\n    model, X, y,\n    cv=kf,\n    scoring=\"accuracy\"\n)\n\nscores, scores.mean()\n\n(array([0.98507463, 0.97014925, 0.98507463, 1.        , 0.98484848]),\n np.float64(0.9850293984622344))",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#stratified-k-fold",
    "href": "part3/01. 데이터 분할 및 검정.html#stratified-k-fold",
    "title": "19  데이터 분할 및 검정",
    "section": "19.6 Stratified K-Fold",
    "text": "19.6 Stratified K-Fold\n\n19.6.1 왜 필요한가?\n\n분류 문제에서 클래스 불균형이 있으면\n\n폴드마다 클래스 비율이 달라질 수 있음\n\n\n각 폴드에 클래스 비율을 유지\n\n\n19.6.2 예제\n\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=42\n)\n\nscores = cross_val_score(\n    model, X, y,\n    cv=skf,\n    scoring=\"accuracy\"\n)\n\nscores, scores.mean()\n\n(array([0.97014925, 0.98507463, 0.98507463, 1.        , 0.98484848]),\n np.float64(0.9850293984622344))",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#교차-검증-vs-홀드아웃-검증",
    "href": "part3/01. 데이터 분할 및 검정.html#교차-검증-vs-홀드아웃-검증",
    "title": "19  데이터 분할 및 검정",
    "section": "19.7 교차 검증 vs 홀드아웃 검증",
    "text": "19.7 교차 검증 vs 홀드아웃 검증\n\n\n\n항목\n홀드아웃\n교차 검증\n\n\n\n\n계산 비용\n낮음\n높음\n\n\n안정성\n낮음\n높음\n\n\n데이터 소량\n부적합\n적합\n\n\n실무 사용\n빠른 실험\n최종 평가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#시계열-데이터의-주의점",
    "href": "part3/01. 데이터 분할 및 검정.html#시계열-데이터의-주의점",
    "title": "19  데이터 분할 및 검정",
    "section": "19.8 시계열 데이터의 주의점",
    "text": "19.8 시계열 데이터의 주의점\n\n미래 데이터를 학습에 사용하면 안 됨\n반드시 시간 순서 유지\n\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(n_splits=5)\n\n일반 K-Fold 사용 금지",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#nested-cross-validation",
    "href": "part3/01. 데이터 분할 및 검정.html#nested-cross-validation",
    "title": "19  데이터 분할 및 검정",
    "section": "19.9 Nested Cross-Validation",
    "text": "19.9 Nested Cross-Validation\n\n19.9.1 개념\n\n하이퍼파라미터 튜닝과 모델 평가를 분리 및 성능 과대평가 방지",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#체크리스트",
    "href": "part3/01. 데이터 분할 및 검정.html#체크리스트",
    "title": "19  데이터 분할 및 검정",
    "section": "19.10 체크리스트",
    "text": "19.10 체크리스트\n\n분류 문제인가?\n\nstratify 사용\n\n데이터가 적은가?\n\n교차 검증\n\n하이퍼파라미터 튜닝 중인가?\n\n검증 데이터 분리\n\n최종 성능 보고인가?\n\n테스트 데이터는 마지막까지 보존",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검정</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html",
    "href": "part3/02. 특성 선택.html",
    "title": "20  특성 선택",
    "section": "",
    "text": "20.1 특성 선택",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#특성-선택",
    "href": "part3/02. 특성 선택.html#특성-선택",
    "title": "20  특성 선택",
    "section": "",
    "text": "20.1.1 특성 선택이란?\n특성 선택(feature selection)이란 모델 학습에 사용되는 여러 입력 변수(특성) 중에서 중요한 특성만 선별하는 과정이다.\n\n모든 변수를 사용하는 것이 항상 좋은 것은 아님\n불필요하거나 중복된 특성은 오히려 성능을 저하시킬 수 있음\n\n\n20.1.1.1 특성 선택의 목적\n\n모델 성능 향상 (과적합 감소)\n학습 시간 단축\n모델 해석력 향상\n데이터 수집 및 관리 비용 감소\n\n\n\n\n20.1.2 특성 선택 vs 특성 추출\n혼동하기 쉬운 개념이므로 먼저 구분한다.\n\n특성 선택\n\n기존 변수 중 일부를 선택\n변수의 의미가 유지됨\n\n특성 추출\n\n기존 변수를 변환하여 새로운 변수 생성\n예: PCA",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#필터filter-방법",
    "href": "part3/02. 특성 선택.html#필터filter-방법",
    "title": "20  특성 선택",
    "section": "20.2 필터(Filter) 방법",
    "text": "20.2 필터(Filter) 방법\n\n20.2.1 개념\n모델을 사용하지 않고 데이터의 통계적 특성만으로 특성을 평가하는 방법이다. 각 특성을 독립적으로 평가한다.\n\n\n20.2.2 주요 특징\n\n계산 속도가 빠름\n모델과 무관\n변수 간 상호작용은 고려하지 못함\n\n\n\n20.2.3 대표적인 필터 방법\n\n20.2.3.1 (1) 분산 기반 선택\n\n분산이 거의 없는 변수는 정보량이 적음\n예: 대부분 값이 동일한 변수 제거\n\n\n\n20.2.3.2 (2) 상관계수 기반 선택\n\n입력 변수와 타깃 변수 간 상관관계 측정\nPearson, Spearman, Kendall 등 활용 가능\n절댓값이 큰 변수일수록 중요\n\n\n\n20.2.3.3 (3) 통계적 검정\n\n분류 문제\n\n카이제곱 검정\n\n회귀 문제\n\nF-test\n\n\n\n\n\n20.2.4 필터 방법 예제\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nselector = SelectKBest(score_func=f_classif, k=3)\nX_selected = selector.fit_transform(X, y)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#래퍼wrapper-방법",
    "href": "part3/02. 특성 선택.html#래퍼wrapper-방법",
    "title": "20  특성 선택",
    "section": "20.3 래퍼(Wrapper) 방법",
    "text": "20.3 래퍼(Wrapper) 방법\n\n20.3.1 개념\n모델의 성능을 기준으로 특성 조합을 평가하는 방법으로 특정 모델에 최적화된 특성 선택이 가능하다.\n\n\n20.3.2 주요 특징\n\n성능은 좋지만 계산 비용이 큼\n모델에 종속적\n\n\n\n20.3.3 대표적인 래퍼 방법\n\n20.3.3.1 (1) 전진 선택 (Forward Selection)\n\n특성이 없는 상태에서 시작\n하나씩 추가하면서 성능 개선 여부 평가\n\n\n\n20.3.3.2 (2) 후진 제거 (Backward Elimination)\n\n모든 특성으로 시작\n하나씩 제거하면서 성능 변화 확인\n\n\n\n20.3.3.3 (3) 재귀적 특성 제거 (RFE)\n\n모델 학습 → 중요도 낮은 특성 제거 반복\n\n\n\n\n20.3.4 래퍼 방법 예제\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select=3)\nX_selected = rfe.fit_transform(X, y)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#임베디드embedded-방법",
    "href": "part3/02. 특성 선택.html#임베디드embedded-방법",
    "title": "20  특성 선택",
    "section": "20.4 3.2.5 임베디드(Embedded) 방법",
    "text": "20.4 3.2.5 임베디드(Embedded) 방법\n\n20.4.1 개념\n모델 학습 과정 자체에 특성 선택이 포함된 방법이다. 필터와 래퍼의 중간 성격을 가진다.\n\n\n20.4.2 주요 특징\n\n계산 효율과 성능의 균형\n특정 모델에 내장된 방식\n\n\n\n20.4.3 대표적인 임베디드 방법\n\n20.4.3.1 (1) 정규화 기반 방법\n\nL1 정규화 (Lasso)\n\n중요하지 않은 특성의 계수를 0으로 만듦\n\nL2 정규화 (Ridge)\n\n계수 크기를 줄이지만 제거하지는 않음\n\n\n\n\n20.4.3.2 (2) 트리 기반 모델\n\n결정트리\n랜덤 포레스트\nGradient Boosting\n\n→ 특성 중요도(feature importance) 제공\n\n\n\n20.4.4 임베디드 방법 예제\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\n\nimportances = model.feature_importances_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#방법별-비교-정리",
    "href": "part3/02. 특성 선택.html#방법별-비교-정리",
    "title": "20  특성 선택",
    "section": "20.5 방법별 비교 정리",
    "text": "20.5 방법별 비교 정리\n\n필터 방법\n\n빠름\n모델 독립적\n상호작용 고려 어려움\n\n래퍼 방법\n\n성능 우수\n계산 비용 큼\n모델 의존적\n\n임베디드 방법\n\n효율과 성능의 균형\n특정 알고리즘에 종속",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#활용-전략",
    "href": "part3/02. 특성 선택.html#활용-전략",
    "title": "20  특성 선택",
    "section": "20.6 활용 전략",
    "text": "20.6 활용 전략\n\n데이터가 크고 특성이 많을 때\n\n필터 → 임베디드 순서\n\n모델 성능이 중요한 경우\n\n래퍼 또는 임베디드\n\n해석이 중요한 경우\n\n필터 또는 Lasso 기반 접근",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html",
    "href": "part3/03. 차원 축소.html",
    "title": "21  차원 축소",
    "section": "",
    "text": "21.1 차원 축소",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#차원-축소",
    "href": "part3/03. 차원 축소.html#차원-축소",
    "title": "21  차원 축소",
    "section": "",
    "text": "21.1.1 차원 축소란\n차원 축소(Dimensionality Reduction)란 여러 개의 입력 변수(차원)를 더 적은 수의 변수로 변환하는 과정이다. 원본 특성을 그대로 쓰지 않고 * 새로운 특성(축)을 만들어 데이터를 표현하는 방법이다. 참고로 특성 추출(feature extraction)을 포함하는 개념이다.\n\n\n21.1.2 차원 축소가 필요한 이유\n\n고차원 데이터의 시각화 어려움\n차원의 저주(curse of dimensionality)\n모델 학습 시간 증가\n잡음(noise) 포함 가능성 증가\n\n\n21.1.2.1 기대 효과\n\n데이터 구조 단순화\n학습 속도 향상\n일반화 성능 개선\n시각화 가능 (2D, 3D)\n\n\n\n\n21.1.3 특성 선택 vs 특성 추출\n\n특성 선택\n\n기존 변수 중 일부 선택\n변수 의미 유지\n\n특성 추출\n\n기존 변수를 조합하여 새로운 변수 생성\n변수 의미는 직관적으로 해석하기 어려움\n\n\n차원 축소 = 특성 추출의 대표적인 방법",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#선형-차원-축소",
    "href": "part3/03. 차원 축소.html#선형-차원-축소",
    "title": "21  차원 축소",
    "section": "21.2 선형 차원 축소",
    "text": "21.2 선형 차원 축소\n\n21.2.1 PCA\nPCA(Principal Component Analysis)는 대표적인 선형 차원 축소 방법이다.\n\n21.2.1.1 기본 개념\n\n데이터의 분산을 가장 잘 설명하는 방향을 찾음\n기존 변수의 선형 결합으로 새로운 축 생성\n새 축을 주성분(Principal Component)이라 부름\n\n\n\n\n21.2.2 PCA의 직관적 이해\n\n데이터가 가장 길게 퍼져 있는 방향이 1번 주성분\n그 다음으로 퍼진 방향이 2번 주성분\n서로 직교(독립)\n\n\n\n21.2.3 PCA의 특징\n\n선형 변환\n변수 간 상관관계 제거\n스케일에 민감 → 표준화 필요\n\n\n\n21.2.4 PCA 주요 용어\n\n설명 분산 비율 (explained variance ratio)\n\n각 주성분이 설명하는 정보량 비율\n\n누적 설명 분산을 기준으로 차원 수 결정\n\n\n\n21.2.5 PCA 예제\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\npca.explained_variance_ratio_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#비선형-차원-축소",
    "href": "part3/03. 차원 축소.html#비선형-차원-축소",
    "title": "21  차원 축소",
    "section": "21.3 비선형 차원 축소",
    "text": "21.3 비선형 차원 축소\n선형 변환으로는 복잡한 데이터 구조를 표현하기 어려운 경우 사용한다\n\n데이터의 국소 구조(local structure) 보존\n주로 시각화 목적\n\n\n21.3.1 t-SNE\n대표적인 비선형 차원 축소 방법이 t-SNE(t-Distributed Stochastic Neighbor Embedding)이다.\n\n21.3.1.1 개념\n\n데이터 간 이웃 관계를 확률적으로 모델링\n가까운 데이터는 가깝게, 먼 데이터는 멀게 배치\n\n\n\n21.3.1.2 특징\n\n비선형\n시각화(2D, 3D)에 특화\n계산 비용 큼\n결과가 매번 달라질 수 있음\n\n\n\n\n21.3.2 t-SNE 예제\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X_scaled)\n\n\n21.3.3 UMAP\n또다른 비선형 차원 축소방법으로 UMAP(Uniform Manifold Approximation and Projection)이 있다.\n\n21.3.3.1 개념\n\n데이터가 저차원 다양체(manifold)에 놓여 있다고 가정\n전역 구조와 국소 구조를 동시에 보존하려는 접근\n\n\n\n\n21.3.4 UMAP의 특징\n\nt-SNE보다 빠름\n대규모 데이터에 적합\n결과 재현성 비교적 좋음\n시각화 + 전처리 용도로 활용 가능\n\n\n\n21.3.5 UMAP 예제\nimport umap\n\numap_model = umap.UMAP(n_components=2, random_state=42)\nX_umap = umap_model.fit_transform(X_scaled)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#pca-vs-t-sne-vs-umap-비교",
    "href": "part3/03. 차원 축소.html#pca-vs-t-sne-vs-umap-비교",
    "title": "21  차원 축소",
    "section": "21.4 PCA vs t-SNE vs UMAP 비교",
    "text": "21.4 PCA vs t-SNE vs UMAP 비교\n\nPCA\n\n선형\n빠름\n해석 가능\n전처리 및 모델 입력으로 사용 가능\n\nt-SNE\n\n비선형\n시각화 특화\n모델 입력용으로는 부적합\n\nUMAP\n\n비선형\n빠름\n시각화 + 전처리 가능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#실무-활용-가이드",
    "href": "part3/03. 차원 축소.html#실무-활용-가이드",
    "title": "21  차원 축소",
    "section": "21.5 3.3.7 실무 활용 가이드",
    "text": "21.5 3.3.7 실무 활용 가이드\n\n모델 학습 전 차원 축소\n\nPCA\n\n데이터 탐색 및 시각화\n\nt-SNE, UMAP\n\n대용량 데이터 시각화\n\nUMAP 우선 고려",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#주의-사항",
    "href": "part3/03. 차원 축소.html#주의-사항",
    "title": "21  차원 축소",
    "section": "21.6 주의 사항",
    "text": "21.6 주의 사항\n\n차원 축소 후 변수 해석이 어려워질 수 있음\n지도 학습에서는 타깃 정보가 반영되지 않음\n과도한 차원 축소는 정보 손실 초래",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html",
    "href": "part3/04. 회귀 모델.html",
    "title": "22  회귀 모델",
    "section": "",
    "text": "22.1 회귀 모델",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#회귀-모델",
    "href": "part3/04. 회귀 모델.html#회귀-모델",
    "title": "22  회귀 모델",
    "section": "",
    "text": "22.1.1 회귀 모델\n회귀 모델(Regression Models)은 입력 변수(X)와 연속형 타깃 변수(y) 사이의 관계를 학습하여 새로운 입력에 대한 수치값을 예측하는 모델이다.\n\n예측 대상이 수치형\n관계를 함수 형태로 모델링\n\n\n\n22.1.2 회귀 vs 분류\n\n회귀\n\n출력: 연속형 값\n예: 체중 예측, 매출 예측\n\n분류\n\n출력: 범주\n예: 종 분류, 합격/불합격",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#선형-회귀",
    "href": "part3/04. 회귀 모델.html#선형-회귀",
    "title": "22  회귀 모델",
    "section": "22.2 선형 회귀",
    "text": "22.2 선형 회귀\n\n22.2.1 기본 개념\n선형 회귀(Linear Regression)는 입력 변수와 출력 변수 사이의 관계를 선형 결합으로 표현한다.\n\n22.2.1.1 수식 형태\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon\n\\]\n\n\\(\\beta\\): 회귀 계수\n\\(\\varepsilon\\): 오차항\n\n\n\n\n22.2.2 선형 회귀의 가정\n\n선형성\n독립성\n등분산성\n정규성 (오차)\n\n현실 데이터에서는 완벽히 만족하지 않는 경우가 많음\n\n\n22.2.3 선형 회귀 예제\n\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\nX = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]]\ny = df[\"body_mass_g\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\nmodel.coef_, model.intercept_\n\n(array([ 3.85768347, 10.05813347, 50.24725463]),\n np.float64(-6227.688410615796))\n\n\n\n\n22.2.4 선형 회귀의 한계\n\n다중공선성 문제\n과적합 위험\n변수 수가 많을수록 불안정\n\n이를 해결하기 위한 방법이 정규화 회귀",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#정규화-개념",
    "href": "part3/04. 회귀 모델.html#정규화-개념",
    "title": "22  회귀 모델",
    "section": "22.3 정규화 개념",
    "text": "22.3 정규화 개념\n\n22.3.1 왜 정규화가 필요한가?\n정규화(Regularization)가 필요한 이유는 다음과 같다.\n\n계수가 너무 커지는 것을 방지\n불필요한 변수 영향 축소\n과적합 감소\n\n손실 함수에 패널티 항을 추가\n\n\n22.3.2 일반적인 손실 함수 구조\n\\[\n\\text{Loss} = \\text{오차} + \\lambda \\times \\text{패널티}\n\\]\n\n\\(\\lambda\\): 규제 강도\n값이 클수록 규제 강함",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#ridge-회귀-l2-정규화",
    "href": "part3/04. 회귀 모델.html#ridge-회귀-l2-정규화",
    "title": "22  회귀 모델",
    "section": "22.4 Ridge 회귀 (L2 정규화)",
    "text": "22.4 Ridge 회귀 (L2 정규화)\n\n22.4.1 개념\n계수의 제곱합(L2)에 패널티 부여하여 모든 변수를 유지하되 계수 크기를 줄이는 방식이다.\n\n\n22.4.2 특징\n\n다중공선성 완화\n변수 선택은 하지 않음\n계수를 0에 가깝게 만듦\n\n\n\n22.4.3 Ridge 예제\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=1.0)\nridge.fit(X_train, y_train)\n\nridge.coef_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#lasso-회귀-l1-정규화",
    "href": "part3/04. 회귀 모델.html#lasso-회귀-l1-정규화",
    "title": "22  회귀 모델",
    "section": "22.5 Lasso 회귀 (L1 정규화)",
    "text": "22.5 Lasso 회귀 (L1 정규화)\n\n22.5.1 개념\n계수의 절댓값 합(L1)에 패널티 부여 일부 계수를 0으로 만든다.\n\n\n22.5.2 특징\n\n자동 변수 선택 효과\n해석이 쉬움\n변수 수가 많을 때 유리\n\n\n\n22.5.3 Lasso 예제\nfrom sklearn.linear_model import Lasso\n\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train, y_train)\n\nlasso.coef_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#elasticnet-l1-l2",
    "href": "part3/04. 회귀 모델.html#elasticnet-l1-l2",
    "title": "22  회귀 모델",
    "section": "22.6 ElasticNet (L1 + L2)",
    "text": "22.6 ElasticNet (L1 + L2)\n\n22.6.1 개념\nRidge + Lasso의 혼합 방법으로 두 패널티의 장점 결합한 형태이다.\n\n\n22.6.2 하이퍼파라미터\n\nalpha: 전체 규제 강도\nl1_ratio: L1 비율\n\n\n\n22.6.3 ElasticNet 예제\nfrom sklearn.linear_model import ElasticNet\n\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic.fit(X_train, y_train)\n\nelastic.coef_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#회귀-모델-비교-요약",
    "href": "part3/04. 회귀 모델.html#회귀-모델-비교-요약",
    "title": "22  회귀 모델",
    "section": "22.7 회귀 모델 비교 요약",
    "text": "22.7 회귀 모델 비교 요약\n\n선형 회귀\n\n기준 모델\n규제 없음\n\nRidge\n\n안정적인 계수\n다중공선성 대응\n\nLasso\n\n변수 선택\n해석 용이\n\nElasticNet\n\n복합적인 상황에 적합",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#선택-가이드",
    "href": "part3/04. 회귀 모델.html#선택-가이드",
    "title": "22  회귀 모델",
    "section": "22.8 선택 가이드",
    "text": "22.8 선택 가이드\n\n변수 수 적고 해석 중요\n\n선형 회귀\n\n변수 간 상관 높음\n\nRidge\n\n변수 선택 필요\n\nLasso\n\n고차원 데이터\n\nElasticNet",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#전처리-주의-사항",
    "href": "part3/04. 회귀 모델.html#전처리-주의-사항",
    "title": "22  회귀 모델",
    "section": "22.9 전처리 주의 사항",
    "text": "22.9 전처리 주의 사항\n\n정규화 회귀는 스케일에 민감\n반드시 표준화 후 적용\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", Ridge(alpha=1.0))\n])\n\npipe.fit(X_train, y_train)\n회귀 모델은 연속형 예측의 기본이고 정규화는 과적합 방지의 핵심이다. 모델 선택보다 전처리 + 하이퍼파라미터 튜닝이 더 중요할 수 있다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html",
    "href": "part3/05. 분류 모델.html",
    "title": "23  분류 모델",
    "section": "",
    "text": "23.1 분류 모델 개요\n분류 모델(Classification Models)은 입력 변수(X)를 기반으로 범주형 타깃 변수(y) 를 예측하는 모델이다.\npenguins 데이터셋에서는 다음이 대표적인 분류 문제다.\nimport seaborn as sns\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\ndf = sns.load_dataset(\"penguins\").dropna()\n\nX = df[[\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]]\n\ny = df[\"species\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\nstratify=y",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#분류-모델-개요",
    "href": "part3/05. 분류 모델.html#분류-모델-개요",
    "title": "23  분류 모델",
    "section": "",
    "text": "출력: 이산적인 클래스\n예: 펭귄 종(species) 분류\n\n\n\n타깃 변수: species\n입력 변수: 부리 길이, 부리 깊이, 날개 길이, 체중 등\n\n\n\n\n클래스 비율 유지 (분류 문제에서 매우 중요)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#선형-분류-모델",
    "href": "part3/05. 분류 모델.html#선형-분류-모델",
    "title": "23  분류 모델",
    "section": "23.2 선형 분류 모델",
    "text": "23.2 선형 분류 모델\n\n23.2.1 로지스틱 회귀\n로지스틱 회귀(Logistic Regression)의 이름은 회귀지만 분류 모델이다.\n\n선형 결정 경계\n확률 기반 예측\n이진 분류, 다중 분류 모두 가능\n\n\n\n23.2.2 개념 요약\n\n선형 결합 결과 → 시그모이드 함수\n출력값을 확률로 해석\n가장 확률이 높은 클래스를 예측값으로 선택\n\n\n\n23.2.3 로지스틱 회귀 예제\nfrom sklearn.linear_model import LogisticRegression\n\npipe_lr = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression(max_iter=1000))\n])\n\npipe_lr.fit(X_train, y_train)\n\npipe_lr.score(X_test, y_test)\n\n\n23.2.4 특징\n\n해석 가능성 높음\n고차원 데이터에 강함\n비선형 경계 표현에는 한계",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#거리-기반-모델",
    "href": "part3/05. 분류 모델.html#거리-기반-모델",
    "title": "23  분류 모델",
    "section": "23.3 거리 기반 모델",
    "text": "23.3 거리 기반 모델\n\n23.3.1 k-최근접 이웃 (k-NN)\n가장 직관적인 분류 모델로 새로운 데이터 주변의 k개 이웃을 기준으로 분류한다. 거리 기반으로 클래스를 판단한다.\n\n\n23.3.2 개념 요약\n\n학습 과정 없음\n거리 계산이 핵심\n스케일에 매우 민감\n\n\n\n23.3.3 k-NN 예제\nfrom sklearn.neighbors import KNeighborsClassifier\n\npipe_knn = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", KNeighborsClassifier(n_neighbors=5))\n])\n\npipe_knn.fit(X_train, y_train)\n\npipe_knn.score(X_test, y_test)\n\n\n23.3.4 k 값의 영향\n\nk가 작음 → 과적합\nk가 큼 → 과소적합\n\n교차 검증으로 선택하는 것이 일반적\n\n\n23.3.5 거리 기반 모델의 한계\n\n데이터 수 증가 시 예측 속도 저하\n고차원 공간에서 거리 의미 약화",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#트리-기반-모델",
    "href": "part3/05. 분류 모델.html#트리-기반-모델",
    "title": "23  분류 모델",
    "section": "23.4 트리 기반 모델",
    "text": "23.4 트리 기반 모델\n\n23.4.1 결정 트리 (Decision Tree)\n질문을 반복하며 분기하며, 사람이 이해하기 쉬운 구조를 갖는다.\n\n\n23.4.2 개념 요약\n\n불순도 감소 기준으로 분기\n비선형 경계 표현 가능\n스케일링 불필요\n\n\n\n23.4.3 결정 트리 예제\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(\n    max_depth=4,\n    random_state=42\n)\n\ntree.fit(X_train, y_train)\n\ntree.score(X_test, y_test)\n\n\n23.4.4 단점\n\n과적합에 매우 취약\n데이터 변화에 민감\n\n이를 보완한 것이 앙상블 모델",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#앙상블-모델",
    "href": "part3/05. 분류 모델.html#앙상블-모델",
    "title": "23  분류 모델",
    "section": "23.5 3.5.5 앙상블 모델",
    "text": "23.5 3.5.5 앙상블 모델\n\n23.5.1 랜덤 포레스트 (Random Forest)\n여러 결정 트리를 결합한 형태로 배깅(Bagging) 기반 모델이다.\n\n\n23.5.2 특징\n\n과적합 감소\n안정적인 성능\n변수 중요도 제공\n\n\n\n23.5.3 랜덤 포레스트 예제\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    random_state=42\n)\n\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\n\n23.5.4 변수 중요도 확인\npd.Series(\n    rf.feature_importances_,\n    index=X.columns\n).sort_values(ascending=False)\n\n\n23.5.5 그래디언트 부스팅 계열\n\n이전 모델의 오류를 보완하며 학습\n성능은 좋지만 해석과 튜닝 난이도 ↑",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#모델-비교-요약",
    "href": "part3/05. 분류 모델.html#모델-비교-요약",
    "title": "23  분류 모델",
    "section": "23.6 모델 비교 요약",
    "text": "23.6 모델 비교 요약\n\n로지스틱 회귀\n\n기준 모델\n해석 용이\n\nk-NN\n\n직관적\n스케일 중요\n\n결정 트리\n\n비선형\n과적합 주의\n\n랜덤 포레스트\n\n안정적\n실무 활용도 높음",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#선택-가이드",
    "href": "part3/05. 분류 모델.html#선택-가이드",
    "title": "23  분류 모델",
    "section": "23.7 선택 가이드",
    "text": "23.7 선택 가이드\n\n빠른 기준선\n\n로지스틱 회귀\n\n데이터 구조 탐색\n\n결정 트리\n\n성능 우선\n\n랜덤 포레스트\n\n데이터 수 적음\n\nk-NN (튜닝 필수)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html",
    "href": "part3/06. 군집 분석.html",
    "title": "24  군집 분석",
    "section": "",
    "text": "24.1 군집 분석 개요\n군집 분석(Clustering)은 라벨이 없는 데이터에서 유사한 데이터끼리 묶는 비지도 학습 기법이다.\n주요 목적은 다음과 같다.\npenguins 데이터처럼",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#군집-분석-개요",
    "href": "part3/06. 군집 분석.html#군집-분석-개요",
    "title": "24  군집 분석",
    "section": "",
    "text": "데이터 구조 탐색\n패턴 및 그룹 발견\n이후 모델링을 위한 전처리 또는 특성 생성\n\n\n\n“종(species)을 모른다고 가정하고도 묶일까?” 를 실험하기에 매우 적합하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#군집-분석의-주요-가정",
    "href": "part3/06. 군집 분석.html#군집-분석의-주요-가정",
    "title": "24  군집 분석",
    "section": "24.2 군집 분석의 주요 가정",
    "text": "24.2 군집 분석의 주요 가정\n군집 알고리즘마다 전제가 다르다.\n\n거리 기반인가?\n밀도를 가정하는가?\n분포를 가정하는가?\n\n알고리즘 선택 = 데이터 가정 선택",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#분할-기반-군집",
    "href": "part3/06. 군집 분석.html#분할-기반-군집",
    "title": "24  군집 분석",
    "section": "24.3 분할 기반 군집",
    "text": "24.3 분할 기반 군집\n분할 기반 군집(Partition-based Clustering)의 대표적인 알고리즘으로 K-Means가 있다.\n\n24.3.1 K-Means\n\n24.3.1.1 핵심 아이디어\n\nK개의 군집 중심(centroid)을 미리 정함\n각 데이터 → 가장 가까운 중심에 할당\n중심 재계산 → 반복\n\n\n\n24.3.1.2 특징\n\n빠르고 구현이 단순\n군집 수 K를 사전에 지정해야 함\n구형(spherical) 군집에 적합\n이상치에 민감\n\n\n\n24.3.1.3 예제\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom seaborn import load_dataset\n\ndf = load_dataset('penguins')\nX_cluster = df[\n    [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\n].dropna()\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\nlabels_kmeans\n\narray([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1,\n       1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)\n\n\n결과 해석\n\nn_clusters=3\n\npenguins의 실제 종 개수와 일치\n\n하지만 군집 ≠ 실제 라벨\n구조적 유사성만 반영\n\n\n\n\n24.3.2 K 선택 방법\n\nElbow Method\nSilhouette Score\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X_scaled, labels_kmeans)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#밀도-기반-군집",
    "href": "part3/06. 군집 분석.html#밀도-기반-군집",
    "title": "24  군집 분석",
    "section": "24.4 밀도 기반 군집",
    "text": "24.4 밀도 기반 군집\n대표적인 밀도 기반 군집(Density-based Clustering) 알고리즘으로 DBSCAN이 있다.\n\n24.4.1 DBSCAN\n\n24.4.1.1 핵심 아이디어\n\n데이터가 빽빽한 영역을 군집으로 간주\n밀도가 낮은 영역은 노이즈(-1) 처리\n\n\n\n24.4.1.2 주요 파라미터\n\neps: 이웃으로 간주할 거리\nmin_samples: 최소 이웃 수\n\n\n\n24.4.1.3 특징\n\n군집 개수 자동 결정\n이상치 탐지 가능\n비구형 군집 탐지 가능\n밀도 차이가 크면 성능 저하\n\n\n\n24.4.1.4 예제\nfrom sklearn.cluster import DBSCAN\n\ndbscan = DBSCAN(eps=0.8, min_samples=5)\nlabels_dbscan = dbscan.fit_predict(X_scaled)\n결과 해석\n\nlabel = -1\n\n군집에 속하지 않는 이상치\n\n군집 수는 eps에 따라 크게 변함\n\nDBSCAN은 시각화 + 반복 실험 필수",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#혼합-모델-군집",
    "href": "part3/06. 군집 분석.html#혼합-모델-군집",
    "title": "24  군집 분석",
    "section": "24.5 혼합 모델 군집",
    "text": "24.5 혼합 모델 군집\n밀도, 거리 등을 혼합한 혼합 모델 군집(Model-based Clustering)도 있다.\n\n24.5.1 Gaussian Mixture Model (GMM)\n\n24.5.1.1 핵심 아이디어\n\n데이터가 여러 개의 가우시안 분포 혼합으로 생성되었다고 가정\n각 데이터는 군집에 확률적으로 속함\n\n\n\n24.5.1.2 특징\n\n타원형 군집 가능\n소프트 군집 (확률)\n분포 가정이 맞지 않으면 성능 저하\n\n\n\n24.5.1.3 예제\nfrom sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=3, random_state=42)\nlabels_gmm = gmm.fit_predict(X_scaled)\n\n\n24.5.1.4 확률 기반 해석\nprobs = gmm.predict_proba(X_scaled)\nprobs[:5]\n“이 데이터는 군집 1에 70% 속한다”",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#알고리즘-비교-정리",
    "href": "part3/06. 군집 분석.html#알고리즘-비교-정리",
    "title": "24  군집 분석",
    "section": "24.6 알고리즘 비교 정리",
    "text": "24.6 알고리즘 비교 정리\n\n\n\n구분\nK-Means\nDBSCAN\nGMM\n\n\n\n\n군집 수\n사전 지정\n자동\n사전 지정\n\n\n군집 형태\n구형\n자유\n타원\n\n\n이상치 처리\n약함\n강함\n약함\n\n\n소프트 할당\nX\nX\nO\n\n\n계산 비용\n낮음\n중간\n중간",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#선택-가이드",
    "href": "part3/06. 군집 분석.html#선택-가이드",
    "title": "24  군집 분석",
    "section": "24.7 선택 가이드",
    "text": "24.7 선택 가이드\n\n빠른 탐색, 기준선 모델 → K-Means\n이상치 탐지, 비구형 구조 → DBSCAN\n분포 기반 해석, 확률 필요 → GMM\n\n한 가지로 끝내지 말고 여러 군집 결과 비교",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#군집-결과-평가",
    "href": "part3/06. 군집 분석.html#군집-결과-평가",
    "title": "24  군집 분석",
    "section": "24.8 군집 결과 평가",
    "text": "24.8 군집 결과 평가\n군집에는 “정답”이 없다. 따라서 평가 기준도 다르다.\n\nSilhouette Score\nDavies–Bouldin Index\n시각적 해석 (PCA, t-SNE)\n\n군집은 수치 + 해석을 함께 봐야 한다",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html",
    "href": "part3/07. 서포트 벡터 머신.html",
    "title": "25  서포트 벡터 머신",
    "section": "",
    "text": "25.1 SVM 개요\n서포트 벡터 머신(Support Vector Machine, SVM) 은 클래스를 가장 잘 구분하는 결정 경계(초평면) 를 찾는 분류 모델이다.\n핵심 아이디어는 단순하다.\n이 “경계에 가장 가까운 데이터”를 서포트 벡터라고 부른다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#svm-개요",
    "href": "part3/07. 서포트 벡터 머신.html#svm-개요",
    "title": "25  서포트 벡터 머신",
    "section": "",
    "text": "클래스 간 마진(margin) 을 최대화\n경계에 가장 가까운 데이터만 학습에 사용",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#마진과-서포트-벡터",
    "href": "part3/07. 서포트 벡터 머신.html#마진과-서포트-벡터",
    "title": "25  서포트 벡터 머신",
    "section": "25.2 마진과 서포트 벡터",
    "text": "25.2 마진과 서포트 벡터\n\n25.2.1 마진(Margin)\n\n결정 경계와 가장 가까운 데이터 사이의 거리\nSVM은 마진이 최대가 되도록 경계를 찾는다\n\n노이즈에 강하고 일반화 성능이 좋음\n\n\n25.2.2 서포트 벡터\n\n결정 경계를 결정하는 핵심 데이터\n전체 데이터 중 일부만 모델을 결정\n\n즉, SVM은\n\n“모든 데이터를 쓰지 않는다”",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#하드-마진-vs-소프트-마진",
    "href": "part3/07. 서포트 벡터 머신.html#하드-마진-vs-소프트-마진",
    "title": "25  서포트 벡터 머신",
    "section": "25.3 하드 마진 vs 소프트 마진",
    "text": "25.3 하드 마진 vs 소프트 마진\n\n25.3.1 하드 마진\n\n모든 데이터를 완벽히 분리\n현실 데이터에서는 거의 불가능\n\n\n\n25.3.2 소프트 마진\n\n일부 오분류 허용\nC 파라미터로 허용 정도 조절\n\n\n\n25.3.3 C 파라미터의 의미\n\nC ↑\n\n오분류 허용 ↓\n과적합 위험 ↑\n\nC ↓\n\n마진 넓어짐\n과소적합 가능성 ↑",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#선형-svm",
    "href": "part3/07. 서포트 벡터 머신.html#선형-svm",
    "title": "25  서포트 벡터 머신",
    "section": "25.4 선형 SVM",
    "text": "25.4 선형 SVM\n\n25.4.1 특징\n\n선형 결정 경계\n고차원 데이터에 강함\n로지스틱 회귀와 비교 대상\n\n\n\n25.4.2 예제\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipe_svm_linear = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", SVC(kernel=\"linear\", C=1.0))\n])\n\npipe_svm_linear.fit(X_train, y_train)\n\npipe_svm_linear.score(X_test, y_test)\nSVM은 스케일링 필수",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#비선형-svm과-커널-트릭",
    "href": "part3/07. 서포트 벡터 머신.html#비선형-svm과-커널-트릭",
    "title": "25  서포트 벡터 머신",
    "section": "25.5 비선형 SVM과 커널 트릭",
    "text": "25.5 비선형 SVM과 커널 트릭\n현실 데이터는 선형으로 분리되지 않는 경우가 많다. 이때 사용하는 것이 커널 함수(kernel)이다.\n\n25.5.1 커널 트릭 개념\n\n원래 공간에서는 비선형\n고차원 공간으로 변환하면 선형 분리 가능\n실제로 변환을 계산하지 않고 내적만 계산\n\n\n\n25.5.2 대표적인 커널\n\nLinear\nPolynomial\nRBF (Gaussian), 가장 많이 사용\nSigmoid",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#rbf-커널과-γ-gamma",
    "href": "part3/07. 서포트 벡터 머신.html#rbf-커널과-γ-gamma",
    "title": "25  서포트 벡터 머신",
    "section": "25.6 RBF 커널과 γ (gamma)",
    "text": "25.6 RBF 커널과 γ (gamma)\n\n25.6.1 gamma의 의미\n\n하나의 데이터가 미치는 영향 범위\n\n\n\n25.6.2 gamma 값의 영향\n\ngamma ↑\n\n영향 범위 좁음\n결정 경계 복잡\n과적합 위험 ↑\n\ngamma ↓\n\n영향 범위 넓음\n단순한 경계\n과소적합 가능성 ↑\n\n\n\n\n25.6.3 예제 (RBF SVM)\npipe_svm_rbf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", SVC(\n        kernel=\"rbf\",\n        C=1.0,\n        gamma=\"scale\"\n    ))\n])\n\npipe_svm_rbf.fit(X_train, y_train)\n\npipe_svm_rbf.score(X_test, y_test)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#다중-분류에서의-svm",
    "href": "part3/07. 서포트 벡터 머신.html#다중-분류에서의-svm",
    "title": "25  서포트 벡터 머신",
    "section": "25.7 다중 분류에서의 SVM",
    "text": "25.7 다중 분류에서의 SVM\nSVM은 본래 이진 분류 모델이나 다중 클래스에서는 내부적으로 다음 방식을 사용한다.\n\nOne-vs-Rest (OvR)\nOne-vs-One (OvO, 기본값)\n\nscikit-learn에서는 자동 처리된다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#svm의-장단점",
    "href": "part3/07. 서포트 벡터 머신.html#svm의-장단점",
    "title": "25  서포트 벡터 머신",
    "section": "25.8 SVM의 장단점",
    "text": "25.8 SVM의 장단점\n\n25.8.1 장점\n\n일반화 성능 우수\n고차원 데이터에 강함\n커널을 통한 강력한 비선형 표현\n\n\n\n25.8.2 단점\n\n대규모 데이터에서 학습 비용 큼\n파라미터(C, gamma) 튜닝 필요\n모델 해석 어려움",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#로지스틱-회귀-vs-svm",
    "href": "part3/07. 서포트 벡터 머신.html#로지스틱-회귀-vs-svm",
    "title": "25  서포트 벡터 머신",
    "section": "25.9 로지스틱 회귀 vs SVM",
    "text": "25.9 로지스틱 회귀 vs SVM\n\n\n\n구분\n로지스틱 회귀\nSVM\n\n\n\n\n결정 기준\n확률 최대화\n마진 최대화\n\n\n출력\n확률\n클래스\n\n\n해석\n쉬움\n어려움\n\n\n비선형\n제한적\n커널 사용",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#활용-가이드",
    "href": "part3/07. 서포트 벡터 머신.html#활용-가이드",
    "title": "25  서포트 벡터 머신",
    "section": "25.10 활용 가이드",
    "text": "25.10 활용 가이드\n\n데이터 수가 많지 않음\n특성 차원이 높음\n명확한 결정 경계 필요\n\n이런 경우 SVM은 매우 강력한 선택\n반면,\n\n데이터가 매우 큼\n모델 해석이 중요\n\n트리 계열 또는 로지스틱 회귀 고려",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html",
    "href": "part3/08. 모델 성능 평가.html",
    "title": "26  모델 성능 평가",
    "section": "",
    "text": "27 모델 성능 평가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#모델-성능-평가의-목적",
    "href": "part3/08. 모델 성능 평가.html#모델-성능-평가의-목적",
    "title": "26  모델 성능 평가",
    "section": "27.1 모델 성능 평가의 목적",
    "text": "27.1 모델 성능 평가의 목적\n모델 성능 평가(Model Evaluation)는 단순히 “정확한가?”를 묻는 과정이 아니다.\n핵심 질문은 다음과 같다.\n\n학습 데이터가 아닌 새 데이터에서도 잘 작동하는가?\n어떤 오류를 더 중요하게 봐야 하는가?\n모델 간 공정한 비교가 가능한가?\n\n문제 유형에 따라 평가 지표는 달라져야 한다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#회귀-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#회귀-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "27.2 회귀 성능 평가",
    "text": "27.2 회귀 성능 평가\n회귀 문제에서는\n\n“예측값이 실제값과 얼마나 다른가?”\n\n를 측정한다.\n\n27.2.1 주요 지표\n\n27.2.1.1 MAE (Mean Absolute Error)\n\n오차의 절댓값 평균\n직관적 해석 가능\n\nfrom sklearn.metrics import mean_absolute_error\n\nmean_absolute_error(y_test, y_pred)\n\n\n27.2.1.2 MSE (Mean Squared Error)\n\n오차 제곱 평균\n큰 오차에 더 큰 패널티\n\nfrom sklearn.metrics import mean_squared_error\n\nmean_squared_error(y_test, y_pred)\n\n\n27.2.1.3 RMSE (Root MSE)\n\nMSE의 제곱근\n원래 단위로 해석 가능\n\nmean_squared_error(y_test, y_pred, squared=False)\n\n\n27.2.1.4 R² (결정계수)\n\n모델이 분산을 얼마나 설명하는가\n상대적 지표\n\nfrom sklearn.metrics import r2_score\n\nr2_score(y_test, y_pred)\n\n\n\n27.2.2 회귀 지표 선택 가이드\n\n\n\n상황\n추천 지표\n\n\n\n\n직관적 오차 크기\nMAE\n\n\n큰 오차 중요\nRMSE\n\n\n모델 비교\nR²",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#분류-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#분류-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "27.3 분류 성능 평가",
    "text": "27.3 분류 성능 평가\n분류 문제에서는\n\n“얼마나 정확히 구분했는가?” 보다 “어떤 실수를 했는가?”\n\n가 더 중요할 수 있다.\n\n27.3.1 혼동 행렬 (Confusion Matrix)\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)\n\n\n\n실제  예측\nPositive\nNegative\n\n\n\n\nPositive\nTP\nFN\n\n\nNegative\nFP\nTN\n\n\n\n\n\n27.3.2 주요 지표\n\n27.3.2.1 Accuracy\n\n전체 중 맞춘 비율\n클래스 불균형에 취약\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, y_pred)\n\n\n27.3.2.2 Precision\n\nPositive로 예측한 것 중 실제 Positive\n오탐(False Positive) 최소화\n\nfrom sklearn.metrics import precision_score\n\n\n27.3.2.3 Recall\n\n실제 Positive 중 맞춘 비율\n미탐(False Negative) 최소화\n\nfrom sklearn.metrics import recall_score\n\n\n27.3.2.4 F1-score\n\nPrecision과 Recall의 조화 평균\n불균형 데이터에 적합\n\nfrom sklearn.metrics import f1_score\n\n\n\n27.3.3 ROC-AUC\n\n분류 임계값 변화에 따른 성능\n확률 기반 평가\n\nfrom sklearn.metrics import roc_auc_score\n\n\n27.3.4 분류 예제\n\nspecies 분류 문제\n클래스 균형이 비교적 양호\nAccuracy + F1 병행 권장",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#다중-클래스-분류-평가",
    "href": "part3/08. 모델 성능 평가.html#다중-클래스-분류-평가",
    "title": "26  모델 성능 평가",
    "section": "27.4 다중 클래스 분류 평가",
    "text": "27.4 다중 클래스 분류 평가\n\nmacro 평균\nmicro 평균\nweighted 평균\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n클래스 수가 늘어나면 accuracy 단독 사용 금지",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#군집-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#군집-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "27.5 군집 성능 평가",
    "text": "27.5 군집 성능 평가\n군집은 정답 라벨이 없는 경우가 많다. 따라서 평가 방식도 다르다.\n\n27.5.1 내부 평가 지표 (라벨 없음)\n\n27.5.1.1 Silhouette Score\n\n군집 내 응집도 vs 군집 간 분리도\n-1 ~ 1\n\nfrom sklearn.metrics import silhouette_score\n\nsilhouette_score(X_scaled, labels)\n\n\n27.5.1.2 Davies–Bouldin Index\n\n군집 간 유사도\n작을수록 좋음\n\nfrom sklearn.metrics import davies_bouldin_score\n\n\n\n27.5.2 외부 평가 지표 (라벨 존재 시)\npenguins처럼 실제 종을 알고 있는 경우\n\n27.5.2.1 Adjusted Rand Index (ARI)\nfrom sklearn.metrics import adjusted_rand_score\n\nadjusted_rand_score(true_labels, cluster_labels)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#평가-지표-선택-요약",
    "href": "part3/08. 모델 성능 평가.html#평가-지표-선택-요약",
    "title": "26  모델 성능 평가",
    "section": "27.6 평가 지표 선택 요약",
    "text": "27.6 평가 지표 선택 요약\n\n\n\n문제 유형\n주요 지표\n\n\n\n\n회귀\nMAE, RMSE, R²\n\n\n이진 분류\nPrecision, Recall, F1, ROC-AUC\n\n\n다중 분류\nF1 (macro/weighted)\n\n\n군집\nSilhouette, ARI",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#평가-시-중요한-원칙",
    "href": "part3/08. 모델 성능 평가.html#평가-시-중요한-원칙",
    "title": "26  모델 성능 평가",
    "section": "27.7 평가 시 중요한 원칙",
    "text": "27.7 평가 시 중요한 원칙\n\n하나의 지표로 결론 내리지 않는다\n문제 맥락에 맞는 지표 선택\n항상 테스트 데이터 기준 평가\n\n모델 성능 평가는 숫자보다 판단의 근거다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html",
    "href": "part3/09. 파이프라인 및 자동화.html",
    "title": "27  파이프라인 및 자동화",
    "section": "",
    "text": "27.1 왜 파이프라인이 필요한가\n머신러닝 실무에서 가장 흔한 실수는 다음과 같다.\n파이프라인은 이 모든 문제를 구조적으로 차단한다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#왜-파이프라인이-필요한가",
    "href": "part3/09. 파이프라인 및 자동화.html#왜-파이프라인이-필요한가",
    "title": "27  파이프라인 및 자동화",
    "section": "",
    "text": "학습 데이터와 테스트 데이터에 다른 전처리 적용\n교차 검증 전에 스케일링 수행\n실험 재현 불가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#파이프라인의-개념",
    "href": "part3/09. 파이프라인 및 자동화.html#파이프라인의-개념",
    "title": "27  파이프라인 및 자동화",
    "section": "27.2 파이프라인의 개념",
    "text": "27.2 파이프라인의 개념\n파이프라인(Pipeline) 은 전처리 → 모델 → 평가 단계를 하나의 객체로 묶는 구조다.\n특징\n\n순차 실행 보장\n데이터 누수 방지\n코드 단순화\n자동화에 최적",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#기본-파이프라인-구성",
    "href": "part3/09. 파이프라인 및 자동화.html#기본-파이프라인-구성",
    "title": "27  파이프라인 및 자동화",
    "section": "27.3 기본 파이프라인 구성",
    "text": "27.3 기본 파이프라인 구성\n\n27.3.1 penguins 분류 예제\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\npipe = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression(max_iter=1000))\n])\n\n\n27.3.2 학습 및 평가\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)\nfit() 호출 시\n\nscaler → fit + transform\nmodel → fit\n\n이 순서가 자동으로 수행된다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#columntransformer",
    "href": "part3/09. 파이프라인 및 자동화.html#columntransformer",
    "title": "27  파이프라인 및 자동화",
    "section": "27.4 ColumnTransformer",
    "text": "27.4 ColumnTransformer\n실제 데이터는 수치형 + 범주형이 섞여 있다. 컬럼별 전처리를 명시적으로 분리해야 한다.\n\n27.4.1 예제\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]\ncat_cols = [\"sex\"]\n\npreprocessor = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols),\n    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n])\n\n\n27.4.2 전체 파이프라인\npipe = Pipeline([\n    (\"preprocess\", preprocessor),\n    (\"model\", LogisticRegression(max_iter=1000))\n])",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#하이퍼파라미터-최적화-개요",
    "href": "part3/09. 파이프라인 및 자동화.html#하이퍼파라미터-최적화-개요",
    "title": "27  파이프라인 및 자동화",
    "section": "27.5 하이퍼파라미터 최적화 개요",
    "text": "27.5 하이퍼파라미터 최적화 개요\n모델 성능은 하이퍼파라미터 설정에 크게 좌우된다.\n\n사람이 일일이 조정 → 비효율\n자동 탐색 필요",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#gridsearchcv",
    "href": "part3/09. 파이프라인 및 자동화.html#gridsearchcv",
    "title": "27  파이프라인 및 자동화",
    "section": "27.6 3.9.6 GridSearchCV",
    "text": "27.6 3.9.6 GridSearchCV\n\n27.6.1 개념\n\n지정한 모든 파라미터 조합 탐색\n계산 비용 큼\n기준선 모델에 적합\n\n\n\n27.6.2 예제\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"model__C\": [0.1, 1, 10],\n    \"model__penalty\": [\"l2\"]\n}\n\ngrid = GridSearchCV(\n    pipe,\n    param_grid,\n    cv=5,\n    scoring=\"f1_macro\"\n)\n\ngrid.fit(X_train, y_train)\n\n\n27.6.3 결과 확인\ngrid.best_params_\ngrid.best_score_",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#randomizedsearchcv",
    "href": "part3/09. 파이프라인 및 자동화.html#randomizedsearchcv",
    "title": "27  파이프라인 및 자동화",
    "section": "27.7 3.9.7 RandomizedSearchCV",
    "text": "27.7 3.9.7 RandomizedSearchCV\n\n27.7.1 특징\n\n파라미터 공간에서 무작위 샘플링\n대규모 탐색에 효율적\n실무에서 자주 사용\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandom_search = RandomizedSearchCV(\n    pipe,\n    param_distributions={\n        \"model__C\": [0.01, 0.1, 1, 10, 100]\n    },\n    n_iter=10,\n    cv=5,\n    scoring=\"f1_macro\",\n    random_state=42\n)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#교차-검증과-파이프라인의-결합",
    "href": "part3/09. 파이프라인 및 자동화.html#교차-검증과-파이프라인의-결합",
    "title": "27  파이프라인 및 자동화",
    "section": "27.8 교차 검증과 파이프라인의 결합",
    "text": "27.8 교차 검증과 파이프라인의 결합\n중요 포인트\n\n교차 검증 안에서 전처리 수행\n데이터 누수 완전 차단\n\n파이프라인 없이 CV는 위험",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#성능-평가-흐름-정리",
    "href": "part3/09. 파이프라인 및 자동화.html#성능-평가-흐름-정리",
    "title": "27  파이프라인 및 자동화",
    "section": "27.9 성능 평가 흐름 정리",
    "text": "27.9 성능 평가 흐름 정리\n\n파이프라인 정의\n교차 검증 기반 탐색\n최적 모델 선택\n테스트 데이터 최종 평가\n\n테스트 데이터는 마지막에 한 번만",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#권장-사항",
    "href": "part3/09. 파이프라인 및 자동화.html#권장-사항",
    "title": "27  파이프라인 및 자동화",
    "section": "27.10 권장 사항",
    "text": "27.10 권장 사항\n\n모든 실험은 파이프라인으로 시작\n전처리 로직을 코드 밖으로 빼지 않기\nscoring 기준을 문제에 맞게 선택\nrandom_state 고정",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html",
    "href": "part3/10. 모델 해석.html",
    "title": "28  모델 해석",
    "section": "",
    "text": "28.1 왜 모델 해석이 필요한가\n머신러닝 모델은 점점 복잡해지고 있다. 하지만 실무에서는 다음 질문에 답해야 한다.\n👉 모델 해석(Model Interpretation)은 선택이 아니라 필수",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#왜-모델-해석이-필요한가",
    "href": "part3/10. 모델 해석.html#왜-모델-해석이-필요한가",
    "title": "28  모델 해석",
    "section": "",
    "text": "어떤 변수가 가장 중요한가?\n특정 예측은 왜 이렇게 나왔는가?\n모델이 편향되어 있지는 않은가?",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#모델-해석의-범주",
    "href": "part3/10. 모델 해석.html#모델-해석의-범주",
    "title": "28  모델 해석",
    "section": "28.2 모델 해석의 범주",
    "text": "28.2 모델 해석의 범주\n모델 해석은 크게 두 가지 관점으로 나뉜다.\n\n\n\n구분\n설명\n\n\n\n\n전역 해석\n모델 전체에서 중요한 변수는 무엇인가\n\n\n국소 해석\n개별 샘플의 예측 이유",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#특성-중요도",
    "href": "part3/10. 모델 해석.html#특성-중요도",
    "title": "28  모델 해석",
    "section": "28.3 특성 중요도",
    "text": "28.3 특성 중요도\n\n28.3.1 개념\n특성 중요도(Feature Importance)는 모델이 예측에 얼마나 해당 변수를 활용했는지를 수치로 표현한 것이다. 주로 트리 기반 모델에서 제공된다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#트리-기반-모델의-특성-중요도",
    "href": "part3/10. 모델 해석.html#트리-기반-모델의-특성-중요도",
    "title": "28  모델 해석",
    "section": "28.4 트리 기반 모델의 특성 중요도",
    "text": "28.4 트리 기반 모델의 특성 중요도\n\n28.4.1 penguins 데이터 준비\n\nfrom palmerpenguins import load_penguins\nimport pandas as pd\n\ndf = load_penguins().dropna()\n\nX = df[[\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]]\ny = df[\"species\"]\n\n\n\n28.4.2 RandomForest 학습\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators=200,\n    random_state=42\n)\n\nrf.fit(X, y)\n\nRandomForestClassifier(n_estimators=200, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nn_estimators n_estimators: int, default=100\n\nThe number of trees in the forest.\n\n.. versionchanged:: 0.22\nThe default value of ``n_estimators`` changed from 10 to 100\nin 0.22.\n200\n\n\n\ncriterion criterion: {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n\nThe function to measure the quality of a split. Supported criteria are\n\"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\nShannon information gain, see :ref:`tree_mathematical_formulation`.\nNote: This parameter is tree-specific.\n'gini'\n\n\n\nmax_depth max_depth: int, default=None\n\nThe maximum depth of the tree. If None, then nodes are expanded until\nall leaves are pure or until all leaves contain less than\nmin_samples_split samples.\nNone\n\n\n\nmin_samples_split min_samples_split: int or float, default=2\n\nThe minimum number of samples required to split an internal node:\n\n- If int, then consider `min_samples_split` as the minimum number.\n- If float, then `min_samples_split` is a fraction and\n`ceil(min_samples_split * n_samples)` are the minimum\nnumber of samples for each split.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n2\n\n\n\nmin_samples_leaf min_samples_leaf: int or float, default=1\n\nThe minimum number of samples required to be at a leaf node.\nA split point at any depth will only be considered if it leaves at\nleast ``min_samples_leaf`` training samples in each of the left and\nright branches. This may have the effect of smoothing the model,\nespecially in regression.\n\n- If int, then consider `min_samples_leaf` as the minimum number.\n- If float, then `min_samples_leaf` is a fraction and\n`ceil(min_samples_leaf * n_samples)` are the minimum\nnumber of samples for each node.\n\n.. versionchanged:: 0.18\nAdded float values for fractions.\n1\n\n\n\nmin_weight_fraction_leaf min_weight_fraction_leaf: float, default=0.0\n\nThe minimum weighted fraction of the sum total of weights (of all\nthe input samples) required to be at a leaf node. Samples have\nequal weight when sample_weight is not provided.\n0.0\n\n\n\nmax_features max_features: {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n\nThe number of features to consider when looking for the best split:\n\n- If int, then consider `max_features` features at each split.\n- If float, then `max_features` is a fraction and\n`max(1, int(max_features * n_features_in_))` features are considered at each\nsplit.\n- If \"sqrt\", then `max_features=sqrt(n_features)`.\n- If \"log2\", then `max_features=log2(n_features)`.\n- If None, then `max_features=n_features`.\n\n.. versionchanged:: 1.1\nThe default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n\nNote: the search for a split does not stop until at least one\nvalid partition of the node samples is found, even if it requires to\neffectively inspect more than ``max_features`` features.\n'sqrt'\n\n\n\nmax_leaf_nodes max_leaf_nodes: int, default=None\n\nGrow trees with ``max_leaf_nodes`` in best-first fashion.\nBest nodes are defined as relative reduction in impurity.\nIf None then unlimited number of leaf nodes.\nNone\n\n\n\nmin_impurity_decrease min_impurity_decrease: float, default=0.0\n\nA node will be split if this split induces a decrease of the impurity\ngreater than or equal to this value.\n\nThe weighted impurity decrease equation is the following::\n\nN_t / N * (impurity - N_t_R / N_t * right_impurity\n- N_t_L / N_t * left_impurity)\n\nwhere ``N`` is the total number of samples, ``N_t`` is the number of\nsamples at the current node, ``N_t_L`` is the number of samples in the\nleft child, and ``N_t_R`` is the number of samples in the right child.\n\n``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\nif ``sample_weight`` is passed.\n\n.. versionadded:: 0.19\n0.0\n\n\n\nbootstrap bootstrap: bool, default=True\n\nWhether bootstrap samples are used when building trees. If False, the\nwhole dataset is used to build each tree.\nTrue\n\n\n\noob_score oob_score: bool or callable, default=False\n\nWhether to use out-of-bag samples to estimate the generalization score.\nBy default, :func:`~sklearn.metrics.accuracy_score` is used.\nProvide a callable with signature `metric(y_true, y_pred)` to use a\ncustom metric. Only available if `bootstrap=True`.\n\nFor an illustration of out-of-bag (OOB) error estimation, see the example\n:ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`.\nFalse\n\n\n\nn_jobs n_jobs: int, default=None\n\nThe number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n:meth:`decision_path` and :meth:`apply` are all parallelized over the\ntrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\ncontext. ``-1`` means using all processors. See :term:`Glossary\n` for more details.\nNone\n\n\n\nrandom_state random_state: int, RandomState instance or None, default=None\n\nControls both the randomness of the bootstrapping of the samples used\nwhen building trees (if ``bootstrap=True``) and the sampling of the\nfeatures to consider when looking for the best split at each node\n(if ``max_features &lt; n_features``).\nSee :term:`Glossary ` for details.\n42\n\n\n\nverbose verbose: int, default=0\n\nControls the verbosity when fitting and predicting.\n0\n\n\n\nwarm_start warm_start: bool, default=False\n\nWhen set to ``True``, reuse the solution of the previous call to fit\nand add more estimators to the ensemble, otherwise, just fit a whole\nnew forest. See :term:`Glossary ` and\n:ref:`tree_ensemble_warm_start` for details.\nFalse\n\n\n\nclass_weight class_weight: {\"balanced\", \"balanced_subsample\"}, dict or list of dicts, default=None\n\nWeights associated with classes in the form ``{class_label: weight}``.\nIf not given, all classes are supposed to have weight one. For\nmulti-output problems, a list of dicts can be provided in the same\norder as the columns of y.\n\nNote that for multioutput (including multilabel) weights should be\ndefined for each class of every column in its own dict. For example,\nfor four-class multilabel classification weights should be\n[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n[{1:1}, {2:5}, {3:1}, {4:1}].\n\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``\n\nThe \"balanced_subsample\" mode is the same as \"balanced\" except that\nweights are computed based on the bootstrap sample for every tree\ngrown.\n\nFor multi-output, the weights of each column of y will be multiplied.\n\nNote that these weights will be multiplied with sample_weight (passed\nthrough the fit method) if sample_weight is specified.\nNone\n\n\n\nccp_alpha ccp_alpha: non-negative float, default=0.0\n\nComplexity parameter used for Minimal Cost-Complexity Pruning. The\nsubtree with the largest cost complexity that is smaller than\n``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n:ref:`minimal_cost_complexity_pruning` for details. See\n:ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\nfor an example of such pruning.\n\n.. versionadded:: 0.22\n0.0\n\n\n\nmax_samples max_samples: int or float, default=None\n\nIf bootstrap is True, the number of samples to draw from X\nto train each base estimator.\n\n- If None (default), then draw `X.shape[0]` samples.\n- If int, then draw `max_samples` samples.\n- If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n`max_samples` should be in the interval `(0.0, 1.0]`.\n\n.. versionadded:: 0.22\nNone\n\n\n\nmonotonic_cst monotonic_cst: array-like of int of shape (n_features), default=None\n\nIndicates the monotonicity constraint to enforce on each feature.\n- 1: monotonic increase\n- 0: no constraint\n- -1: monotonic decrease\n\nIf monotonic_cst is None, no constraints are applied.\n\nMonotonicity constraints are not supported for:\n- multiclass classifications (i.e. when `n_classes &gt; 2`),\n- multioutput classifications (i.e. when `n_outputs_ &gt; 1`),\n- classifications trained on data with missing values.\n\nThe constraints hold over the probability of the positive class.\n\nRead more in the :ref:`User Guide `.\n\n.. versionadded:: 1.4\nNone\n\n\n\n\n            \n        \n    \n\n\n\n\n28.4.3 특성 중요도 확인\n\nimport pandas as pd\n\nimportance = pd.Series(\n    rf.feature_importances_,\n    index=X.columns\n).sort_values(ascending=False)\n\nimportance\n\nbill_length_mm       0.419905\nflipper_length_mm    0.324162\nbill_depth_mm        0.172930\nbody_mass_g          0.083002\ndtype: float64\n\n\n\n\n28.4.4 해석 포인트\n\n값이 클수록 모델 분기에서 자주 사용됨\n인과 관계는 아님\n변수 간 상관성이 있으면 왜곡 가능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#permutation-importance",
    "href": "part3/10. 모델 해석.html#permutation-importance",
    "title": "28  모델 해석",
    "section": "28.5 Permutation Importance",
    "text": "28.5 Permutation Importance\n\n28.5.1 개념\n\n특정 변수를 무작위로 섞었을 때\n성능이 얼마나 떨어지는지 측정\n\n모델 종류에 상관없이 사용 가능\n\n\n28.5.2 예제\n\nfrom sklearn.inspection import permutation_importance\n\nresult = permutation_importance(\n    rf,\n    X,\n    y,\n    n_repeats=10,\n    random_state=42\n)\n\nperm_importance = pd.Series(\n    result.importances_mean,\n    index=X.columns\n).sort_values(ascending=False)\n\nperm_importance\n\nbill_length_mm       0.323123\nbill_depth_mm        0.139039\nflipper_length_mm    0.096697\nbody_mass_g          0.023423\ndtype: float64\n\n\n실무에서는 기본 feature_importance보다 신뢰도 높음",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#shap-개요",
    "href": "part3/10. 모델 해석.html#shap-개요",
    "title": "28  모델 해석",
    "section": "28.6 SHAP 개요",
    "text": "28.6 SHAP 개요\nshap는 Python 3.6 ~ 3.10에서 지원한다. 이외 버전인 경우 대체, 보완 방법을 사용한다.\n\n28.6.1 SHAP (SHapley Additive exPlanations)\n\n게임 이론 기반\n각 특성이 예측값에 기여한 정도를 계산\n전역 + 국소 해석 모두 가능\n\n현재 가장 표준적인 모델 해석 방법",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#shap-값의-핵심-개념",
    "href": "part3/10. 모델 해석.html#shap-값의-핵심-개념",
    "title": "28  모델 해석",
    "section": "28.7 SHAP 값의 핵심 개념",
    "text": "28.7 SHAP 값의 핵심 개념\n\n기준값(base value)에서 출발\n각 특성이 예측값을 얼마나 밀었는지\n모든 기여도의 합 = 최종 예측",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#shap-적용-tree-기반",
    "href": "part3/10. 모델 해석.html#shap-적용-tree-기반",
    "title": "28  모델 해석",
    "section": "28.8 SHAP 적용 (Tree 기반)",
    "text": "28.8 SHAP 적용 (Tree 기반)\nimport shap\n\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X)\n\n28.8.1 전역 해석 – Summary Plot\nshap.summary_plot(\n    shap_values,\n    X,\n    plot_type=\"bar\"\n)\n의미\n\n전체 모델에서 중요한 변수 순위\n방향성은 알 수 없음 (막대형)\n\n\n\n28.8.2 분포 기반 Summary Plot\nshap.summary_plot(\n    shap_values,\n    X\n)\n해석 포인트\n\n색상: 특성 값 크기\n위치: 예측에 미친 영향 방향\n변수별 영향 분포 확인 가능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#개별-예측-해석-local-explanation",
    "href": "part3/10. 모델 해석.html#개별-예측-해석-local-explanation",
    "title": "28  모델 해석",
    "section": "28.9 개별 예측 해석 (Local Explanation)",
    "text": "28.9 개별 예측 해석 (Local Explanation)\n\n28.9.1 특정 샘플 선택\ni = 0\n\n\n28.9.2 Force Plot\nshap.force_plot(\n    explainer.expected_value[0],\n    shap_values[0][i],\n    X.iloc[i]\n)\n“이 펭귄이 왜 이 종으로 분류됐는지” 설명 가능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#shap-사용-시-주의점",
    "href": "part3/10. 모델 해석.html#shap-사용-시-주의점",
    "title": "28  모델 해석",
    "section": "28.10 SHAP 사용 시 주의점",
    "text": "28.10 SHAP 사용 시 주의점\n\n계산 비용 큼\n데이터 크기 클 경우 샘플링 필요\n모델 구조에 따라 explainer 선택\n\n\n\n\n모델\nExplainer\n\n\n\n\nTree 모델\nTreeExplainer\n\n\n일반 모델\nKernelExplainer",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#특성-중요도-vs-shap-비교",
    "href": "part3/10. 모델 해석.html#특성-중요도-vs-shap-비교",
    "title": "28  모델 해석",
    "section": "28.11 특성 중요도 vs SHAP 비교",
    "text": "28.11 특성 중요도 vs SHAP 비교\n\n\n\n항목\n특성 중요도\nSHAP\n\n\n\n\n전역 해석\n가능\n가능\n\n\n국소 해석\n불가\n가능\n\n\n방향성\n불가\n가능\n\n\n계산 비용\n낮음\n높음",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#정리",
    "href": "part3/10. 모델 해석.html#정리",
    "title": "28  모델 해석",
    "section": "28.12 정리",
    "text": "28.12 정리\n\n모델 성능만 보고 끝내지 말 것\n의사결정 모델은 반드시 해석 포함\n보고서에는 SHAP 시각화 적극 활용",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#python-3.12-기준-모델-해석",
    "href": "part3/10. 모델 해석.html#python-3.12-기준-모델-해석",
    "title": "28  모델 해석",
    "section": "28.13 Python 3.12 기준 모델 해석",
    "text": "28.13 Python 3.12 기준 모델 해석\nPython 3.12 환경에서는 SHAP 대신 Permutation Importance + PDP 조합을 사용한다.\n\n28.13.1 Permutation Importance (전역 중요도)\n\n28.13.1.1 개념\n특정 변수를 무작위로 섞었을 때모델 성능이 얼마나 감소하는지로 중요도를 판단한다.\n\n\n28.13.1.2 언제 쓰나\n모델이 어떤 변수를 실제로 의존하는지 알고 싶을 때 트리·SVM·앙상블 등 모델 종류 무관하게 사용할 수 있다.\n\n\n28.13.1.3 예제\n\nfrom palmerpenguins import load_penguins\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nimport pandas as pd\n\ndf = load_penguins().dropna()\n\nX = df[['bill_length_mm', 'bill_depth_mm',\n        'flipper_length_mm', 'body_mass_g']]\ny = df['species']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42\n)\n\nmodel = RandomForestClassifier(random_state=42)\nmodel.fit(X_train, y_train)\n\nresult = permutation_importance(\n    model, X_test, y_test,\n    n_repeats=10, random_state=42\n)\n\nperm_importance = pd.Series(\n    result.importances_mean,\n    index=X.columns\n).sort_values(ascending=False)\n\nperm_importance\n\nbill_length_mm       0.307143\nbill_depth_mm        0.146429\nflipper_length_mm    0.033333\nbody_mass_g          0.001190\ndtype: float64\n\n\n\n\n28.13.1.4 해석 포인트\n\n값이 클수록 예측 성능에 중요한 변수\n“모델이 실제로 쓰는 변수”를 보여줌\n\n\n\n\n28.13.2 Partial Dependence Plot (PDP)\n\n28.13.2.1 개념\n특정 변수가 변할 때 예측값이 평균적으로 어떻게 변하는지 시각화한다.\n\n\n28.13.2.2 언제 쓰나\n변수의 영향 방향(+/−)을 알고 싶을 때, 전역적인 경향 설명용으로 적합하다.\n\n\n28.13.2.3 예제\n\nfrom sklearn.inspection import PartialDependenceDisplay\nimport matplotlib.pyplot as plt\n\nPartialDependenceDisplay.from_estimator(\n    model,\n    X_train,\n    features=['flipper_length_mm', 'body_mass_g'],\n    target='Gentoo'\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n28.13.3 해석 포인트\n\n“날개 길이가 증가할수록 특정 종으로 분류될 확률이 증가”\n평균 효과 → 개별 샘플 설명은 아님",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#정리-1",
    "href": "part3/10. 모델 해석.html#정리-1",
    "title": "28  모델 해석",
    "section": "28.14 정리",
    "text": "28.14 정리\n\nPermutation Importance\n\n무엇이 중요한가 (전역)\n\nPDP\n\n어떻게 영향을 주는가 (방향·형태)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  }
]