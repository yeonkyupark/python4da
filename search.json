[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "파이썬 데이터 분석",
    "section": "",
    "text": "들어가기\n기계학습으로 데이터를 분석한다. 분석 도구는 파이썬으로 데이터 전처리, 통계 분석, 기계학습 모델링 및 평가까지 코드 중심으로 구성된다. 실무에 필요한 내용을 정리하여 이론적, 학문적으로 미흡한 부분이 있다. 부족한 부분은 향후 성능 개선과 향상된 알고리즘 등으로 수정키로 한다.\n파이썬 버전은 3.12 기반으로 numpy, pandas, sklearn, scipy, stats와 같은 기본적인 라이브러리를 사용한다.\n전체 목차는 다음과 같다.",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-데이터-전처리-분포-이해",
    "href": "index.html#부.-데이터-전처리-분포-이해",
    "title": "파이썬 데이터 분석",
    "section": "1부. 데이터 전처리 & 분포 이해",
    "text": "1부. 데이터 전처리 & 분포 이해\n\n1.1 데이터 로드 및 구조 점검\n\n데이터 로드 (CSV, Excel, SQL)\n데이터 구조 및 타입 확인\n\n\n\n1.2 탐색적 데이터 분석(EDA)\n\n기술통계량 요약\n분포 시각화 (히스토그램, KDE, 박스플롯)\n상관관계 탐색\n\n\n\n1.3 데이터 분포 이해\n\n연속형·이산형 데이터 분포\n왜도와 첨도\n분포 해석을 통한 전처리 전략\n\n\n\n1.4 결측치 처리\n\n결측치 탐지 및 요약\n단순 대치 기법\n고급 대치 기법 (KNN, Iterative)\n\n\n\n1.5 이상치 탐지\n\n정규분포 기반 이상치\nIQR 기반 이상치\n밀도 기반 이상치 (LOF, DBSCAN)\n트리 기반 이상치 (IsolationForest)\n\n\n\n1.6 스케일링\n\n정규화 (Min-Max)\n표준화 (Standard, Robust, MaxAbs)\n\n\n\n1.7 데이터 분포 변환\n\n로그 변환\nBox-Cox 변환\nYeo-Johnson 변환\n분위수 변환\n변환 전·후 분포 비교\n\n\n\n1.8 범주형 데이터 처리\n\n명목형 인코딩\n순서형 인코딩\n\n\n\n1.9 연속형 데이터 범주화\n\n구간 분할 (cut, qcut, KBins)\n\n\n\n1.10 불균형 데이터 처리\n\n오버샘플링\n언더샘플링\n\n\n\n1.11 피처 엔지니어링\n\n다항 특성 생성\n집계 및 롤링 특성",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "href": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "title": "파이썬 데이터 분석",
    "section": "2부. 통계 기반 데이터 분석 & 가설검정",
    "text": "2부. 통계 기반 데이터 분석 & 가설검정\n\n2.1 확률분포와 표본\n\n연속형 확률분포\n이산형 확률분포\n표본 분포 개념\n\n\n\n2.2 정규성 검정\n\nShapiro-Wilk 검정\nKolmogorov-Smirnov 검정\nAnderson-Darling 검정\nQ-Q plot 해석\n\n\n\n2.3 등분산성 검정\n\nLevene 검정\nBartlett 검정\nFligner-Killeen 검정\n\n\n\n2.4 적합성 검정 & 독립성 검정\n\n카이제곱 적합성 검정\n분포 적합성 검정\n카이제곱 독립성 검정\nF 검정 (분산 비교)\n\n\n\n2.5 평균 비교 검정\n\n단일 표본 t-검정\n독립 표본 t-검정\n대응 표본 t-검정\n\n\n\n2.6 분산분석\n\n일원 분산 분석 (One-way ANOVA)\n이원 분산 분석 (Two-way ANOVA)\n사후 검정\n\n\n\n2.7 비모수 검정\n\nMann-Whitney U 검정\nWilcoxon 순위합 검정\nKruskal-Wallis 검정\nFriedman 검정\n\n\n\n2.8 상관 분석\n\nPearson 상관 분석\nSpearman 순위 상관",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-머신러닝-모델링-평가",
    "href": "index.html#부.-머신러닝-모델링-평가",
    "title": "파이썬 데이터 분석",
    "section": "3부. 머신러닝 모델링 & 평가",
    "text": "3부. 머신러닝 모델링 & 평가\n\n3.1 데이터 분할 및 검증\n\n학습·검증·테스트 분할\n교차 검증 기법\n\n\n\n3.2 특성 선택\n\n필터 방법\n래퍼 방법\n임베디드 방법\n\n\n\n3.3 차원 축소\n\n선형 차원 축소 (PCA)\n비선형 차원 축소 (t-SNE, UMAP)\n\n\n\n3.4 회귀 모델\n\n선형 회귀\n정규화 회귀 (Ridge, Lasso, ElasticNet)\n\n\n\n3.5 분류 모델\n\n선형 분류 모델\n거리 기반 모델\n트리 및 앙상블 모델\n\n\n\n3.6 서포트 벡터 머신\n\nSVM 분류\n\n\n\n3.7 군집 분석\n\n분할 기반 군집\n밀도 기반 군집\n혼합 모델 군집\n\n\n\n3.8 모델 성능 평가\n\n분류 성능 평가\n회귀 성능 평가\n\n\n\n3.9 파이프라인 & 자동화\n\n파이프라인 구성\n하이퍼파라미터 최적화\n\n\n\n3.10 모델 해석\n\n특성 중요도\nSHAP 기반 모델 해석",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html",
    "href": "part1/01. 데이터 로드 및 구조 점검.html",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1 데이터 로드\n데이터 분석에 있어 가장 먼저 하는 작업이 데이터를 수집하는 것이다. 데이터 수집 방법에는 여러 가지가 있을 수 있고 대부분 로컬 파일이나 데이터베이스 또는 다른 시스템의 산출물에서 확보한다. 또한 데이터 원천에서 수집되는 데이터 형태는 시스템이나 상황에 따라 다양할 수 있지만 일반적으로 CSV나 Excel 형태로 처리하게 된다. 물론 API나 데이터베이스에 직접 Query해서 취합할 수도 있다.\n가장 일반적인 자료 형태인 CSV 파일과 Excel 파일을 메모리에 적재하는 방법을 알아 본다. 아래는 예제로 사용할 palmerpenguins 데이터셋이다.\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\ndf = load_penguins()\n\ndf.head()\n\ndf.to_csv(\"penguins.csv\", index=False) # CSV 파일로 저장\ndf.to_excel(\"./penguins.xlsx\", index=False) # Excel 파일로 저장\n\n# sqlite DB\nimport sqlite3\nconn = sqlite3.connect(\"penguins.db\")\nresult = df.to_sql(\"penguins\", conn, if_exists=\"replace\", index=False)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1.1 CSV 파일 로드\n\nimport pandas as pd\n\ndf_from_csv = pd.read_csv(\"penguins.csv\")\n\ndf_from_csv.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.2 Excel 파일 로드\n\nimport pandas as pd\n\ndf_from_excel = pd.read_excel(\"penguins.xlsx\")\n\ndf_from_excel.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.3 SQLite 데이터베이스 로드\n\nimport sqlite3\n\nconn = sqlite3.connect(\"penguins.db\")\n\nquery = \"SELECT * FROM penguins\"\ndf_from_sql = pd.read_sql(query, conn)\n\ndf_from_sql.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "1.2 데이터 구조 및 타입 확인",
    "text": "1.2 데이터 구조 및 타입 확인\n수집된 데이터에서 가장 먼저 확인하는 것은 데이터의 크기와 컬럼에 대한 정보이다.\n\n1.2.1 데이터 크기와 기본 정보\n\nimport pandas as pd\n\ndf = pd.read_csv(\"penguins.csv\")\n\ndf.shape\n\n(344, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "",
    "text": "2.1 라이브러리 로드 & 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# seaborn 내장 penguins 데이터셋 로드\ndf = sns.load_dataset(\"penguins\")\n\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.2 데이터 구조 확인",
    "text": "2.2 데이터 구조 확인\n\ndf.shape\n\n(344, 7)\n\n\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \ndtypes: float64(4), str(3)\nmemory usage: 18.9 KB\n\n\n확인 포인트\n\n행(row), 열(column) 개수\n변수 타입 (수치형 / 범주형)\n결측치 여부",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.3 기초 통계량 확인",
    "text": "2.3 기초 통계량 확인\n\n2.3.1 수치형 변수 요약\n\ndf.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n\n\n2.3.2 범주형 변수 요약\n\ndf.describe(include=\"object\")\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\n\n\n\n\ncount\n344\n344\n333\n\n\nunique\n3\n3\n2\n\n\ntop\nAdelie\nBiscoe\nMale\n\n\nfreq\n152\n168\n168",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.4 결측치 탐색",
    "text": "2.4 결측치 탐색\n\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\n\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.5 범주형 변수 탐색",
    "text": "2.5 범주형 변수 탐색\n\n2.5.1 종(species) 분포\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\nsns.countplot(data=df, x=\"species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.5.2 성별(sex) 분포\n\nsns.countplot(data=df, x=\"sex\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.6 수치형 변수 분포 확인",
    "text": "2.6 수치형 변수 분포 확인\n\n2.6.1 히스토그램\n\ndf.hist(bins=20, figsize=(10, 8))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.6.2 박스플롯 (이상치 탐색)\n\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=df, x=\"species\", y=\"body_mass_g\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.7 변수 간 관계 탐색",
    "text": "2.7 변수 간 관계 탐색\n\n2.7.1 두 수치형 변수 관계\n\nsns.scatterplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.7.2 페어플롯\n\nsns.pairplot(\n    df,\n    hue=\"species\",\n    diag_kind=\"hist\"\n)\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.8 그룹별 통계 확인",
    "text": "2.8 그룹별 통계 확인\n\ndf.groupby(\"species\")[[\"bill_length_mm\", \"body_mass_g\"]].mean()\n\n\n\n\n\n\n\n\nbill_length_mm\nbody_mass_g\n\n\nspecies\n\n\n\n\n\n\nAdelie\n38.791391\n3700.662252\n\n\nChinstrap\n48.833824\n3733.088235\n\n\nGentoo\n47.504878\n5076.016260",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#간단한-eda-결론-예시",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#간단한-eda-결론-예시",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.9 간단한 EDA 결론 예시",
    "text": "2.9 간단한 EDA 결론 예시\n\n종(species)에 따라 부리 길이와 몸무게 분포가 뚜렷하게 구분된다.\nAdelie 종은 상대적으로 몸무게와 부리 길이가 작다.\n일부 변수에 결측치가 존재하므로 분석 전 처리 전략이 필요하다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#eda-기본-흐름-요약",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#eda-기본-흐름-요약",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.10 EDA 기본 흐름 요약",
    "text": "2.10 EDA 기본 흐름 요약\n\n데이터 구조 확인 → info(), shape\n기초 통계 → describe()\n결측치 → isna()\n분포 → 히스토그램, 박스플롯\n관계 → 산점도, pairplot\n그룹 비교 → groupby()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html",
    "href": "part1/03. 결측치 처리.html",
    "title": "3  결측치 처리",
    "section": "",
    "text": "3.1 데이터 로드 & 결측치 현황 재확인\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")\n\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "href": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "title": "3  결측치 처리",
    "section": "3.2 결측치 비율 확인",
    "text": "3.2 결측치 비율 확인\n\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "href": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "title": "3  결측치 처리",
    "section": "3.3 결측치 제거 (행 삭제)",
    "text": "3.3 결측치 제거 (행 삭제)\n\n3.3.1 전체 결측치가 있는 행 제거\n\ndf_drop_all = df.dropna()\ndf_drop_all.shape\n\n(333, 7)\n\n\n\n\n3.3.2 결측치가 3개 이상인 행 제거\n\ndf[\"na_count\"] = df.isna().sum(axis=1)\ndf_row_filtered = df[df[\"na_count\"] &lt; 3].drop(columns=\"na_count\")\ndf_row_filtered.shape\n\n(342, 7)\n\n\n\n# 결측치가 전체 컬럼에서 2개까지만 허용()\ndf_thresh = df.dropna(thresh=len(df.columns)-2)\n\n\n\n3.3.3 특정 컬럼 기준 제거\n\ndf_drop_sex = df.dropna(subset=[\"sex\"])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.4 수치형 변수 결측치 대체",
    "text": "3.4 수치형 변수 결측치 대체\n\n3.4.1 평균(mean) 대체\n특정 컬럼에 대한 평균 대체\n\ndf_mean = df.copy()\ndf_mean[\"bill_length_mm\"] = df_mean[\"bill_length_mm\"].fillna(\n    df_mean[\"bill_length_mm\"].mean()\n)\n\n다중 컬럼에 대한 처리 - 1\n\ndf_cols = df.copy()\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\n\ndf_cols[num_cols] = df_cols[num_cols].fillna(df[num_cols].mean())\n\n다중 컬럼에 대한 처리 - 2\n\nfor col in num_cols:\n    mean_value = df[col].mean()\n    df[col] = df[col].fillna(mean_value)\n\n범주별 다중 컬럼 처리\n\ndf_group = df.copy()\n\ndf_group[num_cols] = df_group[num_cols].fillna(\n    df_group.groupby(\"species\")[num_cols].transform(\"mean\")\n)\n\n\n\n3.4.2 중앙값(median) 대체\n\ndf_median = df.copy()\ndf_median[\"bill_depth_mm\"] = df_median[\"bill_depth_mm\"].fillna(\n    df_median[\"bill_depth_mm\"].median()\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.5 범주형 변수 결측치 대체",
    "text": "3.5 범주형 변수 결측치 대체\n\n3.5.1 최빈값(mode) 대체\n\ndf_mode = df.copy()\nmode_sex = df_mode[\"sex\"].mode()[0]\ndf_mode[\"sex\"] = df_mode[\"sex\"].fillna(mode_sex)\n\n\n\n3.5.2 명시적 범주 추가\n\ndf_category = df.copy()\ndf_category[\"sex\"] = df_category[\"sex\"].fillna(\"Unknown\")\n\n\ndf_mode = df.copy()\n\ncat_cols = [\"sex\", \"island\"]\n\nfor col in cat_cols:\n    mode_value = df_mode[col].mode()[0]\n    df_mode[col] = df_mode[col].fillna(mode_value)\n\n\ndf_mode = df.copy()\n\ndf_mode[\"sex\"] = df_mode[\"sex\"].fillna(\n    df_mode.groupby(\"species\")[\"sex\"].transform(\n        lambda x: x.mode()[0]\n    )\n)\n\n\ndf_mode = df.copy()\n\ncat_cols = [\"sex\", \"island\"]\n\nfor col in cat_cols:\n    df_mode[col] = df_mode[col].fillna(\n        df_mode.groupby(\"species\")[col].transform(\n            lambda x: x.mode()[0]\n        )\n    )",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "href": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "title": "3  결측치 처리",
    "section": "3.6 결측치 처리 후 검증",
    "text": "3.6 결측치 처리 후 검증\n\ndf_group.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        0\nbill_depth_mm         0\nflipper_length_mm     0\nbody_mass_g           0\nsex                  11\nna_count              0\ndtype: int64\n\n\n\ndf_group.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     344 non-null    float64\n 3   bill_depth_mm      344 non-null    float64\n 4   flipper_length_mm  344 non-null    float64\n 5   body_mass_g        344 non-null    float64\n 6   sex                333 non-null    str    \n 7   na_count           344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-처리-권장-순서",
    "href": "part1/03. 결측치 처리.html#결측치-처리-권장-순서",
    "title": "3  결측치 처리",
    "section": "3.7 결측치 처리 권장 순서",
    "text": "3.7 결측치 처리 권장 순서\n\n결측치 현황 파악\n제거 가능한 행/열 판단\n수치형 → 그룹 기반 대체\n범주형 → mode 또는 Unknown\n최종 검증",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html",
    "href": "part1/04. 이상치 탐지.html",
    "title": "4  이상치 탐지",
    "section": "",
    "text": "4.1 분포 기반 이상치 탐지\n분석 대상은 수치형 변수만 사용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "",
    "text": "4.1.1 박스플롯으로 이상치 확인\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\ndf_num_scaled = pd.DataFrame(scale.fit_transform(df_num[num_cols]),\n                              columns=df_num.columns)\n\ndf_num_scaled.boxplot(figsize=(10, 5))\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.2 IQR 기반 이상치 탐지",
    "text": "4.2 IQR 기반 이상치 탐지\n\n4.2.1 IQR 계산\n\nQ1 = df_num.quantile(0.25)\nQ3 = df_num.quantile(0.75)\nIQR = Q3 - Q1\n\n\n\n4.2.2 이상치 조건 정의\n\noutlier_condition = (df_num &lt; (Q1 - 1.5 * IQR)) | (df_num &gt; (Q3 + 1.5 * IQR))\n\n\n\n4.2.3 이상치가 있는 행 확인\n\noutlier_rows = outlier_condition.any(axis=1)\ndf[outlier_rows]\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n\n\n\n\n\n컬럼 중 하나라도 이상치면 해당 행 전체를 이상치로 판단\n\n\n4.2.4 이상치 제거\n\ndf_iqr_clean = df[~outlier_rows]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.3 Z-score 기반 이상치 탐지",
    "text": "4.3 Z-score 기반 이상치 탐지\n\nfrom scipy.stats import zscore\n\n\nz_scores = df_num.apply(zscore)\n\n\n4.3.1 임계값 기준 이상치 탐지 (|z| &gt; 3)\n\noutlier_z = (np.abs(z_scores) &gt; 3).any(axis=1)\ndf_z_clean = df[~outlier_z]\n\n계산이 간단한 장점이 있으나 분포 왜곡에 취약한 단점 존재",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.4 범주별 이상치 탐지",
    "text": "4.4 범주별 이상치 탐지\n\n4.4.1 species별 IQR 이상치 탐지\n\ndef iqr_outlier_by_group(group):\n    Q1 = group[num_cols].quantile(0.25)\n    Q3 = group[num_cols].quantile(0.75)\n    IQR = Q3 - Q1\n\n    condition = (group[num_cols] &lt; (Q1 - 1.5 * IQR)) | \\\n                (group[num_cols] &gt; (Q3 + 1.5 * IQR))\n    return group[~condition.any(axis=1)]\n\n\ndf_group_iqr_clean = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(iqr_outlier_by_group)\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "href": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "title": "4  이상치 탐지",
    "section": "4.5 밀도 기반 이상치 탐지 (LOF)",
    "text": "4.5 밀도 기반 이상치 탐지 (LOF)\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n\ndf_lof = df_num.dropna()\n\nlof = LocalOutlierFactor(n_neighbors=20)\noutlier_lof = lof.fit_predict(df_lof)\n\n\ndf_lof_clean = df.loc[df_lof.index][outlier_lof == 1]\n\n주변 데이터와 너무 다른 밀도를 가지면 이상치로 판단하는 방식으로 비선형 데이터에 강함",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "href": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "title": "4  이상치 탐지",
    "section": "4.6 트리 기반 이상치 탐지 (Isolation Forest)",
    "text": "4.6 트리 기반 이상치 탐지 (Isolation Forest)\n\nfrom sklearn.ensemble import IsolationForest\n\n\niso = IsolationForest(contamination=0.05, random_state=42)\noutlier_if = iso.fit_predict(df_lof)\n\n\ndf_if_clean = df.loc[df_lof.index][outlier_if == 1]\n\n이상치는 트리 분류 기준으로 쉽게 분리가 된다는 가정하에 이상치를 탐지하는 방식으로 고차원 데이터에 강함하고 실무에서 자주 사용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#단계별-선택-가이드",
    "href": "part1/04. 이상치 탐지.html#단계별-선택-가이드",
    "title": "4  이상치 탐지",
    "section": "4.7 단계별 선택 가이드",
    "text": "4.7 단계별 선택 가이드\n\n\n\n단계\n방법\n언제 쓰나\n\n\n\n\n1\n시각화\n이해, 설명\n\n\n2\nIQR\n기본, 안전\n\n\n3\nZ-score\n정규분포 가정\n\n\n4\n그룹별 IQR\n가장 실무적\n\n\n5\nLOF\n비선형, 밀도\n\n\n6\nIsolationForest\n대규모, 고급",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html",
    "href": "part1/05. 스케일링.html",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1 스케일링 이유\n예제 데이터셋 적재",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#스케일링-이유",
    "href": "part1/05. 스케일링.html#스케일링-이유",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1.1 스케일 차이 확인\n\ndf_num.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n변수별 측정 단위가 다름(상호 비교가 어려움)\n- 거리 기반 모델(KNN, SVM, K-means)에서는 큰 값이 모델을 지배",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "href": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "title": "5  스케일링",
    "section": "5.2 Min-Max 스케일링 (정규화)",
    "text": "5.2 Min-Max 스케일링 (정규화)\n모든 값을 0~1 범위로 변환\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nscaler_minmax = MinMaxScaler()\ndf_minmax = scaler_minmax.fit_transform(df_num)\n\n\ndf_minmax = pd.DataFrame(df_minmax, columns=num_cols)\nprint(df_minmax.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.429888       0.482282           0.490088     0.417154\nstd          0.198530       0.235094           0.238334     0.222765\nmin          0.000000       0.000000           0.000000     0.000000\n25%          0.259091       0.297619           0.305085     0.236111\n50%          0.449091       0.500000           0.423729     0.375000\n75%          0.596364       0.666667           0.694915     0.569444\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n변수 범위가 고정(0~1) 되며 이상치에 매우 민감한 특징\n참고. 아래와 같이 수식을 통해 직접 구현\n\\[\nx' = \\frac{x-x_{min}}{x_{max} - x_{min}}\n\\]\n\ndf_minmax_manual = df_num.copy()\n\nfor col in num_cols:\n    col_min = df_minmax_manual[col].min()\n    col_max = df_minmax_manual[col].max()\n\n    df_minmax_manual[col] = (df_minmax_manual[col] - col_min) \\\n                            / (col_max - col_min)\n\n(a, b) 구간으로 일반화한 수식\n\\[\nx' = a + \\frac{x-x_{min}}{x_{max} - x_{min}}\\times(b-a)\n\\]\n\ndef minmax_scale_ab(series, a, b):\n    x_min = series.min()\n    x_max = series.max()\n\n    return a + (series - x_min) * (b - a) / (x_max - x_min)\n\n\nscaler_genernal = MinMaxScaler(feature_range=(-1, 1))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#표준화-standard-scaling",
    "href": "part1/05. 스케일링.html#표준화-standard-scaling",
    "title": "5  스케일링",
    "section": "5.3 표준화 (Standard Scaling)",
    "text": "5.3 표준화 (Standard Scaling)\n변수 변위를 평균 0, 표준편차 1로 변환\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nscaler_std = StandardScaler()\ndf_std = scaler_std.fit_transform(df_num)\ndf_std = pd.DataFrame(df_std, columns=num_cols)\n\nprint(df_std.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm   body_mass_g\ncount    3.420000e+02   3.420000e+02       3.420000e+02  3.420000e+02\nmean     8.310441e-17  -1.412775e-15      -8.310441e-16  4.155221e-17\nstd      1.001465e+00   1.001465e+00       1.001465e+00  1.001465e+00\nmin     -2.168526e+00  -2.054446e+00      -2.059320e+00 -1.875362e+00\n25%     -8.615697e-01  -7.866355e-01      -7.773731e-01 -8.138982e-01\n50%      9.686524e-02   7.547549e-02      -2.788381e-01 -1.895079e-01\n75%      8.397670e-01   7.854492e-01       8.606705e-01  6.846384e-01\nmax      2.875868e+00   2.205397e+00       2.142618e+00  2.620248e+00\n\n\n가장 널리 사용되며 변수가 정규분포 가정(완전하지 않아도 OK)하에 수행\n참고. 아래와 같이 수식을 통해 직접 구현\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n\ndf_std_manual = df_num.copy()\n\nfor col in num_cols:\n    mean = df_std_manual[col].mean()\n    std = df_std_manual[col].std()\n\n    df_std_manual[col] = (df_std_manual[col] - mean) \\\n                         / std",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#robust-scaling-이상치-대응",
    "href": "part1/05. 스케일링.html#robust-scaling-이상치-대응",
    "title": "5  스케일링",
    "section": "5.4 Robust Scaling (이상치 대응)",
    "text": "5.4 Robust Scaling (이상치 대응)\n중앙값과 IQR 기준으로 변환을 수행하여 이상치에 대응\n\nfrom sklearn.preprocessing import RobustScaler\n\n\nscaler_robust = RobustScaler()\ndf_robust = scaler_robust.fit_transform(df_num)\ndf_robust = pd.DataFrame(df_robust, columns=num_cols)\n\nprint(df_robust.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount    3.420000e+02     342.000000         342.000000   342.000000\nmean    -5.693479e-02      -0.048010           0.170226     0.126462\nstd      5.886344e-01       0.637030           0.611379     0.668295\nmin     -1.331536e+00      -1.354839          -1.086957    -1.125000\n25%     -5.633423e-01      -0.548387          -0.304348    -0.416667\n50%     -3.833739e-16       0.000000           0.000000     0.000000\n75%      4.366577e-01       0.451613           0.695652     0.583333\nmax      1.633423e+00       1.354839           1.478261     1.875000\n\n\n이상치 영향 최소화",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#maxabs-scaling",
    "href": "part1/05. 스케일링.html#maxabs-scaling",
    "title": "5  스케일링",
    "section": "5.5 MaxAbs Scaling",
    "text": "5.5 MaxAbs Scaling\n최대 절댓값으로 나눔\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\n\nscaler_maxabs = MaxAbsScaler()\ndf_maxabs = scaler_maxabs.fit_transform(df_num)\ndf_maxabs = pd.DataFrame(df_maxabs, columns=num_cols)\n\nprint(df_maxabs.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.736945       0.797729           0.869763     0.666945\nstd          0.091604       0.091851           0.060873     0.127294\nmin          0.538591       0.609302           0.744589     0.428571\n25%          0.658138       0.725581           0.822511     0.563492\n50%          0.745805       0.804651           0.852814     0.642857\n75%          0.813758       0.869767           0.922078     0.753968\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n희소 행렬에 유리하며 값 부호 유지하는 특징",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#범주별-스케일링",
    "href": "part1/05. 스케일링.html#범주별-스케일링",
    "title": "5  스케일링",
    "section": "5.6 범주별 스케일링",
    "text": "5.6 범주별 스케일링\n\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef scale_by_species(group):\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(group[num_cols])\n    return pd.DataFrame(scaled, columns=num_cols, index=group.index)\n\n\ndf_scaled_group = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(scale_by_species)\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#파이프라인-적용",
    "href": "part1/05. 스케일링.html#파이프라인-적용",
    "title": "5  스케일링",
    "section": "5.7 파이프라인 적용",
    "text": "5.7 파이프라인 적용\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression())\n])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#스케일링-방법-선택-가이드",
    "href": "part1/05. 스케일링.html#스케일링-방법-선택-가이드",
    "title": "5  스케일링",
    "section": "5.8 스케일링 방법 선택 가이드",
    "text": "5.8 스케일링 방법 선택 가이드\n\n\n\n방법\n특징\n추천 상황\n\n\n\n\nMin-Max\n0~1\n신경망, 시각화\n\n\nStandard\n평균 0\n대부분의 모델\n\n\nRobust\n이상치 강함\n실무 기본\n\n\nMaxAbs\n부호 유지\n희소 데이터\n\n\n그룹별\n분포 반영\n도메인 지식 있을 때",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html",
    "href": "part1/06. 데이터 분포 변환.html",
    "title": "6  데이터 분포 변환",
    "section": "",
    "text": "6.1 분포 변환 이유\n예제 데이터셋 적재\ndf_num.skew()\n\nbill_length_mm       0.053118\nbill_depth_mm       -0.143465\nflipper_length_mm    0.345682\nbody_mass_g          0.470329\ndtype: float64\n대부분 데이터는 양의 왜도(skewness) 형태를 보이며 평균·분산 기반 모델(선형 회귀, PCA 등)에 불리\n분포를 대칭에 가깝게 변환함으로써 극단값 영향 완화, 모델 가정 충족이 목표",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "href": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.2 로그 변환 (Log Transform)",
    "text": "6.2 로그 변환 (Log Transform)\n오른쪽 꼬리가 긴 분포(양의 왜도, Positive)에 효과적\n\n6.2.1 수식\n\\[\nx' = \\log(x)\n\\]\n0 이하 값 변환 불가로 보정 필요\n\n\n6.2.2 코드\n\ndf_log = df_num.copy()\n\nfor col in num_cols:\n    df_log[col] = np.log(df_log[col])\n\nprint(df_log)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          3.666122       2.928524           5.198497     8.229511\n1          3.676301       2.856470           5.225747     8.242756\n2          3.696351       2.890372           5.273000     8.086410\n3               NaN            NaN                NaN          NaN\n4          3.602777       2.960105           5.262690     8.146130\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        3.845883       2.660260           5.370638     8.486734\n341        3.919991       2.753661           5.402677     8.656955\n342        3.811097       2.694627           5.356586     8.556414\n343        3.910021       2.778819           5.361292     8.594154\n\n[344 rows x 4 columns]\n\n\n\n\n6.2.3 0 포함 변수 안전 처리\n\ndf_log_safe = np.log1p(df_num)\n\nprint(df_log_safe)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          3.691376       2.980619           5.204007     8.229778\n1          3.701302       2.912351           5.231109     8.243019\n2          3.720862       2.944439           5.278115     8.086718\n3               NaN            NaN                NaN          NaN\n4          3.629660       3.010621           5.267858     8.146419\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        3.867026       2.727853           5.375278     8.486940\n341        3.939638       2.815409           5.407172     8.657129\n342        3.832980       2.760010           5.361292     8.556606\n343        3.929863       2.839078           5.365976     8.594339\n\n[344 rows x 4 columns]\n\n\n\n\n6.2.4 왜도 변화 확인\n\ndf_log_safe.skew()\n\nbill_length_mm      -0.143463\nbill_depth_mm       -0.315791\nflipper_length_mm    0.251965\nbody_mass_g          0.170721\ndtype: float64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "href": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.3 제곱근 변환 (Square Root Transform)",
    "text": "6.3 제곱근 변환 (Square Root Transform)\n로그보다 완만한 변환\n\n6.3.1 수식\n\\[\nx' = \\sqrt{x}\n\\]\n\n\n6.3.2 코드\n\ndf_sqrt = np.sqrt(df_num)\n\nprint(df_sqrt)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0          6.252999       4.324350          13.453624    61.237244\n1          6.284903       4.171331          13.638182    61.644140\n2          6.348228       4.242641          13.964240    57.008771\n3               NaN            NaN                NaN          NaN\n4          6.058052       4.393177          13.892444    58.736701\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        6.841053       3.781534          14.662878    69.641941\n341        7.099296       3.962323          14.899664    75.828754\n342        6.723095       3.847077          14.560220    72.111026\n343        7.063993       4.012481          14.594520    73.484692\n\n[344 rows x 4 columns]\n\n\n\n\n6.3.3 제곱근 변환과 비교\n제곱근이 적합한 경우\n\n왜도가 심하지 않음\n데이터 범위가 크지 않음\n분포를 살짝만 정리하고 싶을 때\n카운트 데이터 (포아송 계열)\n\n로그가 적합한 경우\n\n왜도가 매우 큼\n극단값(outlier)이 많음\n선형 회귀, PCA, 거리 기반 모델\n“비율 변화”가 중요한 문제",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#boxcox-변환",
    "href": "part1/06. 데이터 분포 변환.html#boxcox-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.4 Box–Cox 변환",
    "text": "6.4 Box–Cox 변환\n로그의 일반화 버전으로 λ(람다)를 자동 추정 (단, 양수 전용)\n\n6.4.1 수식\n\\[\nx' =\n\\begin{cases}\n\\frac{x^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\n\\log(x), & \\lambda = 0\n\\end{cases}\n\\]\n\n\n6.4.2 코드\n\nfrom scipy.stats import boxcox\n\n\ndf_boxcox = df_num.copy()\nlambdas = {}\n\nfor col in num_cols:\n    data = df_boxcox[col].dropna()\n    transformed, lam = boxcox(data)\n    df_boxcox.loc[data.index, col] = transformed\n    lambdas[col] = lam\n\n\nlambdas\n\n{'bill_length_mm': np.float64(0.6201739671602454),\n 'bill_depth_mm': np.float64(1.4747606585324315),\n 'flipper_length_mm': np.float64(-2.0632096556190813),\n 'body_mass_g': np.float64(-0.4656520321320674)}",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#yeojohnson-변환",
    "href": "part1/06. 데이터 분포 변환.html#yeojohnson-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.5 Yeo–Johnson 변환",
    "text": "6.5 Yeo–Johnson 변환\nBox–Cox의 확장 방식으로 0, 음수에 대해서도 변환 가능\n\n6.5.1 sklearn 사용\n\nfrom sklearn.preprocessing import PowerTransformer\n\n\npt = PowerTransformer(method=\"yeo-johnson\")\n\ndf_yeojohnson = pd.DataFrame(\n    pt.fit_transform(df_num),\n    columns=num_cols\n)\n\n0, 음수 허용를 허용하며 결측치 제외 후 자동 처리",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "href": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.6 분위수 변환 (Quantile Transform)",
    "text": "6.6 분위수 변환 (Quantile Transform)\n분포 모양 자체를 바꿈\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n\nqt = QuantileTransformer(\n    output_distribution=\"normal\",\n    random_state=42\n)\n\ndf_quantile = pd.DataFrame(\n    qt.fit_transform(df_num),\n    columns=num_cols\n)\n\nprint(df_quantile)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0         -0.704794       0.690633          -1.663512    -0.366106\n1         -0.649469       0.033085          -1.127632    -0.269985\n2         -0.506872       0.288983          -0.176300    -1.347428\n3               NaN            NaN                NaN          NaN\n4         -1.237926       1.107167          -0.335021    -0.890537\n..              ...            ...                ...          ...\n339             NaN            NaN                NaN          NaN\n340        0.449514      -1.286551           0.796647     0.737791\n341        1.120759      -0.627072           1.413753     1.848596\n342        0.091477      -0.963539           0.596232     1.042077\n343        0.965467      -0.465743           0.653988     1.245827\n\n[344 rows x 4 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "href": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "title": "6  데이터 분포 변환",
    "section": "6.7 변환 전·후 왜도 비교",
    "text": "6.7 변환 전·후 왜도 비교\n\nskew_compare = pd.DataFrame({\n    \"original\": df_num.skew(),\n    \"log\": df_log_safe.skew(),\n    \"yeo-johnson\": df_yeojohnson.skew()\n})\n\nskew_compare\n\n\n\n\n\n\n\n\noriginal\nlog\nyeo-johnson\n\n\n\n\nbill_length_mm\n0.053118\n-0.143463\n-0.023577\n\n\nbill_depth_mm\n-0.143465\n-0.315791\n-0.052609\n\n\nflipper_length_mm\n0.345682\n0.251965\n0.050271\n\n\nbody_mass_g\n0.470329\n0.170721\n0.026153",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#선택-가이드-교안-요약",
    "href": "part1/06. 데이터 분포 변환.html#선택-가이드-교안-요약",
    "title": "6  데이터 분포 변환",
    "section": "6.8 선택 가이드 (교안 요약)",
    "text": "6.8 선택 가이드 (교안 요약)\n\n\n\n방법\n특징\n추천\n\n\n\n\nLog\n단순\n기초\n\n\nSqrt\n완만\n보조\n\n\nBox–Cox\n최적 λ\n양수 데이터\n\n\nYeo–Johnson\n범용\n실무 기본\n\n\nQuantile\n분포 강제\n비정상 분포",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#스케일링과의-관계",
    "href": "part1/06. 데이터 분포 변환.html#스케일링과의-관계",
    "title": "6  데이터 분포 변환",
    "section": "6.9 스케일링과의 관계",
    "text": "6.9 스케일링과의 관계\n\n분포 변환 → 모양 변경\n스케일링 → 크기 조정\n일반 순서 분포 변환 → 스케일링",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html",
    "href": "part1/07. 범주형 데이터 인코딩.html",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "7.1 Label Encoding\n예제 데이터셋 적재\n범주를 정수 값으로 매핑하는 방식으로 순서가 없는데 숫자로 바뀌므로 주의 필요",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "7.1.1 예제 코드\n\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\ndf_label = df.copy()\ndf_label[\"species_enc\"] = le.fit_transform(df_label[\"species\"])\n\nprint(df_label)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  species_enc  \n0         3750.0    Male            0  \n1         3800.0  Female            0  \n2         3250.0  Female            0  \n3            NaN     NaN            0  \n4         3450.0  Female            0  \n..           ...     ...          ...  \n339          NaN     NaN            2  \n340       4850.0  Female            2  \n341       5750.0    Male            2  \n342       5200.0  Female            2  \n343       5400.0    Male            2  \n\n[344 rows x 8 columns]\n\n\n\n\n7.1.2 사용 시점\n트리 기반 모델 (Decision Tree, RandomForest, XGBoost) 사용 시 순서 의미가 없어도 되는 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.2 One-Hot Encoding",
    "text": "7.2 One-Hot Encoding\n각 범주를 이진 컬럼으로 분리, 거리 기반, 선형 모델에서 가장 안전\n\n7.2.1 pandas 방식\n\ndf_ohe = pd.get_dummies(\n    df,\n    columns=[\"species\", \"island\"],\n    drop_first=False\n)\n\nprint(df_ohe)\n\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  \\\n0              39.1           18.7              181.0       3750.0    Male   \n1              39.5           17.4              186.0       3800.0  Female   \n2              40.3           18.0              195.0       3250.0  Female   \n3               NaN            NaN                NaN          NaN     NaN   \n4              36.7           19.3              193.0       3450.0  Female   \n..              ...            ...                ...          ...     ...   \n339             NaN            NaN                NaN          NaN     NaN   \n340            46.8           14.3              215.0       4850.0  Female   \n341            50.4           15.7              222.0       5750.0    Male   \n342            45.2           14.8              212.0       5200.0  Female   \n343            49.9           16.1              213.0       5400.0    Male   \n\n     species_Adelie  species_Chinstrap  species_Gentoo  island_Biscoe  \\\n0              True              False           False          False   \n1              True              False           False          False   \n2              True              False           False          False   \n3              True              False           False          False   \n4              True              False           False          False   \n..              ...                ...             ...            ...   \n339           False              False            True           True   \n340           False              False            True           True   \n341           False              False            True           True   \n342           False              False            True           True   \n343           False              False            True           True   \n\n     island_Dream  island_Torgersen  \n0           False              True  \n1           False              True  \n2           False              True  \n3           False              True  \n4           False              True  \n..            ...               ...  \n339         False             False  \n340         False             False  \n341         False             False  \n342         False             False  \n343         False             False  \n\n[344 rows x 11 columns]\n\n\ndrop_first 옵션으로 첫번째 더미 변수 제거, 다중공선성 방지 (회귀 모델에서 중요)\n\ndf_ohe = pd.get_dummies(\n    df,\n    columns=[\"species\"],\n    drop_first=True\n)\n\nprint(df_ohe)\n\n        island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g  \\\n0    Torgersen            39.1           18.7              181.0       3750.0   \n1    Torgersen            39.5           17.4              186.0       3800.0   \n2    Torgersen            40.3           18.0              195.0       3250.0   \n3    Torgersen             NaN            NaN                NaN          NaN   \n4    Torgersen            36.7           19.3              193.0       3450.0   \n..         ...             ...            ...                ...          ...   \n339     Biscoe             NaN            NaN                NaN          NaN   \n340     Biscoe            46.8           14.3              215.0       4850.0   \n341     Biscoe            50.4           15.7              222.0       5750.0   \n342     Biscoe            45.2           14.8              212.0       5200.0   \n343     Biscoe            49.9           16.1              213.0       5400.0   \n\n        sex  species_Chinstrap  species_Gentoo  \n0      Male              False           False  \n1    Female              False           False  \n2    Female              False           False  \n3       NaN              False           False  \n4    Female              False           False  \n..      ...                ...             ...  \n339     NaN              False            True  \n340  Female              False            True  \n341    Male              False            True  \n342  Female              False            True  \n343    Male              False            True  \n\n[344 rows x 8 columns]\n\n\n선형 회귀, 로지스틱 회귀, KNN, SVM, PCA 등의 모델 사용 시 적용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.3 Ordinal Encoding",
    "text": "7.3 Ordinal Encoding\n순서가 있는 범주형 변수에 사용, 사용자가 순서를 직접 정의\n\n7.3.1 예제 코드\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\noe = OrdinalEncoder(\n    categories=[[\"small\", \"medium\", \"large\"]]\n)\n\ndf_ord = pd.DataFrame(\n    oe.fit_transform(\n        pd.DataFrame({\"size\": [\"small\", \"large\", \"medium\"]})\n    ),\n    columns=[\"size_enc\"]\n)\n\nprint(df_ord)\n\n   size_enc\n0       0.0\n1       2.0\n2       1.0\n\n\n명확한 순서 존재하는 등급, 크기, 단계 표현 등에 적용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#targetmean-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#targetmean-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.4 Target(Mean) Encoding",
    "text": "7.4 Target(Mean) Encoding\n범주를 타깃 변수의 평균값으로 치환, 정보량은 크지만 데이터 누수 위험 존재\n\n7.4.1 예제 코드\n\ndf_te = df.copy()\n\ntarget = \"body_mass_g\"\ncol = \"species\"\n\nmean_map = df_te.groupby(col)[target].mean()\ndf_te[\"species_enc\"] = df_te[col].map(mean_map)\n\nprint(df_te)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  species_enc  \n0         3750.0    Male  3700.662252  \n1         3800.0  Female  3700.662252  \n2         3250.0  Female  3700.662252  \n3            NaN     NaN  3700.662252  \n4         3450.0  Female  3700.662252  \n..           ...     ...          ...  \n339          NaN     NaN  5076.016260  \n340       4850.0  Female  5076.016260  \n341       5750.0    Male  5076.016260  \n342       5200.0  Female  5076.016260  \n343       5400.0    Male  5076.016260  \n\n[344 rows x 8 columns]\n\n\n반드시 train 데이터 기준으로만 계산, CV 기반 smoothing 권장",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#빈도frequency-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#빈도frequency-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.5 빈도(Frequency) 인코딩",
    "text": "7.5 빈도(Frequency) 인코딩\n범주의 출현 빈도로 인코딩, 희귀 범주 처리에 유용\n\n7.5.1 예제 코드\n\nfreq_map = df[\"island\"].value_counts(normalize=True)\ndf_freq = df.copy()\ndf_freq[\"island_enc\"] = df_freq[\"island\"].map(freq_map)\n\nprint(df_freq)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex  island_enc  \n0         3750.0    Male    0.151163  \n1         3800.0  Female    0.151163  \n2         3250.0  Female    0.151163  \n3            NaN     NaN    0.151163  \n4         3450.0  Female    0.151163  \n..           ...     ...         ...  \n339          NaN     NaN    0.488372  \n340       4850.0  Female    0.488372  \n341       5750.0    Male    0.488372  \n342       5200.0  Female    0.488372  \n343       5400.0    Male    0.488372  \n\n[344 rows x 8 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.6 여러 컬럼을 한 번에 인코딩",
    "text": "7.6 여러 컬럼을 한 번에 인코딩\n\n7.6.1 One-Hot (pandas)\n\ncat_cols = [\"species\", \"island\", \"sex\"]\n\ndf_ohe = pd.get_dummies(df, columns=cat_cols)\n\n\n\n7.6.2 Label Encoding (반복 처리)\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndf_label = df.copy()\n\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_label[col] = le.fit_transform(df_label[col].astype(str))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#인코딩-방식-요약",
    "href": "part1/07. 범주형 데이터 인코딩.html#인코딩-방식-요약",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.7 인코딩 방식 요약",
    "text": "7.7 인코딩 방식 요약\n\n\n\n인코딩\n순서 필요\n컬럼 수 증가\n추천 모델\n\n\n\n\nLabel\nX\nX\n트리 모델\n\n\nOne-Hot\nX\nO\n선형, 거리 기반\n\n\nOrdinal\nO\nX\n순서형 데이터\n\n\nTarget\nX\nX\n고급 모델\n\n\nFrequency\nX\nX\n희귀 범주 많을 때",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html",
    "href": "part1/08. 연속형 데이터 범주화.html",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "8.1 연속형 데이터 범주화 (Discretization / Binning)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#연속형-데이터-범주화-discretization-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#연속형-데이터-범주화-discretization-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "8.1.1 개념\n\n연속형 변수를 구간(범주)으로 나누어 범주형 변수로 변환\n정보는 줄지만 해석력 증가, 노이즈 감소, 비선형 관계 단순화 효과",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.2 등간격 구간화 (Equal-width binning)",
    "text": "8.2 등간격 구간화 (Equal-width binning)\n\n8.2.1 개념\n전체 값 범위를 동일한 간격으로 분할\n\\[\n\\text{bin width} = \\frac{\\max(x) - \\min(x)}{k}\n\\]\n\n\n8.2.2 예제\n\nimport pandas as pd\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\")\n\ndf[\"bill_length_bin\"] = pd.cut(df[\"bill_length_mm\"], bins=4)\n\nprint(df[\"bill_length_bin\"])\n\n0       (38.975, 45.85]\n1       (38.975, 45.85]\n2       (38.975, 45.85]\n3                   NaN\n4      (32.072, 38.975]\n             ...       \n339                 NaN\n340     (45.85, 52.725]\n341     (45.85, 52.725]\n342     (38.975, 45.85]\n343     (45.85, 52.725]\nName: bill_length_bin, Length: 344, dtype: category\nCategories (4, interval[float64, right]): [(32.072, 38.975] &lt; (38.975, 45.85] &lt; (45.85, 52.725] &lt; (52.725, 59.6]]\n\n\n\n\n8.2.3 범주 이름 지정\n\ndf[\"bill_length_bin\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=4,\n    labels=[\"짧음\", \"보통\", \"김\", \"매우 김\"]\n)\n\nprint(df[\"bill_length_bin\"])\n\n0       보통\n1       보통\n2       보통\n3      NaN\n4       짧음\n      ... \n339    NaN\n340      김\n341      김\n342     보통\n343      김\nName: bill_length_bin, Length: 344, dtype: category\nCategories (4, str): ['짧음' &lt; '보통' &lt; '김' &lt; '매우 김']\n\n\n구현 간단하지만 이상치에 민감, 데이터 밀도 불균형 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-quantile-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-quantile-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.3 등빈도 구간화 (Equal-frequency, Quantile binning)",
    "text": "8.3 등빈도 구간화 (Equal-frequency, Quantile binning)\n\n8.3.1 개념\n각 구간에 데이터 개수가 동일하도록 분할\n\n\n8.3.2 예제\n\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4\n)\n\nprint(df[\"bill_length_qbin\"])\n\n0      (32.099000000000004, 39.225]\n1                   (39.225, 44.45]\n2                   (39.225, 44.45]\n3                               NaN\n4      (32.099000000000004, 39.225]\n                   ...             \n339                             NaN\n340                   (44.45, 48.5]\n341                    (48.5, 59.6]\n342                   (44.45, 48.5]\n343                    (48.5, 59.6]\nName: bill_length_qbin, Length: 344, dtype: category\nCategories (4, interval[float64, right]): [(32.099000000000004, 39.225] &lt; (39.225, 44.45] &lt; (44.45, 48.5] &lt; (48.5, 59.6]]\n\n\n\n\n8.3.3 라벨 지정\n\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4,\n    labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n)\n\ndf[\"bill_length_qbin\"]\n\n0       Q1\n1       Q2\n2       Q2\n3      NaN\n4       Q1\n      ... \n339    NaN\n340     Q3\n341     Q4\n342     Q3\n343     Q4\nName: bill_length_qbin, Length: 344, dtype: category\nCategories (4, str): ['Q1' &lt; 'Q2' &lt; 'Q3' &lt; 'Q4']\n\n\n데이터 불균형 완화, 분포 반영, 동일값 많으면 에러 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.4 사용자 정의 구간화",
    "text": "8.4 사용자 정의 구간화\n\n8.4.1 개념\n도메인 지식 기반으로 직접 구간 설정\n\n\n8.4.2 예제\n\nbins = [30, 40, 45, 50, 60]\nlabels = [\"매우 짧음\", \"짧음\", \"보통\", \"김\"]\n\ndf[\"bill_length_custom\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=bins,\n    labels=labels\n)\n\nprint(df[\"bill_length_custom\"])\n\n0      매우 짧음\n1      매우 짧음\n2         짧음\n3        NaN\n4      매우 짧음\n       ...  \n339      NaN\n340       보통\n341        김\n342       보통\n343       보통\nName: bill_length_custom, Length: 344, dtype: category\nCategories (4, str): ['매우 짧음' &lt; '짧음' &lt; '보통' &lt; '김']\n\n\n해석력 최고, 주관 개입, 기준 설명 필요",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.5 k-means 기반 구간화",
    "text": "8.5 k-means 기반 구간화\n\n8.5.1 개념\n값의 분포를 고려해 군집 중심 기준으로 구간화, 비선형 구조 반영 가능\n\n\n8.5.2 예제\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\nx = df[[\"bill_length_mm\"]].dropna()\n\nkmeans = KMeans(n_clusters=3, random_state=42)\nx[\"cluster\"] = kmeans.fit_predict(x)\n\ndf.loc[x.index, \"bill_length_kbin\"] = x[\"cluster\"]\n\nprint(df[\"bill_length_kbin\"])\n\n0      2.0\n1      2.0\n2      2.0\n3      NaN\n4      2.0\n      ... \n339    NaN\n340    0.0\n341    1.0\n342    0.0\n343    1.0\nName: bill_length_kbin, Length: 344, dtype: float64\n\n\n\n\n8.5.3 특징\n\n데이터 기반\n해석은 상대적으로 어려움",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.6 Decision Tree 기반 구간화",
    "text": "8.6 Decision Tree 기반 구간화\n\n8.6.1 개념\n타깃 변수를 기준으로 정보이득 최대화, 지도학습 기반 binning\n\n\n8.6.2 예제 (체중 기준)\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nx = df[[\"bill_length_mm\"]].dropna()\ny = df.loc[x.index, \"body_mass_g\"]\n\ntree = DecisionTreeRegressor(\n    max_leaf_nodes=4\n)\ntree.fit(x, y)\n\ndf.loc[x.index, \"bill_length_treebin\"] = tree.apply(x)\n\n타깃 정보 반영, 데이터 누수 주의",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩까지-연결",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩까지-연결",
    "title": "8  연속형 데이터 범주화",
    "section": "8.7 범주화 후 인코딩까지 연결",
    "text": "8.7 범주화 후 인코딩까지 연결\n\ndf_bin = pd.get_dummies(\n    df,\n    columns=[\"bill_length_bin\"],\n    drop_first=True\n)\n\nprint(df_bin)\n\n    species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0    Adelie  Torgersen            39.1           18.7              181.0   \n1    Adelie  Torgersen            39.5           17.4              186.0   \n2    Adelie  Torgersen            40.3           18.0              195.0   \n3    Adelie  Torgersen             NaN            NaN                NaN   \n4    Adelie  Torgersen            36.7           19.3              193.0   \n..      ...        ...             ...            ...                ...   \n339  Gentoo     Biscoe             NaN            NaN                NaN   \n340  Gentoo     Biscoe            46.8           14.3              215.0   \n341  Gentoo     Biscoe            50.4           15.7              222.0   \n342  Gentoo     Biscoe            45.2           14.8              212.0   \n343  Gentoo     Biscoe            49.9           16.1              213.0   \n\n     body_mass_g     sex bill_length_qbin bill_length_custom  \\\n0         3750.0    Male               Q1              매우 짧음   \n1         3800.0  Female               Q2              매우 짧음   \n2         3250.0  Female               Q2                 짧음   \n3            NaN     NaN              NaN                NaN   \n4         3450.0  Female               Q1              매우 짧음   \n..           ...     ...              ...                ...   \n339          NaN     NaN              NaN                NaN   \n340       4850.0  Female               Q3                 보통   \n341       5750.0    Male               Q4                  김   \n342       5200.0  Female               Q3                 보통   \n343       5400.0    Male               Q4                 보통   \n\n     bill_length_kbin  bill_length_treebin  bill_length_bin_보통  \\\n0                 2.0                  4.0                True   \n1                 2.0                  4.0                True   \n2                 2.0                  4.0                True   \n3                 NaN                  NaN               False   \n4                 2.0                  3.0               False   \n..                ...                  ...                 ...   \n339               NaN                  NaN               False   \n340               0.0                  5.0               False   \n341               1.0                  6.0               False   \n342               0.0                  5.0                True   \n343               1.0                  6.0               False   \n\n     bill_length_bin_김  bill_length_bin_매우 김  \n0                False                 False  \n1                False                 False  \n2                False                 False  \n3                False                 False  \n4                False                 False  \n..                 ...                   ...  \n339              False                 False  \n340               True                 False  \n341               True                 False  \n342              False                 False  \n343               True                 False  \n\n[344 rows x 14 columns]",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-상황",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-상황",
    "title": "8  연속형 데이터 범주화",
    "section": "8.8 범주화 상황",
    "text": "8.8 범주화 상황\n\n\n\n상황\n효과\n\n\n\n\n모델 해석이 중요할 때\nO\n\n\n비선형 관계 단순화\nO\n\n\n데이터 노이즈 큼\nO\n\n\n데이터 충분히 많음\nX\n\n\n딥러닝\n거의 사용 안 함",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#스케일링-vs-범주화",
    "href": "part1/08. 연속형 데이터 범주화.html#스케일링-vs-범주화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.9 스케일링 vs 범주화",
    "text": "8.9 스케일링 vs 범주화\n\n\n\n항목\n스케일링\n범주화\n\n\n\n\n정보 손실\n없음\n있음\n\n\n해석력\n낮음\n높음\n\n\n모델 안정성\n보통\n높음\n\n\n주 용도\n거리 계산\n규칙 기반\n\n\n\n범주화는 정보 손실을 감수하고 해석력을 얻는 선택, EDA 결과 보고 결정하는 단계",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html",
    "href": "part1/09. 불균형 데이터 처리.html",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "9.1 불균형 데이터의 문제점\n타깃 클래스 간 샘플 수 차이가 큰 경우, 다수 클래스에 치우친 모델이 생성됨 → 정확도는 높아 보이지만 실제 성능은 나쁨",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "9.1.1 예시\n\n전체 정확도 95%\n소수 클래스 재현율 10%\n\n“잘 맞춘 것처럼 보이지만 중요한 건 다 틀림”",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "title": "9  불균형 데이터 처리",
    "section": "9.2 불균형 여부 진단",
    "text": "9.2 불균형 여부 진단\n\n9.2.1 클래스 분포 확인\n\nimport seaborn as sns\n\ndf = sns.load_dataset(\"penguins\").dropna(subset=[\"species\"])\n\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n\n\n9.2.2 비율로 확인\n\ndf[\"species\"].value_counts(normalize=True)\n\nspecies\nAdelie       0.441860\nGentoo       0.360465\nChinstrap    0.197674\nName: proportion, dtype: float64\n\n\n\n\n9.2.3 시각화\n\nimport matplotlib.pyplot as plt\n\ndf[\"species\"].value_counts().plot(kind=\"bar\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "href": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "title": "9  불균형 데이터 처리",
    "section": "9.3 평가 지표 변경",
    "text": "9.3 평가 지표 변경\n불균형 데이터에서는 Accuracy 사용 금물\n\n9.3.1 대안 지표\n\nPrecision\nRecall\nF1-score\nROC-AUC\nPR-AUC\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_true, y_pred))",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.4 데이터 수준 처리 방법",
    "text": "9.4 데이터 수준 처리 방법\n\n9.4.1 언더샘플링 (Under-sampling)\n\n9.4.1.1 개념\n다수 클래스 일부 제거\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf_dropna = df[['species']+num_cols].dropna()\nX = df_dropna.drop(\"species\", axis=1)\ny = df_dropna[\"species\"]\n\nrus = RandomUnderSampler(random_state=42)\nX_under, y_under = rus.fit_resample(X, y)\nprint(\"Before\\n\", y.value_counts())\nprint(\"After\\n\", y_under.value_counts())\n\nBefore\n species\nAdelie       151\nGentoo       123\nChinstrap     68\nName: count, dtype: int64\nAfter\n species\nAdelie       68\nChinstrap    68\nGentoo       68\nName: count, dtype: int64\n\n\n빠르다는 장점이 있으나 정보가 손신된다는 단점 존재\n\n\n\n9.4.2 오버샘플링 (Over-sampling)\n\n9.4.2.1 단순 복제\n\nfrom imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\nX_over, y_over = ros.fit_resample(X, y)\n\nprint(y_over.value_counts())\n\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n\n\n\n\n9.4.2.2 문제점\n과적합 위험\n\n\n\n9.4.3 SMOTE (Synthetic Minority Over-sampling)\n\n9.4.3.1 개념\n소수 클래스 사이에 가상 샘플 생성\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X, y)\n\nprint(y_smote.value_counts())\n\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n\n\n\n\n\n9.4.4 SMOTE 변형\n\n\n\n기법\n특징\n\n\n\n\nBorderline-SMOTE\n경계 근처만 증강\n\n\nSMOTE-NC\n수치 + 범주 혼합\n\n\nADASYN\n학습 어려운 영역 집중",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.5 모델 수준 처리 방법",
    "text": "9.5 모델 수준 처리 방법\n\n9.5.1 클래스 가중치 조정\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(\n    class_weight=\"balanced\"\n)\n\n\n\n9.5.2 수동 가중치 설정\n\nweights = {\n    \"Adelie\": 1,\n    \"Gentoo\": 2,\n    \"Chinstrap\": 3\n}\n\nmodel = LogisticRegression(class_weight=weights)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "href": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "title": "9  불균형 데이터 처리",
    "section": "9.6 앙상블 + 불균형 대응",
    "text": "9.6 앙상블 + 불균형 대응\n\n9.6.1 대표 기법\n\nBalanced Random Forest\nEasyEnsemble\n\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\nmodel = BalancedRandomForestClassifier(\n    n_estimators=100,\n    random_state=42\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "href": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "title": "9  불균형 데이터 처리",
    "section": "9.7 Pipeline으로 안전하게 적용",
    "text": "9.7 Pipeline으로 안전하게 적용\n샘플링은 반드시 학습 데이터에만!\n\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\n\npipeline = Pipeline([\n    (\"smote\", SMOTE(random_state=42)),\n    (\"model\", LogisticRegression())\n])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#권장-상황",
    "href": "part1/09. 불균형 데이터 처리.html#권장-상황",
    "title": "9  불균형 데이터 처리",
    "section": "9.8 권장 상황",
    "text": "9.8 권장 상황\n\n\n\n상황\n권장\n\n\n\n\n데이터 작음\nSMOTE\n\n\n데이터 큼\n클래스 가중치\n\n\n노이즈 많음\n언더샘플링\n\n\n범주형 포함\nSMOTE-NC\n\n\n트리 모델\n가중치 or Balanced RF",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  }
]