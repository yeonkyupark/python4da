[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "파이썬 데이터 분석",
    "section": "",
    "text": "들어가기\n기계학습으로 데이터를 분석한다. 분석 도구는 파이썬으로 데이터 전처리, 통계 분석, 기계학습 모델링 및 평가까지 코드 중심으로 구성된다. 실무에 필요한 내용을 정리하여 이론적, 학문적으로 미흡한 부분이 있다. 부족한 부분은 향후 성능 개선과 향상된 알고리즘 등으로 수정키로 한다.\n파이썬 버전은 3.12 기반으로 numpy, pandas, sklearn, scipy, stats와 같은 기본적인 라이브러리를 사용한다.\n전체 목차는 다음과 같다.",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-데이터-전처리-분포-이해",
    "href": "index.html#부.-데이터-전처리-분포-이해",
    "title": "파이썬 데이터 분석",
    "section": "1부. 데이터 전처리 & 분포 이해",
    "text": "1부. 데이터 전처리 & 분포 이해\n\n1.1 데이터 로드 및 구조 점검\n\n데이터 로드 (CSV, Excel, SQL)\n데이터 구조 및 타입 확인\n\n\n\n1.2 탐색적 데이터 분석(EDA)\n\n기술통계량 요약\n분포 시각화 (히스토그램, KDE, 박스플롯)\n상관관계 탐색\n\n\n\n1.3 데이터 분포 이해\n\n연속형·이산형 데이터 분포\n왜도와 첨도\n분포 해석을 통한 전처리 전략\n\n\n\n1.4 결측치 처리\n\n결측치 탐지 및 요약\n단순 대치 기법\n고급 대치 기법 (KNN, Iterative)\n\n\n\n1.5 이상치 탐지\n\n정규분포 기반 이상치\nIQR 기반 이상치\n밀도 기반 이상치 (LOF, DBSCAN)\n트리 기반 이상치 (IsolationForest)\n\n\n\n1.6 스케일링\n\n정규화 (Min-Max)\n표준화 (Standard, Robust, MaxAbs)\n\n\n\n1.7 데이터 분포 변환\n\n로그 변환\nBox-Cox 변환\nYeo-Johnson 변환\n분위수 변환\n변환 전·후 분포 비교\n\n\n\n1.8 범주형 데이터 처리\n\n명목형 인코딩\n순서형 인코딩\n\n\n\n1.9 연속형 데이터 범주화\n\n구간 분할 (cut, qcut, KBins)\n\n\n\n1.10 불균형 데이터 처리\n\n오버샘플링\n언더샘플링\n\n\n\n1.11 피처 엔지니어링\n\n다항 특성 생성\n집계 및 롤링 특성",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "href": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "title": "파이썬 데이터 분석",
    "section": "2부. 통계 기반 데이터 분석 & 가설검정",
    "text": "2부. 통계 기반 데이터 분석 & 가설검정\n\n2.1 확률분포와 표본\n\n연속형 확률분포\n이산형 확률분포\n표본 분포 개념\n\n\n\n2.2 정규성 검정\n\nShapiro-Wilk 검정\nKolmogorov-Smirnov 검정\nAnderson-Darling 검정\nQ-Q plot 해석\n\n\n\n2.3 등분산성 검정\n\nLevene 검정\nBartlett 검정\nFligner-Killeen 검정\n\n\n\n2.4 적합성 검정 & 독립성 검정\n\n카이제곱 적합성 검정\n분포 적합성 검정\n카이제곱 독립성 검정\nF 검정 (분산 비교)\n\n\n\n2.5 평균 비교 검정\n\n단일 표본 t-검정\n독립 표본 t-검정\n대응 표본 t-검정\n\n\n\n2.6 분산분석\n\n일원 분산 분석 (One-way ANOVA)\n이원 분산 분석 (Two-way ANOVA)\n사후 검정\n\n\n\n2.7 비모수 검정\n\nMann-Whitney U 검정\nWilcoxon 순위합 검정\nKruskal-Wallis 검정\nFriedman 검정\n\n\n\n2.8 상관 분석\n\nPearson 상관 분석\nSpearman 순위 상관",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-머신러닝-모델링-평가",
    "href": "index.html#부.-머신러닝-모델링-평가",
    "title": "파이썬 데이터 분석",
    "section": "3부. 머신러닝 모델링 & 평가",
    "text": "3부. 머신러닝 모델링 & 평가\n\n3.1 데이터 분할 및 검증\n\n학습·검증·테스트 분할\n교차 검증 기법\n\n\n\n3.2 특성 선택\n\n필터 방법\n래퍼 방법\n임베디드 방법\n\n\n\n3.3 차원 축소\n\n선형 차원 축소 (PCA)\n비선형 차원 축소 (t-SNE, UMAP)\n\n\n\n3.4 회귀 모델\n\n선형 회귀\n정규화 회귀 (Ridge, Lasso, ElasticNet)\n\n\n\n3.5 분류 모델\n\n선형 분류 모델\n거리 기반 모델\n트리 및 앙상블 모델\n\n\n\n3.6 군집 분석\n\n분할 기반 군집\n밀도 기반 군집\n혼합 모델 군집\n\n\n\n3.7 서포트 벡터 머신\n\nSVM 분류\n\n\n\n3.8 모델 성능 평가\n\n분류 성능 평가\n회귀 성능 평가\n\n\n\n3.9 파이프라인 & 자동화\n\n파이프라인 구성\n하이퍼파라미터 최적화\n\n\n\n3.10 모델 해석\n\n특성 중요도\nSHAP 기반 모델 해석",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html",
    "href": "part1/01. 데이터 로드 및 구조 점검.html",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1 데이터 로드\n데이터 분석의 첫 단계는 데이터 수집이다. 데이터는 로컬 파일, 데이터베이스, 또는 외부 시스템에서 확보할 수 있으며, 형태는 상황에 따라 다양하다. 가장 일반적인 형태는 CSV와 Excel 파일이지만, API 호출이나 데이터베이스 쿼리를 통해 직접 취합하는 경우도 있다. 이 장에서는 다양한 소스로부터 데이터를 불러오는 방법과 불러온 데이터의 구조를 확인하는 방법을 학습한다.\n데이터를 분석하기 위해서는 먼저 파일이나 데이터베이스에 저장된 데이터를 메모리에 적재해야 한다. Python의 pandas 라이브러리는 다양한 형식의 데이터를 손쉽게 로드할 수 있는 함수를 제공한다. 여기서는 CSV, Excel, SQLite 데이터베이스 형태의 데이터를 불러오는 방법을 살펴본다.\n예제에서는 남극 팔머 제도에 서식하는 펭귄 데이터를 담은 palmerpenguins 데이터셋을 사용한다. 이 데이터셋은 펭귄의 종, 서식지, 신체 측정값 등의 정보를 포함하고 있다.\n예제: palmerpenguins 데이터셋 준비\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\n# 데이터 로드\ndf = load_penguins()\n\n# 데이터 미리보기\ndf.head()\n\n# 다양한 형식으로 저장\ndf.to_csv(\"penguins.csv\", index=False)        # CSV 파일로 저장\ndf.to_excel(\"penguins.xlsx\", index=False)     # Excel 파일로 저장\n\n# SQLite 데이터베이스에 저장\nimport sqlite3\nconn = sqlite3.connect(\"penguins.db\")\nresult = df.to_sql(\"penguins\", conn, if_exists=\"replace\", index=False)\nconn.close()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-로드",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1.1 CSV 파일 로드\nCSV(Comma-Separated Values)는 쉼표로 구분된 텍스트 파일 형식으로, 데이터 교환에 가장 널리 사용되는 형식이다. pandas의 read_csv() 함수를 사용하면 CSV 파일을 DataFrame 형태로 불러올 수 있다.\n예제: CSV 파일 불러오기\n\nimport pandas as pd\n\n# CSV 파일 로드\ndf_from_csv = pd.read_csv(\"penguins.csv\")\n\n# 상위 5개 행 확인\ndf_from_csv.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.2 Excel 파일 로드\nExcel 파일(.xlsx, .xls)은 비즈니스 환경에서 자주 사용되는 형식이다. pandas의 read_excel() 함수를 사용하면 Excel 파일을 불러올 수 있으며, 특정 시트를 지정하여 읽을 수도 있다.\n예제: Excel 파일 불러오기\n\nimport pandas as pd\n\n# Excel 파일 로드\ndf_from_excel = pd.read_excel(\"penguins.xlsx\")\n\n# 상위 5개 행 확인\ndf_from_excel.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.3 SQLite 데이터베이스 로드\n데이터베이스는 대량의 데이터를 체계적으로 관리하는 시스템이다. SQLite는 파일 기반의 경량 데이터베이스로, 별도의 서버 설치 없이 사용할 수 있다. pandas의 read_sql() 함수를 사용하면 SQL 쿼리 결과를 DataFrame으로 불러올 수 있다.\n예제: SQLite 데이터베이스에서 데이터 불러오기\n\nimport sqlite3\nimport pandas as pd\n\n# 데이터베이스 연결\nconn = sqlite3.connect(\"penguins.db\")\n\n# SQL 쿼리 실행 및 결과 로드\nquery = \"SELECT * FROM penguins\"\ndf_from_sql = pd.read_sql(query, conn)\n\n# 연결 종료\nconn.close()\n\n# 상위 5개 행 확인\ndf_from_sql.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "1.2 데이터 구조 및 타입 확인",
    "text": "1.2 데이터 구조 및 타입 확인\n데이터를 성공적으로 불러온 후에는 데이터의 전반적인 구조를 파악해야 한다. 데이터의 크기(행과 열의 개수), 각 컬럼의 이름과 데이터 타입, 결측값 유무 등을 확인하는 것이 데이터 분석의 기본이다. 이러한 정보는 후속 분석 방향을 결정하고 데이터 전처리 계획을 수립하는 데 필수적이다.\n\n1.2.1 데이터 크기 확인\nDataFrame의 크기는 shape 속성을 통해 확인할 수 있다. shape는 (행의 개수, 열의 개수) 형태의 튜플을 반환한다.\n예제: 데이터 크기 확인\n\nimport pandas as pd\n\ndf = pd.read_csv(\"penguins.csv\")\n\n# 데이터 크기 확인 (행, 열)\ndf.shape\n\n(344, 8)\n\n\n\n\n1.2.2 데이터 기본 정보 확인\ninfo() 메서드는 DataFrame의 전반적인 정보를 요약하여 보여준다. 각 컬럼의 데이터 타입, 결측값이 아닌 데이터의 개수, 메모리 사용량 등을 한눈에 파악할 수 있다.\n예제: 데이터 기본 정보 확인\n\n# 데이터 구조 및 타입 정보\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB\n\n\n위 코드를 실행하면 다음과 같은 정보를 확인할 수 있다.\n\n\n\n정보 항목\n설명\n\n\n\n\nRangeIndex\n전체 행의 개수\n\n\nData columns\n컬럼의 개수\n\n\nColumn\n각 컬럼의 이름\n\n\nNon-Null Count\n결측값이 아닌 데이터의 개수\n\n\nDtype\n각 컬럼의 데이터 타입 (int64, float64, object 등)\n\n\nmemory usage\nDataFrame이 사용하는 메모리 크기\n\n\n\n\n\n1.2.3 컬럼별 데이터 타입 확인\n각 컬럼의 데이터 타입만 간단히 확인하고 싶을 때는 dtypes 속성을 사용한다.\n예제: 컬럼별 데이터 타입 확인\n\n# 각 컬럼의 데이터 타입\ndf.dtypes\n\nspecies                  str\nisland                   str\nbill_length_mm       float64\nbill_depth_mm        float64\nflipper_length_mm    float64\nbody_mass_g          float64\nsex                      str\nyear                   int64\ndtype: object\n\n\n\n\n1.2.4 기술 통계량 확인\n수치형 데이터의 기본적인 통계량(평균, 표준편차, 최솟값, 최댓값 등)은 describe() 메서드로 확인할 수 있다.\n예제: 기술 통계량 확인\n\n# 수치형 컬럼의 기술 통계량\ndf.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nyear\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n344.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n2008.029070\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n0.818356\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n2007.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n2007.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n2008.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n2009.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n2009.000000",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/01. 데이터 로드 및 구조 점검.html#요약",
    "href": "part1/01. 데이터 로드 및 구조 점검.html#요약",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "1.3 요약",
    "text": "1.3 요약\n이 장에서는 데이터 분석의 첫 단계인 데이터 로드와 구조 점검 방법을 학습했다. 주요 내용은 다음과 같다.\n데이터 로드\n\nCSV 파일: pd.read_csv() 함수 사용\n\nExcel 파일: pd.read_excel() 함수 사용\n\nSQLite 데이터베이스: pd.read_sql() 함수 사용\n\n데이터 구조 확인\n\nshape: 데이터의 크기(행, 열) 확인\n\ninfo(): 컬럼명, 데이터 타입, 결측값 개수, 메모리 사용량 확인\n\ndtypes: 각 컬럼의 데이터 타입 확인\n\ndescribe(): 수치형 데이터의 기술 통계량 확인\n\n데이터를 정확하게 불러오고 그 구조를 이해하는 것은 모든 데이터 분석 작업의 기반이다. 다음 장에서는 불러온 데이터를 정제하고 변환하는 전처리 과정을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "",
    "text": "2.1 라이브러리 로드 및 데이터 불러오기\n탐색적 데이터 분석(Exploratory Data Analysis, EDA)은 데이터의 특성과 패턴을 파악하기 위해 시각화와 통계적 방법을 활용하는 과정이다. EDA를 통해 데이터의 분포, 변수 간 관계, 이상치, 결측치 등을 발견할 수 있으며, 이는 본격적인 분석이나 모델링 전략을 수립하는 데 중요한 기반이 된다. 이 장에서는 Python의 pandas, seaborn, matplotlib 라이브러리를 활용하여 체계적인 EDA를 수행하는 방법을 학습한다.\nEDA를 수행하기 위해서는 데이터 처리를 위한 pandas, 시각화를 위한 seaborn과 matplotlib 라이브러리가 필요하다. 여기서는 seaborn에 내장된 penguins 데이터셋을 사용한다.\n예제: 필수 라이브러리 로드 및 데이터 불러오기\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# seaborn 내장 penguins 데이터셋 로드\ndf = sns.load_dataset(\"penguins\")\n\n# 데이터 미리보기\ndf.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#데이터-구조-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.2 데이터 구조 확인",
    "text": "2.2 데이터 구조 확인\n데이터 분석의 첫 단계는 전체 데이터의 구조를 파악하는 것이다. 데이터의 크기, 각 컬럼의 데이터 타입, 결측치 존재 여부 등을 확인하여 데이터의 전반적인 특성을 이해할 수 있다.\n예제: 데이터 크기 확인\n\n# 행과 열의 개수 확인\ndf.shape\n\n(344, 7)\n\n\n예제: 데이터 상세 정보 확인\n\n# 컬럼별 데이터 타입 및 결측치 정보\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \ndtypes: float64(4), str(3)\nmemory usage: 18.9 KB\n\n\n위 결과에서 확인해야 할 주요 사항은 다음과 같다.\n\n\n\n확인 항목\n설명\n\n\n\n\n행(row) 개수\n전체 관측치의 수\n\n\n열(column) 개수\n변수의 수\n\n\n변수 타입\n수치형(int64, float64) 또는 범주형(object)\n\n\n결측치 여부\nNon-Null Count가 전체 행 수보다 적은 경우 결측치 존재",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#기초-통계량-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.3 기초 통계량 확인",
    "text": "2.3 기초 통계량 확인\n기술 통계량은 데이터의 중심 경향성과 분산 정도를 수치로 요약하여 제공한다. 수치형 변수와 범주형 변수는 서로 다른 방법으로 요약해야 한다.\n\n2.3.1 수치형 변수 요약\n수치형 변수는 평균, 표준편차, 최솟값, 최댓값, 사분위수 등의 통계량으로 요약할 수 있다.\n예제: 수치형 변수 기술 통계량\n\n# 수치형 컬럼의 기술 통계량\ndf.describe()\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\n\n\n\n\ncount\n342.000000\n342.000000\n342.000000\n342.000000\n\n\nmean\n43.921930\n17.151170\n200.915205\n4201.754386\n\n\nstd\n5.459584\n1.974793\n14.061714\n801.954536\n\n\nmin\n32.100000\n13.100000\n172.000000\n2700.000000\n\n\n25%\n39.225000\n15.600000\n190.000000\n3550.000000\n\n\n50%\n44.450000\n17.300000\n197.000000\n4050.000000\n\n\n75%\n48.500000\n18.700000\n213.000000\n4750.000000\n\n\nmax\n59.600000\n21.500000\n231.000000\n6300.000000\n\n\n\n\n\n\n\n\n\n2.3.2 범주형 변수 요약\n범주형 변수는 고유값의 개수, 최빈값, 빈도 등으로 요약할 수 있다.\n예제: 범주형 변수 기술 통계량\n\n# 범주형 컬럼의 기술 통계량\ndf.describe(include=\"object\")\n\n\n\n\n\n\n\n\nspecies\nisland\nsex\n\n\n\n\ncount\n344\n344\n333\n\n\nunique\n3\n3\n2\n\n\ntop\nAdelie\nBiscoe\nMale\n\n\nfreq\n152\n168\n168\n\n\n\n\n\n\n\n위 결과에서 확인할 수 있는 정보는 다음과 같다.\n\n\n\n통계량\n설명\n\n\n\n\ncount\n결측치를 제외한 데이터 개수\n\n\nunique\n고유한 값의 개수\n\n\ntop\n최빈값(가장 자주 나타나는 값)\n\n\nfreq\n최빈값의 빈도",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#결측치-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.4 결측치 탐색",
    "text": "2.4 결측치 탐색\n결측치는 데이터 분석 결과를 왜곡할 수 있으므로 반드시 확인하고 적절히 처리해야 한다. 결측치의 개수와 비율을 파악하여 처리 전략을 수립할 수 있다.\n예제: 컬럼별 결측치 개수 확인\n\n# 각 컬럼의 결측치 개수\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\n예제: 컬럼별 결측치 비율 확인\n\n# 각 컬럼의 결측치 비율 (%)\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64\n\n\n결측치가 5% 미만인 경우 해당 행을 제거하는 것이 일반적이며, 10% 이상인 경우 평균값이나 중앙값으로 대체하거나 별도의 결측치 처리 기법을 고려해야 한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#범주형-변수-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.5 범주형 변수 탐색",
    "text": "2.5 범주형 변수 탐색\n범주형 변수는 각 범주의 빈도를 확인하여 데이터의 분포를 파악한다. 시각화를 통해 불균형 여부를 쉽게 확인할 수 있다.\n\n2.5.1 종(species) 분포\n펭귄 종별 개체 수를 확인하여 데이터의 균형 상태를 파악한다.\n예제: 종별 빈도 확인\n\n# 종(species)별 개체 수\ndf[\"species\"].value_counts()\n\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n\n예제: 종별 분포 시각화\n\n# 종별 분포 막대 그래프\nsns.countplot(data=df, x=\"species\")\nplt.title(\"Distribution of Penguin Species\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.5.2 성별(sex) 분포\n성별 분포를 확인하여 데이터 수집 과정에서 성별 편향이 있는지 파악한다.\n예제: 성별 분포 시각화\n\n# 성별 분포 막대 그래프\nsns.countplot(data=df, x=\"sex\")\nplt.title(\"Distribution of Penguin Sex\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Count\")\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#수치형-변수-분포-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.6 수치형 변수 분포 확인",
    "text": "2.6 수치형 변수 분포 확인\n수치형 변수의 분포를 시각화하면 데이터의 중심, 퍼짐 정도, 왜도(skewness), 이상치 등을 직관적으로 파악할 수 있다.\n\n2.6.1 히스토그램\n히스토그램은 수치형 변수의 분포를 구간별 빈도로 나타낸 그래프이다. 데이터가 정규분포를 따르는지, 왜곡되어 있는지 등을 확인할 수 있다.\n예제: 전체 수치형 변수 히스토그램\n\n# 모든 수치형 변수의 히스토그램\ndf.hist(bins=20, figsize=(10, 8))\nplt.tight_layout()\nplt.suptitle(\"Histograms of Numeric Variables\", y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.6.2 박스플롯 (이상치 탐색)\n박스플롯은 사분위수를 기반으로 데이터의 분포와 이상치를 시각화하는 도구이다. 박스 밖의 점들은 이상치 후보로 간주할 수 있다.\n예제: 종별 체중 분포 및 이상치 확인\n\n# 종별 체중 분포 박스플롯\nplt.figure(figsize=(8, 4))\nsns.boxplot(data=df, x=\"species\", y=\"body_mass_g\")\nplt.title(\"Body Mass Distribution by Species\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n\n\n\n\n\n\n\n\n박스플롯에서 확인할 수 있는 정보는 다음과 같다.\n\n\n\n요소\n설명\n\n\n\n\n박스 아래쪽 경계\n제1사분위수(Q1, 25 백분위수)\n\n\n박스 중앙선\n중앙값(Q2, 50 백분위수)\n\n\n박스 위쪽 경계\n제3사분위수(Q3, 75 백분위수)\n\n\n수염(whisker)\nQ1 - 1.5×IQR ~ Q3 + 1.5×IQR 범위\n\n\n개별 점\n이상치 후보",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#변수-간-관계-탐색",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.7 변수 간 관계 탐색",
    "text": "2.7 변수 간 관계 탐색\n두 변수 간의 관계를 파악하면 변수 간 상관성, 패턴, 군집 등을 발견할 수 있다. 이는 예측 모델링이나 가설 검정의 기초 자료가 된다.\n\n2.7.1 두 수치형 변수 관계\n산점도는 두 수치형 변수 간의 관계를 점으로 표현한 그래프이다. 범주형 변수로 색상을 구분하면 그룹별 패턴을 동시에 확인할 수 있다.\n예제: 부리 길이와 깊이의 관계\n\n# 부리 길이와 깊이의 산점도 (종별 구분)\nsns.scatterplot(\n    data=df,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\"\n)\nplt.title(\"Relationship between Bill Length and Depth\")\nplt.xlabel(\"Bill Length (mm)\")\nplt.ylabel(\"Bill Depth (mm)\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2.7.2 페어플롯\n페어플롯은 모든 수치형 변수 쌍에 대한 산점도를 행렬 형태로 배치한 그래프이다. 대각선에는 각 변수의 히스토그램이나 커널 밀도 추정(KDE) 그래프가 표시된다.\n예제: 전체 변수 간 관계 확인\n\n# 모든 수치형 변수 쌍의 관계 시각화\nsns.pairplot(\n    df,\n    hue=\"species\",\n    diag_kind=\"hist\"\n)\nplt.suptitle(\"Pairplot of Penguin Features\", y=1.02)\nplt.show()\n\n\n\n\n\n\n\n\n페어플롯을 통해 다음을 파악할 수 있다.\n\n변수 간 선형 또는 비선형 관계\n그룹별 분포 차이\n다변량 군집 패턴\n변수 간 상관성",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#그룹별-통계-확인",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.8 그룹별 통계 확인",
    "text": "2.8 그룹별 통계 확인\n그룹별 통계량을 비교하면 범주형 변수가 수치형 변수에 미치는 영향을 정량적으로 파악할 수 있다.\n예제: 종별 평균 통계\n\n# 종별 부리 길이와 체중의 평균\ndf.groupby(\"species\")[[\"bill_length_mm\", \"body_mass_g\"]].mean()\n\n\n\n\n\n\n\n\nbill_length_mm\nbody_mass_g\n\n\nspecies\n\n\n\n\n\n\nAdelie\n38.791391\n3700.662252\n\n\nChinstrap\n48.833824\n3733.088235\n\n\nGentoo\n47.504878\n5076.016260\n\n\n\n\n\n\n\n추가적으로 표준편차, 중앙값 등 다양한 통계량을 함께 확인할 수 있다.\n예제: 종별 다양한 통계량\n\n# 종별 다양한 통계량 확인\ndf.groupby(\"species\")[[\"bill_length_mm\", \"body_mass_g\"]].agg([\"mean\", \"std\", \"median\"])\n\n\n\n\n\n\n\n\nbill_length_mm\nbody_mass_g\n\n\n\nmean\nstd\nmedian\nmean\nstd\nmedian\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\nAdelie\n38.791391\n2.663405\n38.80\n3700.662252\n458.566126\n3700.0\n\n\nChinstrap\n48.833824\n3.339256\n49.55\n3733.088235\n384.335081\n3700.0\n\n\nGentoo\n47.504878\n3.081857\n47.30\n5076.016260\n504.116237\n5000.0",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/02. 탐색적 데이터 분석(EDA).html#요약",
    "href": "part1/02. 탐색적 데이터 분석(EDA).html#요약",
    "title": "2  탐색적 데이터 분석(EDA)",
    "section": "2.9 요약",
    "text": "2.9 요약\n이 장에서는 탐색적 데이터 분석(EDA)의 기본 과정과 주요 기법을 학습했다. 주요 내용은 다음과 같다.\nEDA 기본 단계\n\n\n\n\n\n\n\n\n단계\n주요 함수/기법\n목적\n\n\n\n\n데이터 구조 확인\nshape, info()\n데이터 크기, 타입, 결측치 파악\n\n\n기초 통계량 확인\ndescribe()\n중심 경향성과 분산 이해\n\n\n결측치 탐색\nisna(), sum(), mean()\n결측치 개수와 비율 확인\n\n\n분포 확인\n히스토그램, 박스플롯\n데이터 분포와 이상치 탐색\n\n\n관계 탐색\n산점도, 페어플롯\n변수 간 상관성과 패턴 발견\n\n\n그룹 비교\ngroupby()\n그룹별 통계적 차이 확인\n\n\n\n주요 발견 사항 (펭귄 데이터 예시)\n\n펭귄 종(species)에 따라 부리 길이와 체중 분포가 명확하게 구분된다\nAdelie 종은 다른 종에 비해 상대적으로 체중과 부리 길이가 작다\n일부 변수에 결측치가 존재하므로 본격적인 분석 전에 처리 전략이 필요하다\n성별과 종에 따른 신체 측정값의 차이가 관찰된다\n\nEDA는 데이터의 특성을 이해하고 가설을 수립하는 필수 과정이다. 다음 장에서는 EDA를 통해 발견한 결측치와 이상치를 처리하는 데이터 전처리 기법을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>탐색적 데이터 분석(EDA)</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html",
    "href": "part1/03. 결측치 처리.html",
    "title": "3  결측치 처리",
    "section": "",
    "text": "3.1 데이터 로드 및 결측치 현황 확인\n결측치(Missing Value)는 데이터 수집 과정에서 다양한 이유로 발생하는 누락된 값을 의미한다. 결측치가 존재하면 통계 분석이나 머신러닝 모델의 정확도가 저하될 수 있으므로 적절한 처리가 필수적이다. 이 장에서는 결측치를 탐지하고 제거하거나 대체하는 다양한 기법을 학습한다. 결측치 처리 방법은 데이터의 특성과 결측치의 패턴에 따라 달라지므로, 각 상황에 맞는 적절한 전략을 선택하는 것이 중요하다.\n결측치 처리를 시작하기 전에 먼저 데이터를 불러오고 결측치가 어느 컬럼에 얼마나 존재하는지 파악해야 한다.\n예제: 데이터 로드 및 결측치 개수 확인\nimport pandas as pd\nimport seaborn as sns\n\n# 데이터 로드\ndf = sns.load_dataset(\"penguins\")\n\n# 컬럼별 결측치 개수 확인\ndf.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "href": "part1/03. 결측치 처리.html#결측치-비율-확인",
    "title": "3  결측치 처리",
    "section": "3.2 결측치 비율 확인",
    "text": "3.2 결측치 비율 확인\n결측치의 개수뿐만 아니라 전체 데이터 대비 비율을 확인하면 결측치 처리 전략을 수립하는 데 도움이 된다. 일반적으로 결측치 비율이 5% 미만이면 해당 행을 제거하고, 5~10%인 경우 대체 방법을 고려하며, 10% 이상인 경우 신중한 분석이 필요하다.\n예제: 결측치 비율 확인\n\n# 컬럼별 결측치 비율 (%)\n(df.isna().mean() * 100).round(1)\n\nspecies              0.0\nisland               0.0\nbill_length_mm       0.6\nbill_depth_mm        0.6\nflipper_length_mm    0.6\nbody_mass_g          0.6\nsex                  3.2\ndtype: float64\n\n\n위 결과를 통해 각 컬럼의 결측치 비율을 백분율로 확인할 수 있다. 비율이 낮은 컬럼은 행 제거를, 비율이 높은 컬럼은 대체 방법을 고려한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "href": "part1/03. 결측치 처리.html#결측치-제거-행-삭제",
    "title": "3  결측치 처리",
    "section": "3.3 결측치 제거 (행 삭제)",
    "text": "3.3 결측치 제거 (행 삭제)\n결측치가 소수이거나 분석에 큰 영향을 미치지 않는 경우, 해당 행을 삭제하는 것이 가장 간단한 방법이다. 다만, 제거로 인한 데이터 손실이 크지 않은지 반드시 확인해야 한다.\n\n3.3.1 결측치가 있는 모든 행 제거\n하나라도 결측치가 있는 행을 모두 제거하는 방법이다. 가장 보수적인 접근이지만 데이터 손실이 클 수 있다.\n예제: 전체 결측치 행 제거\n\n# 결측치가 하나라도 있는 행 제거\ndf_drop_all = df.dropna()\n\n# 제거 후 데이터 크기 확인\nprint(f\"원본 데이터: {df.shape}\")\nprint(f\"결측치 제거 후: {df_drop_all.shape}\")\n\n원본 데이터: (344, 7)\n결측치 제거 후: (333, 7)\n\n\n\n\n3.3.2 결측치가 특정 개수 이상인 행 제거\n전체 컬럼 중 일부만 결측치인 경우 해당 행을 유지하고, 결측치가 많은 행만 제거하는 방법이다.\n예제: 결측치가 3개 이상인 행 제거\n\n# 각 행의 결측치 개수 계산\ndf[\"na_count\"] = df.isna().sum(axis=1)\n\n# 결측치가 3개 미만인 행만 유지\ndf_row_filtered = df[df[\"na_count\"] &lt; 3].drop(columns=\"na_count\")\n\n# 제거 후 데이터 크기 확인\nprint(f\"결측치 필터링 후: {df_row_filtered.shape}\")\n\n결측치 필터링 후: (342, 7)\n\n\n예제: thresh 파라미터를 사용한 행 제거\n\n# 최소 n개의 비결측값이 있는 행만 유지\n# 전체 컬럼에서 2개까지 결측치 허용\ndf_thresh = df.dropna(thresh=len(df.columns) - 2)\n\nprint(f\"thresh 적용 후: {df_thresh.shape}\")\n\nthresh 적용 후: (342, 8)\n\n\nthresh 파라미터는 행을 유지하기 위해 필요한 최소 비결측값 개수를 지정한다. 예를 들어, 전체 컬럼이 8개인 경우 thresh=6이면 최소 6개의 값이 있어야 행이 유지된다.\n\n\n3.3.3 특정 컬럼 기준 제거\n특정 컬럼의 결측치만 제거하고 싶을 때 사용하는 방법이다. 분석에 필수적인 컬럼의 결측치를 제거할 때 유용하다.\n예제: 특정 컬럼 결측치 제거\n\n# sex 컬럼에 결측치가 있는 행 제거\ndf_drop_sex = df.dropna(subset=[\"sex\"])\n\nprint(f\"sex 컬럼 결측치 제거 후: {df_drop_sex.shape}\")\n\nsex 컬럼 결측치 제거 후: (333, 8)\n\n\n여러 컬럼을 동시에 지정할 수도 있다.\n\n# 여러 컬럼 중 하나라도 결측치가 있으면 제거\ndf_drop_multi = df.dropna(subset=[\"sex\", \"bill_length_mm\"])",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#수치형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.4 수치형 변수 결측치 대체",
    "text": "3.4 수치형 변수 결측치 대체\n결측치를 제거하면 데이터가 손실되므로, 특정 값으로 대체하는 방법을 고려할 수 있다. 수치형 변수는 주로 평균, 중앙값, 최빈값 등의 대푯값으로 대체한다.\n\n3.4.1 평균(mean)으로 대체\n평균은 데이터의 중심 경향을 나타내는 대표적인 값이다. 다만, 이상치에 민감하므로 이상치가 많은 경우 중앙값 사용을 권장한다.\n예제: 단일 컬럼 평균 대체\n\n# 데이터 복사 (원본 보존)\ndf_mean = df.copy()\n\n# bill_length_mm 컬럼의 결측치를 평균으로 대체\ndf_mean[\"bill_length_mm\"] = df_mean[\"bill_length_mm\"].fillna(\n    df_mean[\"bill_length_mm\"].mean()\n)\n\n# 결측치 확인\nprint(df_mean[\"bill_length_mm\"].isna().sum())\n\n0\n\n\n예제: 다중 컬럼 평균 대체 (방법 1)\n\n# 수치형 컬럼 목록\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf_cols = df.copy()\n\n# 여러 컬럼의 결측치를 각각의 평균으로 대체\ndf_cols[num_cols] = df_cols[num_cols].fillna(df_cols[num_cols].mean())\n\n# 결측치 확인\nprint(df_cols[num_cols].isna().sum())\n\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\ndtype: int64\n\n\n예제: 다중 컬럼 평균 대체 (방법 2 - 반복문 활용)\n\ndf_loop = df.copy()\n\n# 반복문을 사용한 평균 대체\nfor col in num_cols:\n    mean_value = df_loop[col].mean()\n    df_loop[col] = df_loop[col].fillna(mean_value)\n\n# 결측치 확인\nprint(df_loop[num_cols].isna().sum())\n\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\ndtype: int64\n\n\n예제: 그룹별 평균으로 대체\n범주형 변수로 그룹을 나눈 후, 각 그룹의 평균으로 결측치를 대체하는 방법이다. 예를 들어, 펭귄 종별로 평균 부리 길이가 다르므로 종별 평균으로 대체하는 것이 더 정확하다.\n\ndf_group = df.copy()\n\n# 종(species)별 평균으로 결측치 대체\ndf_group[num_cols] = df_group.groupby(\"species\")[num_cols].transform(\n    lambda x: x.fillna(x.mean())\n)\n\n# 결측치 확인\nprint(df_group[num_cols].isna().sum())\n\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\ndtype: int64\n\n\ntransform() 메서드는 그룹별 계산 결과를 원래 DataFrame의 인덱스에 맞춰 반환하므로 대체 작업에 적합하다.\n\n\n3.4.2 중앙값(median)으로 대체\n중앙값은 이상치의 영향을 받지 않는 강건한(robust) 통계량이다. 데이터 분포가 왜곡되어 있거나 이상치가 존재하는 경우 평균보다 중앙값 대체가 더 적절하다.\n예제: 중앙값으로 대체\n\ndf_median = df.copy()\n\n# bill_depth_mm 컬럼의 결측치를 중앙값으로 대체\ndf_median[\"bill_depth_mm\"] = df_median[\"bill_depth_mm\"].fillna(\n    df_median[\"bill_depth_mm\"].median()\n)\n\n# 결측치 확인\nprint(df_median[\"bill_depth_mm\"].isna().sum())\n\n0\n\n\n중앙값도 평균과 마찬가지로 그룹별 중앙값으로 대체할 수 있다.\n\n# 종별 중앙값으로 대체\ndf_group_median = df.copy()\ndf_group_median[num_cols] = df_group_median.groupby(\"species\")[num_cols].transform(\n    lambda x: x.fillna(x.median())\n)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "href": "part1/03. 결측치 처리.html#범주형-변수-결측치-대체",
    "title": "3  결측치 처리",
    "section": "3.5 범주형 변수 결측치 대체",
    "text": "3.5 범주형 변수 결측치 대체\n범주형 변수는 수치형과 달리 평균이나 중앙값을 사용할 수 없다. 대신 최빈값(mode)으로 대체하거나, 결측치를 별도의 범주로 처리하는 방법을 사용한다.\n\n3.5.1 최빈값(mode)으로 대체\n최빈값은 가장 자주 나타나는 값으로, 범주형 변수의 결측치를 대체할 때 일반적으로 사용된다.\n예제: 단일 컬럼 최빈값 대체\n\ndf_mode = df.copy()\n\n# sex 컬럼의 최빈값 찾기\nmode_sex = df_mode[\"sex\"].mode()[0]\n\n# 최빈값으로 결측치 대체\ndf_mode[\"sex\"] = df_mode[\"sex\"].fillna(mode_sex)\n\n# 결측치 확인\nprint(df_mode[\"sex\"].isna().sum())\n\n0\n\n\nmode() 메서드는 Series를 반환하므로 [0]을 사용하여 첫 번째 최빈값을 추출한다.\n예제: 다중 범주형 컬럼 최빈값 대체\n\ndf_mode_multi = df.copy()\n\n# 범주형 컬럼 목록\ncat_cols = [\"sex\", \"island\"]\n\n# 반복문을 사용한 최빈값 대체\nfor col in cat_cols:\n    mode_value = df_mode_multi[col].mode()[0]\n    df_mode_multi[col] = df_mode_multi[col].fillna(mode_value)\n\n# 결측치 확인\nprint(df_mode_multi[cat_cols].isna().sum())\n\nsex       0\nisland    0\ndtype: int64\n\n\n예제: 그룹별 최빈값으로 대체\n특정 그룹 내에서 가장 빈번한 값으로 대체하는 방법이다.\n\ndf_mode_group = df.copy()\n\n# 종별 성별 최빈값으로 대체\ndf_mode_group[\"sex\"] = df_mode_group.groupby(\"species\")[\"sex\"].transform(\n    lambda x: x.fillna(x.mode()[0] if not x.mode().empty else \"Unknown\")\n)\n\n# 결측치 확인\nprint(df_mode_group[\"sex\"].isna().sum())\n\n0\n\n\n예제: 다중 컬럼 그룹별 최빈값 대체\n\ndf_mode_group_multi = df.copy()\n\ncat_cols = [\"sex\", \"island\"]\n\n# 여러 범주형 컬럼을 그룹별 최빈값으로 대체\nfor col in cat_cols:\n    df_mode_group_multi[col] = df_mode_group_multi.groupby(\"species\")[col].transform(\n        lambda x: x.fillna(x.mode()[0] if not x.mode().empty else \"Unknown\")\n    )\n\n# 결측치 확인\nprint(df_mode_group_multi[cat_cols].isna().sum())\n\nsex       0\nisland    0\ndtype: int64\n\n\n\n\n3.5.2 명시적 범주 추가\n결측치를 “Unknown”, “Missing” 등의 별도 범주로 명시적으로 표시하는 방법이다. 결측치 자체가 의미 있는 정보일 수 있는 경우 유용하다.\n예제: 결측치를 별도 범주로 대체\n\ndf_category = df.copy()\n\n# sex 컬럼의 결측치를 \"Unknown\"으로 대체\ndf_category[\"sex\"] = df_category[\"sex\"].fillna(\"Unknown\")\n\n# 범주 분포 확인\nprint(df_category[\"sex\"].value_counts())\n\nsex\nMale       168\nFemale     165\nUnknown     11\nName: count, dtype: int64\n\n\n이 방법은 결측치의 패턴이 종속 변수와 관련이 있을 때 특히 유용하다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "href": "part1/03. 결측치 처리.html#결측치-처리-후-검증",
    "title": "3  결측치 처리",
    "section": "3.6 결측치 처리 후 검증",
    "text": "3.6 결측치 처리 후 검증\n결측치 처리를 완료한 후에는 반드시 결측치가 제대로 처리되었는지 검증해야 한다.\n예제: 결측치 처리 후 확인\n\n# 그룹별 대체를 적용한 데이터 검증\nprint(\"결측치 개수:\")\nprint(df_group.isna().sum())\nprint(\"\\n데이터 정보:\")\ndf_group.info()\n\n결측치 개수:\nspecies               0\nisland                0\nbill_length_mm        0\nbill_depth_mm         0\nflipper_length_mm     0\nbody_mass_g           0\nsex                  11\nna_count              0\ndtype: int64\n\n데이터 정보:\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     344 non-null    float64\n 3   bill_depth_mm      344 non-null    float64\n 4   flipper_length_mm  344 non-null    float64\n 5   body_mass_g        344 non-null    float64\n 6   sex                333 non-null    str    \n 7   na_count           344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB\n\n\n모든 컬럼의 결측치가 0인지 확인하고, 각 컬럼의 데이터 타입과 비결측값 개수가 예상과 일치하는지 점검한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/03. 결측치 처리.html#요약",
    "href": "part1/03. 결측치 처리.html#요약",
    "title": "3  결측치 처리",
    "section": "3.7 요약",
    "text": "3.7 요약\n이 장에서는 결측치를 탐지하고 처리하는 다양한 방법을 학습했다. 주요 내용은 다음과 같다.\n결측치 처리 방법 비교\n\n\n\n\n\n\n\n\n\n\n처리 방법\n적용 대상\n장점\n단점\n사용 상황\n\n\n\n\n행 제거\n전체\n간단하고 명확함\n데이터 손실\n결측치 비율 &lt; 5%\n\n\n평균 대체\n수치형\n구현이 쉬움\n이상치에 민감, 분산 감소\n정규분포 데이터\n\n\n중앙값 대체\n수치형\n이상치에 강건함\n분산 감소\n왜곡된 분포, 이상치 존재\n\n\n최빈값 대체\n범주형\n자연스러운 대체\n불균형 심화 가능\n명확한 최빈값 존재\n\n\n그룹별 대체\n전체\n그룹 특성 반영\n복잡한 구현\n그룹 간 차이가 큰 경우\n\n\n범주 추가\n범주형\n정보 손실 없음\n범주 증가\n결측치에 의미가 있는 경우\n\n\n\n결측치 처리 권장 순서\n\n결측치 현황 파악: isna().sum(), isna().mean() 사용\n제거 가능한 행/열 판단: 결측치 비율과 데이터 중요도 고려\n수치형 변수 대체: 그룹별 평균 또는 중앙값 우선 고려\n범주형 변수 대체: 그룹별 최빈값 또는 “Unknown” 범주 추가\n최종 검증: isna().sum(), info()로 처리 결과 확인\n\n결측치 처리는 데이터 분석의 품질을 좌우하는 중요한 과정이다. 데이터의 특성과 분석 목적에 맞는 적절한 방법을 선택해야 하며, 처리 후에는 반드시 결과를 검증해야 한다. 다음 장에서는 이상치 탐지 및 처리 방법을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>결측치 처리</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html",
    "href": "part1/04. 이상치 탐지.html",
    "title": "4  이상치 탐지",
    "section": "",
    "text": "4.1 분석 대상 변수 선택\n이상치(Outlier)는 다른 관측값들과 현저히 다른 값을 의미한다. 이상치는 측정 오류, 입력 오류, 또는 실제로 극단적인 사건에 의해 발생할 수 있다. 이상치가 존재하면 통계 분석 결과가 왜곡되고 머신러닝 모델의 성능이 저하될 수 있으므로, 적절한 탐지와 처리가 필요하다. 이 장에서는 통계적 방법과 머신러닝 기법을 활용한 다양한 이상치 탐지 방법을 학습한다.\n예제: 데이터 로드\n이상치 탐지는 주로 수치형 변수를 대상으로 수행한다. 범주형 변수는 이상치 개념이 명확하지 않기 때문에 일반적으로 제외한다.\n예제: 수치형 변수 추출\n# 분석 대상 수치형 컬럼\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\n# 수치형 데이터만 선택\ndf_num = df[num_cols]\n\n# 기본 통계량 확인\nprint(df_num.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean        43.921930      17.151170         200.915205  4201.754386\nstd          5.459584       1.974793          14.061714   801.954536\nmin         32.100000      13.100000         172.000000  2700.000000\n25%         39.225000      15.600000         190.000000  3550.000000\n50%         44.450000      17.300000         197.000000  4050.000000\n75%         48.500000      18.700000         213.000000  4750.000000\nmax         59.600000      21.500000         231.000000  6300.000000",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#분포-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.2 분포 기반 이상치 탐지",
    "text": "4.2 분포 기반 이상치 탐지\n데이터의 분포를 시각화하여 이상치를 직관적으로 파악하는 방법이다. 박스플롯은 사분위수를 기반으로 이상치를 자동으로 표시해주므로 가장 먼저 사용하는 탐색 도구이다.\n\n4.2.1 박스플롯으로 이상치 확인\n박스플롯은 각 변수의 중앙값, 사분위수, 이상치를 한눈에 보여준다. 다만, 변수마다 척도가 다르면 비교가 어려우므로 표준화를 적용하는 것이 좋다.\n예제: 표준화 후 박스플롯\n\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\n# 표준화 (평균=0, 표준편차=1)\nscaler = StandardScaler()\ndf_num_scaled = pd.DataFrame(\n    scaler.fit_transform(df_num),\n    columns=df_num.columns\n)\n\n# 박스플롯 시각화\ndf_num_scaled.boxplot(figsize=(10, 5))\nplt.title(\"Boxplot of Standardized Numeric Variables\")\nplt.ylabel(\"Standardized Value\")\nplt.axhline(y=0, color='r', linestyle='--', linewidth=0.8, alpha=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n표준화를 통해 모든 변수가 동일한 척도(평균 0, 표준편차 1)로 변환되어 이상치를 더 쉽게 비교할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#iqr-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.3 IQR 기반 이상치 탐지",
    "text": "4.3 IQR 기반 이상치 탐지\nIQR(Interquartile Range, 사분위수 범위)은 Q3(75 백분위수)에서 Q1(25 백분위수)을 뺀 값으로, 데이터의 중간 50%가 분포하는 범위를 나타낸다. 일반적으로 Q1 - 1.5×IQR 미만이거나 Q3 + 1.5×IQR 초과인 값을 이상치로 정의한다.\n\n4.3.1 IQR 계산\n예제: 사분위수 및 IQR 계산\n\n# 제1사분위수 (Q1, 25%)\nQ1 = df_num.quantile(0.25)\n\n# 제3사분위수 (Q3, 75%)\nQ3 = df_num.quantile(0.75)\n\n# IQR 계산\nIQR = Q3 - Q1\n\nprint(\"Q1 (25%):\")\nprint(Q1)\nprint(\"\\nQ3 (75%):\")\nprint(Q3)\nprint(\"\\nIQR:\")\nprint(IQR)\n\nQ1 (25%):\nbill_length_mm         39.225\nbill_depth_mm          15.600\nflipper_length_mm     190.000\nbody_mass_g          3550.000\nName: 0.25, dtype: float64\n\nQ3 (75%):\nbill_length_mm         48.5\nbill_depth_mm          18.7\nflipper_length_mm     213.0\nbody_mass_g          4750.0\nName: 0.75, dtype: float64\n\nIQR:\nbill_length_mm          9.275\nbill_depth_mm           3.100\nflipper_length_mm      23.000\nbody_mass_g          1200.000\ndtype: float64\n\n\n\n\n4.3.2 이상치 조건 정의\nIQR 기반 이상치 범위는 다음과 같다.\n\n하한: Q1 - 1.5 × IQR\n상한: Q3 + 1.5 × IQR\n\n이 범위를 벗어나는 값을 이상치로 판단한다.\n예제: 이상치 조건 정의\n\n# 이상치 조건: 하한 미만 또는 상한 초과\noutlier_condition = (df_num &lt; (Q1 - 1.5 * IQR)) | (df_num &gt; (Q3 + 1.5 * IQR))\n\n# 컬럼별 이상치 개수\nprint(\"컬럼별 이상치 개수:\")\nprint(outlier_condition.sum())\n\n컬럼별 이상치 개수:\nbill_length_mm       0\nbill_depth_mm        0\nflipper_length_mm    0\nbody_mass_g          0\ndtype: int64\n\n\n\n\n4.3.3 이상치가 있는 행 확인\n하나의 컬럼이라도 이상치가 있으면 해당 행 전체를 이상치로 판단하는 방법이다.\n예제: 이상치 행 추출\n\n# 하나라도 이상치가 있는 행\noutlier_rows = outlier_condition.any(axis=1)\n\n# 이상치 행 확인\nprint(f\"이상치가 있는 행 수: {outlier_rows.sum()}\")\nprint(\"\\n이상치 행 샘플:\")\nprint(df[outlier_rows].head())\n\n이상치가 있는 행 수: 0\n\n이상치 행 샘플:\nEmpty DataFrame\nColumns: [species, island, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, sex]\nIndex: []\n\n\n\n\n4.3.4 이상치 제거\n이상치를 제거하여 정제된 데이터셋을 생성한다.\n예제: IQR 기반 이상치 제거\n\n# 이상치가 없는 행만 유지\ndf_iqr_clean = df[~outlier_rows]\n\nprint(f\"원본 데이터: {df.shape}\")\nprint(f\"이상치 제거 후: {df_iqr_clean.shape}\")\nprint(f\"제거된 행 수: {outlier_rows.sum()}\")\n\n원본 데이터: (344, 7)\n이상치 제거 후: (344, 7)\n제거된 행 수: 0\n\n\nIQR 방법은 분포에 대한 가정이 없어 안전하게 사용할 수 있으며, 실무에서 가장 기본적으로 사용되는 방법이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#z-score-기반-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.4 Z-score 기반 이상치 탐지",
    "text": "4.4 Z-score 기반 이상치 탐지\nZ-score는 각 값이 평균으로부터 표준편차의 몇 배만큼 떨어져 있는지를 나타내는 지표이다. 일반적으로 |Z| &gt; 3인 경우를 이상치로 판단한다. 이 방법은 계산이 간단하지만 데이터가 정규분포를 따른다고 가정하므로, 분포가 왜곡된 경우 적절하지 않을 수 있다.\n예제: Z-score 계산 및 이상치 탐지\n\nfrom scipy.stats import zscore\n\n# Z-score 계산\nz_scores = df_num.apply(zscore)\n\nprint(\"Z-score 샘플:\")\nprint(z_scores.head())\n\nZ-score 샘플:\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0             NaN            NaN                NaN          NaN\n1             NaN            NaN                NaN          NaN\n2             NaN            NaN                NaN          NaN\n3             NaN            NaN                NaN          NaN\n4             NaN            NaN                NaN          NaN\n\n\n\n4.4.1 임계값 기준 이상치 탐지\n절댓값이 3을 초과하는 Z-score를 가진 값을 이상치로 판단한다.\n예제: Z-score 기반 이상치 제거\n\n# abs(Z-score) &gt; 3 인 경우 이상치로 판단\noutlier_z = (np.abs(z_scores) &gt; 3).any(axis=1)\n\n# 이상치 제거\ndf_z_clean = df[~outlier_z]\n\nprint(f\"원본 데이터: {df.shape}\")\nprint(f\"Z-score 이상치 제거 후: {df_z_clean.shape}\")\nprint(f\"제거된 행 수: {outlier_z.sum()}\")\n\n원본 데이터: (344, 7)\nZ-score 이상치 제거 후: (344, 7)\n제거된 행 수: 0\n\n\nZ-score 방법은 계산이 간단하고 직관적이지만, 정규분포 가정을 위반하면 정확도가 떨어진다. 따라서 데이터의 분포를 먼저 확인한 후 사용하는 것이 좋다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "href": "part1/04. 이상치 탐지.html#범주별-이상치-탐지",
    "title": "4  이상치 탐지",
    "section": "4.5 범주별 이상치 탐지",
    "text": "4.5 범주별 이상치 탐지\n전체 데이터를 대상으로 이상치를 탐지하면 그룹 간 차이가 큰 경우 정상 값이 이상치로 오판될 수 있다. 예를 들어, 펭귄 종에 따라 체중 분포가 크게 다르므로, 종별로 이상치를 탐지하는 것이 더 정확하다.\n\n4.5.1 그룹별 IQR 이상치 탐지\n각 그룹 내에서 독립적으로 IQR 기반 이상치를 탐지하는 방법이다.\n예제: 종별 IQR 이상치 탐지\n\ndef iqr_outlier_by_group(group):\n    \"\"\"그룹별 IQR 기반 이상치 제거 함수\"\"\"\n    # 그룹 내 Q1, Q3, IQR 계산\n    Q1 = group[num_cols].quantile(0.25)\n    Q3 = group[num_cols].quantile(0.75)\n    IQR = Q3 - Q1\n    \n    # 이상치 조건\n    condition = (group[num_cols] &lt; (Q1 - 1.5 * IQR)) | \\\n                (group[num_cols] &gt; (Q3 + 1.5 * IQR))\n    \n    # 이상치가 없는 행만 반환\n    return group[~condition.any(axis=1)]\n\n# 종별로 이상치 제거 적용\ndf_group_iqr_clean = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(iqr_outlier_by_group)\n      .reset_index(drop=True)\n)\n\nprint(f\"원본 데이터: {df.shape}\")\nprint(f\"그룹별 IQR 이상치 제거 후: {df_group_iqr_clean.shape}\")\nprint(f\"제거된 행 수: {df.shape[0] - df_group_iqr_clean.shape[0]}\")\n\n원본 데이터: (344, 7)\n그룹별 IQR 이상치 제거 후: (338, 6)\n제거된 행 수: 6\n\n\n그룹별 이상치 탐지는 그룹 간 특성 차이를 고려하므로 실무에서 가장 권장되는 방법이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "href": "part1/04. 이상치 탐지.html#밀도-기반-이상치-탐지-lof",
    "title": "4  이상치 탐지",
    "section": "4.6 밀도 기반 이상치 탐지 (LOF)",
    "text": "4.6 밀도 기반 이상치 탐지 (LOF)\nLOF(Local Outlier Factor)는 각 데이터 포인트의 주변 밀도를 계산하여 이상치를 탐지하는 방법이다. 주변 데이터와 비교하여 밀도가 현저히 낮은 점을 이상치로 판단한다. 이 방법은 비선형적인 데이터 분포나 군집 구조가 복잡한 경우에 효과적이다.\n예제: LOF를 사용한 이상치 탐지\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# 결측치 제거 (LOF는 결측치를 처리하지 못함)\ndf_lof = df_num.dropna()\n\n# LOF 모델 생성 (이웃 수 20개)\nlof = LocalOutlierFactor(n_neighbors=20)\n\n# 이상치 예측 (-1: 이상치, 1: 정상)\noutlier_lof = lof.fit_predict(df_lof)\n\n# 정상 데이터만 추출\ndf_lof_clean = df.loc[df_lof.index][outlier_lof == 1]\n\nprint(f\"분석 대상 데이터: {len(df_lof)}\")\nprint(f\"LOF 이상치 제거 후: {df_lof_clean.shape}\")\nprint(f\"탐지된 이상치 수: {(outlier_lof == -1).sum()}\")\n\n분석 대상 데이터: 342\nLOF 이상치 제거 후: (337, 7)\n탐지된 이상치 수: 5\n\n\nLOF는 주변 데이터와의 상대적 밀도를 고려하므로 데이터가 여러 군집으로 구성되어 있거나 비선형 관계가 있는 경우 IQR이나 Z-score보다 우수한 성능을 보인다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "href": "part1/04. 이상치 탐지.html#트리-기반-이상치-탐지-isolation-forest",
    "title": "4  이상치 탐지",
    "section": "4.7 트리 기반 이상치 탐지 (Isolation Forest)",
    "text": "4.7 트리 기반 이상치 탐지 (Isolation Forest)\nIsolation Forest는 이상치가 정상 데이터보다 쉽게 분리(isolate)된다는 아이디어에 기반한다. 랜덤하게 특성과 분할 값을 선택하여 트리를 구성할 때, 이상치는 적은 분할로 고립되는 반면 정상 데이터는 많은 분할이 필요하다. 이 방법은 고차원 데이터에 강건하며 실무에서 널리 사용된다.\n예제: Isolation Forest를 사용한 이상치 탐지\n\nfrom sklearn.ensemble import IsolationForest\n\n# Isolation Forest 모델 생성\n# contamination: 예상 이상치 비율 (5%)\niso = IsolationForest(contamination=0.05, random_state=42)\n\n# 이상치 예측 (-1: 이상치, 1: 정상)\noutlier_if = iso.fit_predict(df_lof)\n\n# 정상 데이터만 추출\ndf_if_clean = df.loc[df_lof.index][outlier_if == 1]\n\nprint(f\"분석 대상 데이터: {len(df_lof)}\")\nprint(f\"Isolation Forest 이상치 제거 후: {df_if_clean.shape}\")\nprint(f\"탐지된 이상치 수: {(outlier_if == -1).sum()}\")\n\n분석 대상 데이터: 342\nIsolation Forest 이상치 제거 후: (324, 7)\n탐지된 이상치 수: 18\n\n\nIsolation Forest는 다음과 같은 장점이 있다.\n\n고차원 데이터에서도 효율적으로 작동\n분포에 대한 가정이 없음\n계산 속도가 빠름\n대규모 데이터셋에 적합\n\ncontamination 파라미터는 데이터셋에서 예상되는 이상치 비율을 지정한다. 일반적으로 0.05~0.1 사이의 값을 사용한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/04. 이상치 탐지.html#요약",
    "href": "part1/04. 이상치 탐지.html#요약",
    "title": "4  이상치 탐지",
    "section": "4.8 요약",
    "text": "4.8 요약\n이 장에서는 통계적 방법부터 머신러닝 기법까지 다양한 이상치 탐지 방법을 학습했다. 주요 내용은 다음과 같다.\n이상치 탐지 방법 비교\n\n\n\n\n\n\n\n\n\n\n방법\n원리\n장점\n단점\n적용 상황\n\n\n\n\n박스플롯\n시각적 탐색\n직관적, 설명 용이\n정량적 기준 부족\n초기 탐색 및 보고서\n\n\nIQR\n사분위수 범위\n간단, 분포 가정 불필요\n그룹 차이 미고려\n기본적인 이상치 제거\n\n\nZ-score\n표준편차 기반\n계산 간단, 해석 명확\n정규분포 가정 필요\n정규분포 데이터\n\n\n그룹별 IQR\n그룹 내 IQR\n그룹 특성 반영\n구현 복잡도 증가\n그룹 간 차이가 큰 경우 (가장 실무적)\n\n\nLOF\n국소 밀도 비교\n비선형 데이터 강건\n계산 비용 높음, 파라미터 민감\n복잡한 군집 구조\n\n\nIsolation Forest\n트리 기반 분리\n고차원 강건, 빠른 속도\n결과 해석 어려움\n대규모 고차원 데이터\n\n\n\n이상치 탐지 단계별 가이드\n\n시각적 탐색: 박스플롯으로 데이터 분포와 이상치 후보 확인\n기본 탐지: IQR 방법으로 명확한 이상치 제거\n정규성 확인: 데이터가 정규분포를 따르면 Z-score 적용\n그룹 고려: 범주별 차이가 있으면 그룹별 IQR 적용 (실무 권장)\n고급 기법: 비선형 관계나 복잡한 구조에는 LOF 적용\n대규모 처리: 고차원 대용량 데이터는 Isolation Forest 사용\n\n이상치 처리 전략\n이상치를 탐지한 후에는 다음 전략 중 하나를 선택한다.\n\n제거: 측정 오류나 입력 오류로 판단되는 경우\n대체: 극단값을 상/하한값으로 조정 (Winsorization)\n변환: 로그 변환이나 Box-Cox 변환으로 분포 정규화\n유지: 실제 극단 사건이며 분석에 중요한 경우\n\n이상치 탐지는 데이터의 품질을 높이는 중요한 과정이지만, 모든 이상치를 무조건 제거하는 것이 아니라 도메인 지식을 바탕으로 신중하게 판단해야 한다. 다음 장에서는 처리된 데이터를 바탕으로 변수 변환과 파생 변수 생성 기법을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>이상치 탐지</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html",
    "href": "part1/05. 스케일링.html",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1 스케일링의 필요성\n스케일링(Scaling)은 서로 다른 척도를 가진 변수들을 동일한 범위나 분포로 변환하는 과정이다. 머신러닝 알고리즘 중 거리 기반 모델(K-Nearest Neighbors, Support Vector Machine, K-means 등)은 변수의 크기에 민감하므로, 스케일링을 통해 모든 변수가 공정하게 모델에 기여하도록 만들어야 한다. 이 장에서는 다양한 스케일링 기법의 원리와 적용 방법, 그리고 상황에 맞는 선택 기준을 학습한다.\n예제: 데이터 로드\n변수마다 측정 단위와 범위가 다르면 모델 학습 과정에서 큰 값을 가진 변수가 지배적인 영향을 미치게 된다. 예를 들어, 체중(g 단위)은 수천의 값을 가지지만 부리 길이(mm 단위)는 수십의 값을 가지므로, 거리를 계산할 때 체중이 과도하게 반영된다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#스케일링의-필요성",
    "href": "part1/05. 스케일링.html#스케일링의-필요성",
    "title": "5  스케일링",
    "section": "",
    "text": "5.1.1 스케일 차이 확인\n각 변수의 평균, 표준편차, 최솟값, 최댓값을 확인하여 스케일 차이를 파악한다.\n예제: 변수별 스케일 확인\n\n# 기술 통계량 확인\nprint(df_num.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean        43.921930      17.151170         200.915205  4201.754386\nstd          5.459584       1.974793          14.061714   801.954536\nmin         32.100000      13.100000         172.000000  2700.000000\n25%         39.225000      15.600000         190.000000  3550.000000\n50%         44.450000      17.300000         197.000000  4050.000000\n75%         48.500000      18.700000         213.000000  4750.000000\nmax         59.600000      21.500000         231.000000  6300.000000\n\n\n위 결과에서 body_mass_g는 2000~6000 범위를 가지지만, bill_depth_mm는 13~22 범위를 가진다. 이러한 스케일 차이는 다음과 같은 문제를 일으킨다.\n\n거리 기반 모델에서 큰 값이 거리 계산을 지배\n경사 하강법 기반 최적화에서 수렴 속도 저하\n가중치 해석의 어려움",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "href": "part1/05. 스케일링.html#min-max-스케일링-정규화",
    "title": "5  스케일링",
    "section": "5.2 Min-Max 스케일링 (정규화)",
    "text": "5.2 Min-Max 스케일링 (정규화)\nMin-Max 스케일링은 모든 값을 지정된 범위(기본값 0~1)로 변환하는 방법이다. 데이터의 분포 형태는 유지하면서 범위만 조정한다.\n변환 공식\n\\[\nx' = \\frac{x - x_{min}}{x_{max} - x_{min}}\n\\]\n예제: Min-Max 스케일링 적용\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# 스케일러 생성\nscaler_minmax = MinMaxScaler()\n\n# 스케일링 수행\ndf_minmax = scaler_minmax.fit_transform(df_num)\n\n# DataFrame으로 변환\ndf_minmax = pd.DataFrame(df_minmax, columns=num_cols)\n\n# 결과 확인\nprint(df_minmax.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.429888       0.482282           0.490088     0.417154\nstd          0.198530       0.235094           0.238334     0.222765\nmin          0.000000       0.000000           0.000000     0.000000\n25%          0.259091       0.297619           0.305085     0.236111\n50%          0.449091       0.500000           0.423729     0.375000\n75%          0.596364       0.666667           0.694915     0.569444\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n스케일링 후 모든 변수의 최솟값은 0, 최댓값은 1이 된다. 이 방법은 신경망의 활성화 함수나 이미지 데이터 전처리에 자주 사용된다.\n장단점\n\n\n\n장점\n단점\n\n\n\n\n범위가 고정되어 해석이 명확함\n이상치에 매우 민감함\n\n\n신경망의 활성화 함수에 적합\n새로운 데이터의 범위를 벗어날 수 있음\n\n\n구현이 간단함\n변수 간 분산 비율이 왜곡될 수 있음\n\n\n\n예제: 수동 구현\n이해를 돕기 위해 공식을 직접 구현하면 다음과 같다.\n\n# Min-Max 스케일링 수동 구현\ndf_minmax_manual = df_num.copy()\n\nfor col in num_cols:\n    col_min = df_minmax_manual[col].min()\n    col_max = df_minmax_manual[col].max()\n    \n    df_minmax_manual[col] = (df_minmax_manual[col] - col_min) / (col_max - col_min)\n\n# 결과 확인\nprint(df_minmax_manual.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.429888       0.482282           0.490088     0.417154\nstd          0.198530       0.235094           0.238334     0.222765\nmin          0.000000       0.000000           0.000000     0.000000\n25%          0.259091       0.297619           0.305085     0.236111\n50%          0.449091       0.500000           0.423729     0.375000\n75%          0.596364       0.666667           0.694915     0.569444\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n\n5.2.1 일반화된 Min-Max 스케일링\n기본 범위인 [0, 1] 대신 임의의 범위 [a, b]로 변환할 수도 있다.\n일반화 공식\n\\[\nx' = a + \\frac{x - x_{min}}{x_{max} - x_{min}} \\times (b - a)\n\\]\n예제: 사용자 정의 범위로 스케일링\n\n# [-1, 1] 범위로 스케일링\nscaler_general = MinMaxScaler(feature_range=(-1, 1))\ndf_minmax_custom = scaler_general.fit_transform(df_num)\ndf_minmax_custom = pd.DataFrame(df_minmax_custom, columns=num_cols)\n\nprint(df_minmax_custom.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean        -0.140223      -0.035436          -0.019824    -0.165692\nstd          0.397061       0.470189           0.476668     0.445530\nmin         -1.000000      -1.000000          -1.000000    -1.000000\n25%         -0.481818      -0.404762          -0.389831    -0.527778\n50%         -0.101818       0.000000          -0.152542    -0.250000\n75%          0.192727       0.333333           0.389831     0.138889\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n예제: 일반화 함수 구현\n\ndef minmax_scale_ab(series, a, b):\n    \"\"\"임의의 [a, b] 범위로 Min-Max 스케일링\"\"\"\n    x_min = series.min()\n    x_max = series.max()\n    return a + (series - x_min) * (b - a) / (x_max - x_min)\n\n# 사용 예시\ndf_custom = df_num.copy()\ndf_custom['bill_length_mm'] = minmax_scale_ab(df_num['bill_length_mm'], -1, 1)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#표준화-standard-scaling",
    "href": "part1/05. 스케일링.html#표준화-standard-scaling",
    "title": "5  스케일링",
    "section": "5.3 표준화 (Standard Scaling)",
    "text": "5.3 표준화 (Standard Scaling)\n표준화는 각 변수의 평균을 0, 표준편차를 1로 변환하는 방법이다. 가장 널리 사용되는 스케일링 기법으로, 대부분의 머신러닝 알고리즘에 적합하다.\n변환 공식\n\\[\nx' = \\frac{x - \\mu}{\\sigma}\n\\]\n여기서 \\(\\mu\\)는 평균, \\(\\sigma\\)는 표준편차이다.\n예제: 표준화 적용\n\nfrom sklearn.preprocessing import StandardScaler\n\n# 스케일러 생성\nscaler_std = StandardScaler()\n\n# 표준화 수행\ndf_std = scaler_std.fit_transform(df_num)\n\n# DataFrame으로 변환\ndf_std = pd.DataFrame(df_std, columns=num_cols)\n\n# 결과 확인\nprint(df_std.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm   body_mass_g\ncount    3.420000e+02   3.420000e+02       3.420000e+02  3.420000e+02\nmean     8.310441e-17  -1.412775e-15      -8.310441e-16  4.155221e-17\nstd      1.001465e+00   1.001465e+00       1.001465e+00  1.001465e+00\nmin     -2.168526e+00  -2.054446e+00      -2.059320e+00 -1.875362e+00\n25%     -8.615697e-01  -7.866355e-01      -7.773731e-01 -8.138982e-01\n50%      9.686524e-02   7.547549e-02      -2.788381e-01 -1.895079e-01\n75%      8.397670e-01   7.854492e-01       8.606705e-01  6.846384e-01\nmax      2.875868e+00   2.205397e+00       2.142618e+00  2.620248e+00\n\n\n표준화 후 모든 변수의 평균은 0에 가깝고, 표준편차는 1에 가까워진다. 표준화는 데이터가 완전히 정규분포를 따르지 않아도 효과적으로 작동한다.\n장단점\n\n\n\n\n\n\n\n장점\n단점\n\n\n\n\n가장 널리 사용되는 방법\n고정된 범위가 없어 해석이 상대적\n\n\n정규분포를 완전히 따르지 않아도 작동\n이상치에 영향을 받음\n\n\n대부분의 머신러닝 알고리즘에 적합\n신경망의 일부 활성화 함수에 부적합할 수 있음\n\n\n\n예제: 수동 구현\n\n# 표준화 수동 구현\ndf_std_manual = df_num.copy()\n\nfor col in num_cols:\n    mean = df_std_manual[col].mean()\n    std = df_std_manual[col].std()\n    \n    df_std_manual[col] = (df_std_manual[col] - mean) / std\n\n# 결과 확인\nprint(df_std_manual.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm   body_mass_g\ncount    3.420000e+02   3.420000e+02       3.420000e+02  3.420000e+02\nmean     1.662088e-16  -1.454327e-15      -8.310441e-16  8.310441e-17\nstd      1.000000e+00   1.000000e+00       1.000000e+00  1.000000e+00\nmin     -2.165354e+00  -2.051440e+00      -2.056307e+00 -1.872618e+00\n25%     -8.603092e-01  -7.854846e-01      -7.762357e-01 -8.127074e-01\n50%      9.672352e-02   7.536506e-02      -2.784301e-01 -1.892307e-01\n75%      8.385383e-01   7.843001e-01       8.594113e-01  6.836368e-01\nmax      2.871660e+00   2.202170e+00       2.139483e+00  2.616415e+00",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#robust-스케일링-이상치-대응",
    "href": "part1/05. 스케일링.html#robust-스케일링-이상치-대응",
    "title": "5  스케일링",
    "section": "5.4 Robust 스케일링 (이상치 대응)",
    "text": "5.4 Robust 스케일링 (이상치 대응)\nRobust 스케일링은 중앙값과 IQR(Interquartile Range)을 사용하여 이상치의 영향을 최소화하는 방법이다. 평균과 표준편차 대신 중앙값과 사분위수를 사용하므로 이상치에 강건하다.\n변환 공식\n\\[\nx' = \\frac{x - \\text{median}}{\\text{IQR}}\n\\]\n여기서 \\(\\text{IQR} = Q_3 - Q_1\\)이다.\n예제: Robust 스케일링 적용\n\nfrom sklearn.preprocessing import RobustScaler\n\n# 스케일러 생성\nscaler_robust = RobustScaler()\n\n# Robust 스케일링 수행\ndf_robust = scaler_robust.fit_transform(df_num)\n\n# DataFrame으로 변환\ndf_robust = pd.DataFrame(df_robust, columns=num_cols)\n\n# 결과 확인\nprint(df_robust.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount    3.420000e+02     342.000000         342.000000   342.000000\nmean    -5.693479e-02      -0.048010           0.170226     0.126462\nstd      5.886344e-01       0.637030           0.611379     0.668295\nmin     -1.331536e+00      -1.354839          -1.086957    -1.125000\n25%     -5.633423e-01      -0.548387          -0.304348    -0.416667\n50%     -3.833739e-16       0.000000           0.000000     0.000000\n75%      4.366577e-01       0.451613           0.695652     0.583333\nmax      1.633423e+00       1.354839           1.478261     1.875000\n\n\nRobust 스케일링은 이상치가 많거나 데이터 분포가 심하게 왜곡된 경우 표준화보다 안정적인 결과를 제공한다.\n적용 상황\n\n이상치가 많은 실무 데이터\n분포가 왜곡되어 있는 경우\n이상치를 제거하지 않고 분석해야 하는 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#maxabs-스케일링",
    "href": "part1/05. 스케일링.html#maxabs-스케일링",
    "title": "5  스케일링",
    "section": "5.5 MaxAbs 스케일링",
    "text": "5.5 MaxAbs 스케일링\nMaxAbs 스케일링은 각 변수의 절댓값 최댓값으로 나누어 [-1, 1] 범위로 변환하는 방법이다. 값의 부호를 유지하므로 희소 행렬(Sparse Matrix)에 적합하다.\n변환 공식\n\\[\nx' = \\frac{x}{|x|_{max}}\n\\]\n예제: MaxAbs 스케일링 적용\n\nfrom sklearn.preprocessing import MaxAbsScaler\n\n# 스케일러 생성\nscaler_maxabs = MaxAbsScaler()\n\n# MaxAbs 스케일링 수행\ndf_maxabs = scaler_maxabs.fit_transform(df_num)\n\n# DataFrame으로 변환\ndf_maxabs = pd.DataFrame(df_maxabs, columns=num_cols)\n\n# 결과 확인\nprint(df_maxabs.describe())\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      342.000000     342.000000         342.000000   342.000000\nmean         0.736945       0.797729           0.869763     0.666945\nstd          0.091604       0.091851           0.060873     0.127294\nmin          0.538591       0.609302           0.744589     0.428571\n25%          0.658138       0.725581           0.822511     0.563492\n50%          0.745805       0.804651           0.852814     0.642857\n75%          0.813758       0.869767           0.922078     0.753968\nmax          1.000000       1.000000           1.000000     1.000000\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n값의 부호를 유지함\n이상치에 민감함\n\n\n희소 행렬의 0을 보존함\n범위가 비대칭일 수 있음\n\n\n계산이 간단함\n사용 사례가 제한적\n\n\n\n적용 상황\n\n희소 행렬 데이터(텍스트 데이터, 추천 시스템 등)\n양수와 음수를 모두 포함하는 데이터\n0 값의 보존이 중요한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#그룹별-스케일링",
    "href": "part1/05. 스케일링.html#그룹별-스케일링",
    "title": "5  스케일링",
    "section": "5.6 그룹별 스케일링",
    "text": "5.6 그룹별 스케일링\n데이터가 여러 그룹으로 구성되어 있고 그룹별로 분포가 다른 경우, 전체 데이터를 대상으로 스케일링하면 그룹 내 패턴이 왜곡될 수 있다. 그룹별 스케일링은 각 그룹 내에서 독립적으로 스케일링을 수행하여 그룹 특성을 보존한다.\n예제: 종별 표준화\n\nfrom sklearn.preprocessing import StandardScaler\n\ndef scale_by_species(group):\n    \"\"\"그룹별 표준화 함수\"\"\"\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(group[num_cols])\n    return pd.DataFrame(scaled, columns=num_cols, index=group.index)\n\n# 종별로 표준화 적용\ndf_scaled_group = (\n    df.groupby(\"species\", group_keys=False)\n      .apply(scale_by_species)\n)\n\n# 결과 확인\nprint(df_scaled_group.groupby(df['species']).describe())\n\n          bill_length_mm                                              \\\n                   count          mean       std       min       25%   \nspecies                                                                \nAdelie             151.0  5.734927e-16  1.003328 -2.520705 -0.769010   \nChinstrap           68.0 -5.020494e-16  1.007435 -2.393591 -0.749356   \nGentoo             123.0  1.487518e-15  1.004090 -2.151914 -0.718364   \n\n                                        bill_depth_mm                ...  \\\n                50%       75%       max         count          mean  ...   \nspecies                                                              ...   \nAdelie     0.003243  0.737825  2.715546         151.0 -2.305735e-15  ...   \nChinstrap  0.216066  0.676151  2.765385          68.0 -9.902210e-16  ...   \nGentoo    -0.066751  0.666315  3.940673         123.0 -2.780071e-16  ...   \n\n          flipper_length_mm           body_mass_g                          \\\n                        75%       max       count          mean       std   \nspecies                                                                     \nAdelie             0.774246  3.075648       151.0  2.705709e-16  1.003328   \nChinstrap          0.731216  2.285051        68.0  2.024524e-16  1.007435   \nGentoo             0.590381  2.138713       123.0  2.346813e-17  1.004090   \n\n                                                             \n                min       25%       50%       75%       max  \nspecies                                                      \nAdelie    -1.861221 -0.767238 -0.001449  0.654941  2.350616  \nChinstrap -2.707974 -0.643746 -0.086732  0.568578  2.796633  \nGentoo    -2.242780 -0.748943 -0.151408  0.844483  2.437910  \n\n[3 rows x 32 columns]\n\n\n그룹별 스케일링은 각 그룹의 평균과 표준편차가 다를 때 유용하며, 그룹 내 비교가 중요한 경우 사용한다.\n적용 상황\n\n범주별로 분포가 크게 다른 경우\n그룹 내 패턴 분석이 목적인 경우\n도메인 지식상 그룹별 처리가 적절한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#파이프라인-적용",
    "href": "part1/05. 스케일링.html#파이프라인-적용",
    "title": "5  스케일링",
    "section": "5.7 파이프라인 적용",
    "text": "5.7 파이프라인 적용\n실무에서는 스케일링을 모델 학습 파이프라인에 포함시켜 데이터 전처리와 모델 학습을 일관성 있게 관리한다. scikit-learn의 Pipeline을 사용하면 전처리와 모델을 하나의 객체로 관리할 수 있다.\n예제: 스케일링과 모델을 파이프라인으로 구성\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# 파이프라인 생성\npipeline = Pipeline([\n    (\"scaler\", StandardScaler()),       # 1단계: 표준화\n    (\"model\", LogisticRegression())     # 2단계: 로지스틱 회귀 모델\n])\n\n# 파이프라인 사용 예시 (실제 학습 시)\n# pipeline.fit(X_train, y_train)\n# predictions = pipeline.predict(X_test)\n\n파이프라인을 사용하면 다음과 같은 장점이 있다.\n\n학습 데이터와 테스트 데이터에 동일한 스케일링 파라미터 적용\n코드 간결성과 재사용성 향상\n교차 검증 시 데이터 누수(data leakage) 방지",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/05. 스케일링.html#요약",
    "href": "part1/05. 스케일링.html#요약",
    "title": "5  스케일링",
    "section": "5.8 요약",
    "text": "5.8 요약\n이 장에서는 다양한 스케일링 기법의 원리와 적용 방법을 학습했다. 주요 내용은 다음과 같다.\n스케일링 방법 비교\n\n\n\n\n\n\n\n\n\n\n\n방법\n범위\n공식\n장점\n단점\n적용 상황\n\n\n\n\nMin-Max\n[0, 1] 또는 [a, b]\n\\((x - x_{min}) / (x_{max} - x_{min})\\)\n범위 고정, 해석 명확\n이상치에 매우 민감\n신경망, 이미지 처리, 시각화\n\n\nStandard\n평균=0, 표준편차=1\n\\((x - \\mu) / \\sigma\\)\n널리 사용, 정규분포 불필요\n고정 범위 없음\n대부분의 머신러닝 모델 (기본 선택)\n\n\nRobust\n중앙값=0 기준\n\\((x - \\text{median}) / \\text{IQR}\\)\n이상치에 강건\n해석이 복잡함\n이상치가 많은 실무 데이터 (실무 권장)\n\n\nMaxAbs\n[-1, 1]\n\\(x / |x|_{max}\\)\n부호 유지, 0 보존\n이상치 민감, 제한적 사용\n희소 행렬, 텍스트 데이터\n\n\n그룹별\n그룹 내 기준 적용\n그룹별로 위 방법 적용\n그룹 특성 반영\n구현 복잡도 증가\n그룹 간 분포 차이가 큰 경우\n\n\n\n스케일링 방법 선택 가이드\n\n기본 선택: 대부분의 경우 표준화(Standard Scaling) 사용\n이상치 존재: Robust 스케일링 고려 (실무에서 가장 안전)\n신경망 모델: Min-Max 스케일링으로 [0, 1] 범위 변환\n희소 데이터: MaxAbs 스케일링으로 0 값 보존\n그룹 차이: 그룹별 스케일링으로 그룹 특성 반영\n파이프라인: Pipeline으로 전처리와 모델 통합 관리\n\n주의사항\n\n학습 데이터로 스케일러를 학습(fit)하고, 동일한 파라미터로 테스트 데이터를 변환(transform)해야 함\n테스트 데이터로 스케일러를 재학습하면 데이터 누수 발생\n범주형 변수는 스케일링 대상에서 제외\n모델에 따라 스케일링이 불필요한 경우도 있음 (트리 기반 모델 등)\n\n스케일링은 머신러닝 모델의 성능과 학습 안정성을 크게 향상시키는 필수 전처리 과정이다. 데이터의 특성과 모델의 요구사항을 고려하여 적절한 방법을 선택하는 것이 중요하다. 다음 장에서는 범주형 변수를 수치형으로 변환하는 인코딩 기법을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>스케일링</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html",
    "href": "part1/06. 데이터 분포 변환.html",
    "title": "6  데이터 분포 변환",
    "section": "",
    "text": "6.1 분포 변환의 필요성\n데이터 분포 변환(Distribution Transformation)은 왜곡된 데이터 분포를 정규분포에 가깝게 만드는 과정이다. 많은 통계 모델과 머신러닝 알고리즘은 데이터가 정규분포를 따른다고 가정하므로, 왜곡된 분포는 모델 성능을 저하시킬 수 있다. 분포 변환을 통해 극단값의 영향을 완화하고 모델의 가정을 충족시킬 수 있다. 이 장에서는 로그 변환, 제곱근 변환, Box-Cox 변환, Yeo-Johnson 변환 등 다양한 분포 변환 기법을 학습한다.\n예제: 데이터 로드\n실제 데이터는 정규분포보다는 왜곡된 분포를 보이는 경우가 많다. 특히 양의 왜도(positive skewness)를 가진 데이터가 일반적이다. 왜도는 분포의 비대칭 정도를 나타내는 지표로, 0에 가까울수록 대칭적이고 절댓값이 클수록 왜곡이 심하다.\n예제: 원본 데이터의 왜도 확인\n# 각 변수의 왜도 계산\nskewness = df_num.skew()\n\nprint(\"변수별 왜도:\")\nprint(skewness)\nprint(\"\\n해석:\")\nprint(\"- 왜도 &gt; 0.5: 오른쪽으로 치우침 (양의 왜도)\")\nprint(\"- -0.5 &lt; 왜도 &lt; 0.5: 대칭에 가까움\")\nprint(\"- 왜도 &lt; -0.5: 왼쪽으로 치우침 (음의 왜도)\")\n\n변수별 왜도:\nbill_length_mm       0.053118\nbill_depth_mm       -0.143465\nflipper_length_mm    0.345682\nbody_mass_g          0.470329\ndtype: float64\n\n해석:\n- 왜도 &gt; 0.5: 오른쪽으로 치우침 (양의 왜도)\n- -0.5 &lt; 왜도 &lt; 0.5: 대칭에 가까움\n- 왜도 &lt; -0.5: 왼쪽으로 치우침 (음의 왜도)\n왜곡된 분포는 다음과 같은 문제를 일으킨다.\n분포 변환의 목표는 데이터를 대칭에 가깝게 만들어 극단값의 영향을 완화하고 모델의 가정을 충족시키는 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#분포-변환의-필요성",
    "href": "part1/06. 데이터 분포 변환.html#분포-변환의-필요성",
    "title": "6  데이터 분포 변환",
    "section": "",
    "text": "문제점\n설명\n\n\n\n\n모델 가정 위배\n선형 회귀, PCA 등은 정규성을 가정함\n\n\n극단값 영향\n평균과 분산이 소수 극단값에 의해 왜곡됨\n\n\n예측 성능 저하\n학습 데이터와 테스트 데이터의 분포 차이 발생\n\n\n해석 어려움\n비선형 관계로 인한 계수 해석의 복잡성 증가",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "href": "part1/06. 데이터 분포 변환.html#로그-변환-log-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.2 로그 변환 (Log Transform)",
    "text": "6.2 로그 변환 (Log Transform)\n로그 변환은 오른쪽 꼬리가 긴 분포(양의 왜도)를 가진 데이터에 가장 효과적인 방법이다. 지수적으로 증가하는 데이터를 선형에 가깝게 만들어준다.\n변환 공식\n\\[\nx' = \\log(x)\n\\]\n로그 변환은 큰 값의 차이를 압축하고 작은 값의 차이를 확대하여 분포를 대칭에 가깝게 만든다. 다만, 0 이하의 값은 변환할 수 없으므로 보정이 필요하다.\n예제: 기본 로그 변환\n\n# 로그 변환 (자연로그 사용)\ndf_log = df_num.copy()\n\nfor col in num_cols:\n    df_log[col] = np.log(df_log[col])\n\nprint(\"로그 변환 결과 (상위 5개):\")\nprint(df_log.head())\n\n로그 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0        3.666122       2.928524           5.198497     8.229511\n1        3.676301       2.856470           5.225747     8.242756\n2        3.696351       2.890372           5.273000     8.086410\n3             NaN            NaN                NaN          NaN\n4        3.602777       2.960105           5.262690     8.146130\n\n\n\n6.2.1 0 포함 변수의 안전한 처리\n데이터에 0이 포함된 경우 np.log(0)는 -inf를 반환하므로 문제가 발생한다. 이를 방지하기 위해 np.log1p(x)를 사용한다. 이 함수는 \\(\\log(1 + x)\\)를 계산하므로 0도 안전하게 처리할 수 있다.\n예제: log1p를 사용한 안전한 로그 변환\n\n# log1p: log(1 + x) 계산\ndf_log_safe = np.log1p(df_num)\n\nprint(\"log1p 변환 결과 (상위 5개):\")\nprint(df_log_safe.head())\n\nlog1p 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0        3.691376       2.980619           5.204007     8.229778\n1        3.701302       2.912351           5.231109     8.243019\n2        3.720862       2.944439           5.278115     8.086718\n3             NaN            NaN                NaN          NaN\n4        3.629660       3.010621           5.267858     8.146419\n\n\n\n\n6.2.2 왜도 변화 확인\n로그 변환 후 왜도가 어떻게 변했는지 확인한다.\n예제: 로그 변환 후 왜도 비교\n\n# 변환 전후 왜도 비교\nprint(\"원본 왜도:\")\nprint(df_num.skew())\nprint(\"\\n로그 변환 후 왜도:\")\nprint(df_log_safe.skew())\n\n원본 왜도:\nbill_length_mm       0.053118\nbill_depth_mm       -0.143465\nflipper_length_mm    0.345682\nbody_mass_g          0.470329\ndtype: float64\n\n로그 변환 후 왜도:\nbill_length_mm      -0.143463\nbill_depth_mm       -0.315791\nflipper_length_mm    0.251965\nbody_mass_g          0.170721\ndtype: float64\n\n\n일반적으로 로그 변환 후 왜도의 절댓값이 감소하여 분포가 더 대칭적이 된다.\n적용 상황\n\n왜도가 매우 큰 경우 (&gt; 1.0)\n극단값(이상치)이 많은 경우\n지수적 증가 패턴을 보이는 데이터\n금융 데이터, 인구 데이터, 소득 데이터 등\n“비율 변화”가 중요한 문제",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "href": "part1/06. 데이터 분포 변환.html#제곱근-변환-square-root-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.3 제곱근 변환 (Square Root Transform)",
    "text": "6.3 제곱근 변환 (Square Root Transform)\n제곱근 변환은 로그 변환보다 완만한 변환 방법으로, 왜도가 그리 심하지 않은 데이터에 적합하다. 포아송 분포를 따르는 카운트 데이터에 특히 효과적이다.\n변환 공식\n\\[\nx' = \\sqrt{x}\n\\]\n예제: 제곱근 변환\n\n# 제곱근 변환\ndf_sqrt = np.sqrt(df_num)\n\nprint(\"제곱근 변환 결과 (상위 5개):\")\nprint(df_sqrt.head())\n\n# 왜도 확인\nprint(\"\\n제곱근 변환 후 왜도:\")\nprint(df_sqrt.skew())\n\n제곱근 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0        6.252999       4.324350          13.453624    61.237244\n1        6.284903       4.171331          13.638182    61.644140\n2        6.348228       4.242641          13.964240    57.008771\n3             NaN            NaN                NaN          NaN\n4        6.058052       4.393177          13.892444    58.736701\n\n제곱근 변환 후 왜도:\nbill_length_mm      -0.048524\nbill_depth_mm       -0.235482\nflipper_length_mm    0.298802\nbody_mass_g          0.322080\ndtype: float64\n\n\n\n6.3.1 로그 변환과 제곱근 변환 비교\n두 변환 방법의 선택 기준은 다음과 같다.\n제곱근 변환이 적합한 경우\n\n왜도가 심하지 않음 (0.5 ~ 1.5)\n데이터 범위가 크지 않음\n분포를 약간만 조정하고 싶을 때\n카운트 데이터 (포아송 분포)\n변환 전후 해석이 직관적이어야 할 때\n\n로그 변환이 적합한 경우\n\n왜도가 매우 큼 (&gt; 1.5)\n극단값(이상치)이 많음\n데이터 범위가 매우 넓음 (여러 자릿수 차이)\n선형 회귀, PCA, 거리 기반 모델 사용\n“비율 변화”가 중요한 문제 (예: 수익률, 성장률)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#box-cox-변환",
    "href": "part1/06. 데이터 분포 변환.html#box-cox-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.4 Box-Cox 변환",
    "text": "6.4 Box-Cox 변환\nBox-Cox 변환은 로그 변환의 일반화된 형태로, 최적의 변환 파라미터 λ(람다)를 자동으로 추정한다. 다양한 변환을 포괄하는 강력한 방법이지만 양수 데이터에만 적용할 수 있다.\n변환 공식\n\\[\nx' =\n\\begin{cases}\n\\frac{x^\\lambda - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n\\log(x), & \\lambda = 0\n\\end{cases}\n\\]\n여기서 λ는 최대우도추정법(MLE)을 통해 자동으로 결정된다. λ 값에 따라 다음과 같은 변환이 수행된다.\n\n\n\nλ 값\n변환 방법\n\n\n\n\nλ = 1\n변환 없음\n\n\nλ = 0.5\n제곱근 변환\n\n\nλ = 0\n로그 변환\n\n\nλ = -1\n역수 변환\n\n\n\n예제: Box-Cox 변환\n\nfrom scipy.stats import boxcox\n\n# Box-Cox 변환\ndf_boxcox = df_num.copy()\nlambdas = {}\n\nfor col in num_cols:\n    # 결측치 제거\n    data = df_boxcox[col].dropna()\n    \n    # Box-Cox 변환 및 최적 람다 추정\n    transformed, lam = boxcox(data)\n    \n    # 변환 결과 저장\n    df_boxcox.loc[data.index, col] = transformed\n    lambdas[col] = lam\n\nprint(\"Box-Cox 변환 결과 (상위 5개):\")\nprint(df_boxcox.head())\n\nBox-Cox 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0       14.051916      50.248024           0.484671     2.101002\n1       14.151106      45.114142           0.484672     2.101288\n2       14.348349      47.461799           0.484673     2.097796\n3             NaN            NaN                NaN          NaN\n4       13.448467      52.676024           0.484672     2.099160\n\n\n예제: 추정된 λ 값 확인\n\n# 각 변수의 최적 람다 값\nprint(\"변수별 최적 λ 값:\")\nfor col, lam in lambdas.items():\n    print(f\"{col}: {lam:.4f}\")\n\n변수별 최적 λ 값:\nbill_length_mm: 0.6202\nbill_depth_mm: 1.4748\nflipper_length_mm: -2.0632\nbody_mass_g: -0.4657\n\n\nλ 값이 0에 가까우면 로그 변환에 가깝고, 0.5에 가까우면 제곱근 변환에 가깝다.\n적용 상황\n\n양수 데이터만 있는 경우\n최적의 변환을 자동으로 찾고 싶을 때\n통계적 가정(정규성)을 엄격히 만족해야 하는 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#yeo-johnson-변환",
    "href": "part1/06. 데이터 분포 변환.html#yeo-johnson-변환",
    "title": "6  데이터 분포 변환",
    "section": "6.5 Yeo-Johnson 변환",
    "text": "6.5 Yeo-Johnson 변환\nYeo-Johnson 변환은 Box-Cox 변환의 확장 버전으로, 0과 음수 값도 처리할 수 있다. 실무에서 가장 권장되는 방법이다.\n변환 공식\nYeo-Johnson 변환은 데이터의 부호에 따라 다른 공식을 적용한다.\n\n\\(x \\geq 0, \\lambda \\neq 0\\): \\(\\frac{(x+1)^\\lambda - 1}{\\lambda}\\)\n\\(x \\geq 0, \\lambda = 0\\): \\(\\log(x+1)\\)\n\\(x &lt; 0, \\lambda \\neq 2\\): \\(-\\frac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda}\\)\n\\(x &lt; 0, \\lambda = 2\\): \\(-\\log(-x+1)\\)\n\n예제: Yeo-Johnson 변환\n\nfrom sklearn.preprocessing import PowerTransformer\n\n# Yeo-Johnson 변환기 생성\npt = PowerTransformer(method=\"yeo-johnson\")\n\n# 변환 수행\ndf_yeojohnson = pd.DataFrame(\n    pt.fit_transform(df_num),\n    columns=num_cols\n)\n\nprint(\"Yeo-Johnson 변환 결과 (상위 5개):\")\nprint(df_yeojohnson.head())\n\n# 추정된 람다 값 확인\nprint(\"\\n변수별 최적 λ 값:\")\nfor i, col in enumerate(num_cols):\n    print(f\"{col}: {pt.lambdas_[i]:.4f}\")\n\nYeo-Johnson 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0       -0.878666       0.778009          -1.580741    -0.481138\n1       -0.802301       0.099099          -1.104687    -0.407311\n2       -0.650437       0.409486          -0.339984    -1.308479\n3             NaN            NaN                NaN          NaN\n4       -1.343166       1.099289          -0.500529    -0.956498\n\n변수별 최적 λ 값:\nbill_length_mm: 0.6153\nbill_depth_mm: 1.5132\nflipper_length_mm: -2.0775\nbody_mass_g: -0.4660\n\n\nYeo-Johnson 변환은 다음과 같은 장점이 있다.\n\n0과 음수 값을 자동으로 처리\n결측치를 제외하고 자동 변환\nsklearn 파이프라인과 통합 가능\n실무에서 가장 안전하고 범용적\n\n적용 상황\n\n0 또는 음수를 포함하는 데이터\n실무에서 기본적으로 사용할 변환 방법\n자동화된 전처리 파이프라인 구축",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "href": "part1/06. 데이터 분포 변환.html#분위수-변환-quantile-transform",
    "title": "6  데이터 분포 변환",
    "section": "6.6 분위수 변환 (Quantile Transform)",
    "text": "6.6 분위수 변환 (Quantile Transform)\n분위수 변환은 데이터의 분위수를 목표 분포의 분위수로 매핑하여 분포 자체를 강제로 변환하는 방법이다. 다른 변환 방법과 달리 순위 정보만 유지하고 분포 형태를 완전히 바꾼다.\n예제: 분위수 변환 (정규분포)\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n# 분위수 변환기 생성 (출력 분포: 정규분포)\nqt = QuantileTransformer(\n    output_distribution=\"normal\",\n    random_state=42\n)\n\n# 변환 수행\ndf_quantile = pd.DataFrame(\n    qt.fit_transform(df_num),\n    columns=num_cols\n)\n\nprint(\"분위수 변환 결과 (상위 5개):\")\nprint(df_quantile.head())\n\n# 왜도 확인\nprint(\"\\n분위수 변환 후 왜도:\")\nprint(df_quantile.skew())\n\n분위수 변환 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n0       -0.704794       0.690633          -1.663512    -0.366106\n1       -0.649469       0.033085          -1.127632    -0.269985\n2       -0.506872       0.288983          -0.176300    -1.347428\n3             NaN            NaN                NaN          NaN\n4       -1.237926       1.107167          -0.335021    -0.890537\n\n분위수 변환 후 왜도:\nbill_length_mm       0.000202\nbill_depth_mm       -0.010882\nflipper_length_mm   -0.020028\nbody_mass_g          0.006647\ndtype: float64\n\n\n분위수 변환은 왜도가 거의 0에 가까워지며 완벽한 정규분포에 가까운 형태가 된다.\n장단점\n\n\n\n장점\n단점\n\n\n\n\n완벽한 정규분포 생성\n원본 데이터의 실제 거리 정보 손실\n\n\n이상치 영향 제거\n새로운 데이터 처리 시 범위 문제 발생 가능\n\n\n비정상 분포에도 효과적\n해석이 어려움\n\n\n\n적용 상황\n\n극심하게 왜곡된 비정상 분포\n트리 기반 모델이 아닌 알고리즘 사용\n이상치를 완전히 무시하고 싶을 때\n순위 정보만 중요한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "href": "part1/06. 데이터 분포 변환.html#변환-전후-왜도-비교",
    "title": "6  데이터 분포 변환",
    "section": "6.7 변환 전후 왜도 비교",
    "text": "6.7 변환 전후 왜도 비교\n다양한 변환 방법의 효과를 정량적으로 비교한다.\n예제: 변환 방법별 왜도 비교\n\n# 왜도 비교 테이블\nskew_compare = pd.DataFrame({\n    \"원본\": df_num.skew(),\n    \"로그\": df_log_safe.skew(),\n    \"제곱근\": df_sqrt.skew(),\n    \"Box-Cox\": df_boxcox.skew(),\n    \"Yeo-Johnson\": df_yeojohnson.skew(),\n    \"분위수\": df_quantile.skew()\n})\n\nprint(\"변환 방법별 왜도 비교:\")\nprint(skew_compare.round(3))\n\n변환 방법별 왜도 비교:\n                      원본     로그    제곱근  Box-Cox  Yeo-Johnson    분위수\nbill_length_mm     0.053 -0.143 -0.049   -0.024       -0.024  0.000\nbill_depth_mm     -0.143 -0.316 -0.235   -0.055       -0.053 -0.011\nflipper_length_mm  0.346  0.252  0.299    0.050        0.050 -0.020\nbody_mass_g        0.470  0.171  0.322    0.026        0.026  0.007\n\n\n일반적으로 분위수 변환이 왜도를 가장 효과적으로 0에 가깝게 만들지만, 원본 데이터의 정보 손실이 가장 크다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#분포-변환과-스케일링의-관계",
    "href": "part1/06. 데이터 분포 변환.html#분포-변환과-스케일링의-관계",
    "title": "6  데이터 분포 변환",
    "section": "6.8 분포 변환과 스케일링의 관계",
    "text": "6.8 분포 변환과 스케일링의 관계\n분포 변환과 스케일링은 서로 다른 목적을 가진 전처리 과정이다.\n분포 변환과 스케일링 비교\n\n\n\n구분\n분포 변환\n스케일링\n\n\n\n\n목적\n분포의 모양 변경 (대칭화)\n변수의 크기 조정\n\n\n효과\n왜도 감소, 정규성 개선\n변수 간 스케일 통일\n\n\n변환 예시\n로그, 제곱근, Box-Cox\nMin-Max, Standard, Robust\n\n\n적용 순서\n먼저 수행\n나중에 수행\n\n\n\n일반적인 전처리 순서\n\n결측치 처리: 결측값 제거 또는 대체\n이상치 처리: 이상치 탐지 및 제거/대체\n분포 변환: 왜곡된 분포를 대칭화 (본 장)\n스케일링: 변수 간 크기 통일 (이전 장)\n인코딩: 범주형 변수를 수치형으로 변환\n\n분포 변환을 먼저 수행하는 이유는 왜곡된 분포에서 스케일링을 하면 극단값의 영향을 충분히 제거하지 못하기 때문이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/06. 데이터 분포 변환.html#요약",
    "href": "part1/06. 데이터 분포 변환.html#요약",
    "title": "6  데이터 분포 변환",
    "section": "6.9 요약",
    "text": "6.9 요약\n이 장에서는 왜곡된 데이터 분포를 정규분포에 가깝게 만드는 다양한 변환 기법을 학습했다. 주요 내용은 다음과 같다.\n분포 변환 방법 비교\n\n\n\n\n\n\n\n\n\n\n방법\n공식\n장점\n단점\n적용 상황\n\n\n\n\n로그\n\\(\\log(x)\\)\n간단, 해석 용이\n0 이하 불가\n왜도가 큰 양수 데이터, 지수적 증가\n\n\n제곱근\n\\(\\sqrt{x}\\)\n완만한 변환, 직관적\n효과 제한적\n약한 왜도, 카운트 데이터\n\n\nBox-Cox\n\\(\\frac{x^\\lambda - 1}{\\lambda}\\)\n최적 λ 자동 추정\n양수 전용\n양수 데이터, 최적 변환 필요\n\n\nYeo-Johnson\nBox-Cox 확장\n0, 음수 처리 가능\n해석 복잡\n실무 기본 권장 (범용적)\n\n\n분위수\n순위 기반 매핑\n완벽한 정규분포 생성\n정보 손실\n극심한 왜곡, 비정상 분포\n\n\n\n분포 변환 선택 가이드\n\n기본 선택: Yeo-Johnson 변환 (0, 음수 처리 가능, 실무 권장)\n단순한 경우: 로그 변환 (양수만 있고 왜도가 큰 경우)\n약한 왜곡: 제곱근 변환 (카운트 데이터, 포아송 분포)\n최적화 필요: Box-Cox 변환 (양수 데이터, 통계적 엄격성)\n극심한 왜곡: 분위수 변환 (비정상 분포, 순위만 중요)\n\n실무 전처리 순서\n분포 변환 → 스케일링 순서로 진행한다.\n\n분포 변환: 데이터 분포의 모양을 대칭에 가깝게 변경\n스케일링: 변수 간 크기를 통일\n\n이 순서를 지키면 극단값의 영향을 효과적으로 완화하고 모델의 성능을 향상시킬 수 있다.\n분포 변환은 모델의 가정을 충족시키고 예측 성능을 향상시키는 중요한 전처리 과정이다. 데이터의 분포 특성과 모델의 요구사항을 고려하여 적절한 변환 방법을 선택하는 것이 중요하다. 다음 장에서는 범주형 변수를 수치형으로 변환하는 인코딩 기법을 학습할 것이다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>데이터 분포 변환</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html",
    "href": "part1/07. 범주형 데이터 인코딩.html",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "7.1 Label Encoding\n범주형 데이터 인코딩(Categorical Encoding)은 범주형 변수를 머신러닝 모델이 처리할 수 있는 수치형으로 변환하는 과정이다. 대부분의 머신러닝 알고리즘은 수치형 데이터만 입력으로 받을 수 있으므로, 범주형 변수를 적절히 인코딩하는 것은 필수적이다. 인코딩 방법의 선택은 데이터의 특성과 사용할 모델에 따라 달라지며, 잘못된 선택은 모델 성능을 크게 저하시킬 수 있다. 이 장에서는 Label Encoding, One-Hot Encoding, Ordinal Encoding, Target Encoding, Frequency Encoding 등 주요 인코딩 기법을 학습한다.\n예제: 데이터 로드\nLabel Encoding은 각 범주를 정수 값으로 매핑하는 가장 단순한 인코딩 방법이다. 예를 들어, “Adelie”, “Chinstrap”, “Gentoo”를 0, 1, 2로 변환한다. 구현이 간단하고 컬럼 수가 늘어나지 않는 장점이 있지만, 순서가 없는 범주에 순서를 부여하는 문제가 있다.\n변환 예시",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#label-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "",
    "text": "원본\n인코딩 결과\n\n\n\n\nAdelie\n0\n\n\nChinstrap\n1\n\n\nGentoo\n2\n\n\n\n\n7.1.1 예제 코드\n예제: Label Encoding 적용\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Label Encoder 생성\nle = LabelEncoder()\n\n# 데이터 복사\ndf_label = df.copy()\n\n# species 컬럼 인코딩\ndf_label[\"species_enc\"] = le.fit_transform(df_label[\"species\"])\n\n# 결과 확인\nprint(\"Label Encoding 결과:\")\nprint(df_label[[\"species\", \"species_enc\"]].head(10))\n\n# 매핑 관계 확인\nprint(\"\\n매핑 관계:\")\nfor i, label in enumerate(le.classes_):\n    print(f\"{label} → {i}\")\n\nLabel Encoding 결과:\n  species  species_enc\n0  Adelie            0\n1  Adelie            0\n2  Adelie            0\n3  Adelie            0\n4  Adelie            0\n5  Adelie            0\n6  Adelie            0\n7  Adelie            0\n8  Adelie            0\n9  Adelie            0\n\n매핑 관계:\nAdelie → 0\nChinstrap → 1\nGentoo → 2\n\n\n주의사항\nLabel Encoding은 범주 간에 순서 관계가 없음에도 불구하고 숫자로 변환하므로, 모델이 숫자의 크기를 의미 있는 것으로 해석할 수 있다. 예를 들어, 모델이 “Gentoo(2)”가 “Adelie(0)”보다 2배 크다고 잘못 학습할 수 있다.\n사용 시점\n\n트리 기반 모델(Decision Tree, Random Forest, XGBoost, LightGBM) 사용 시\n컬럼 수를 늘리고 싶지 않을 때\n범주의 개수가 매우 많을 때 (고차원 문제 완화)\n\n트리 기반 모델은 범주를 분할 기준으로만 사용하므로 숫자의 크기를 의미 있게 해석하지 않는다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#one-hot-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.2 One-Hot Encoding",
    "text": "7.2 One-Hot Encoding\nOne-Hot Encoding은 각 범주를 이진 컬럼으로 분리하는 방법이다. k개의 범주가 있으면 k개의 새로운 컬럼이 생성되며, 각 행은 해당 범주에 해당하는 컬럼에만 1을, 나머지에는 0을 갖는다. 이 방법은 범주 간 순서 관계를 만들지 않으므로 선형 모델과 거리 기반 모델에 가장 안전하다.\n변환 예시\n\n\n\n원본\nspecies_Adelie\nspecies_Chinstrap\nspecies_Gentoo\n\n\n\n\nAdelie\n1\n0\n0\n\n\nChinstrap\n0\n1\n0\n\n\nGentoo\n0\n0\n1\n\n\n\n\n7.2.1 pandas 방식\npandas의 get_dummies 함수를 사용하면 간단하게 One-Hot Encoding을 수행할 수 있다.\n예제: One-Hot Encoding 적용\n\n# One-Hot Encoding (모든 범주 유지)\ndf_ohe = pd.get_dummies(\n    df,\n    columns=[\"species\", \"island\"],\n    drop_first=False\n)\n\nprint(\"One-Hot Encoding 결과 (컬럼 목록):\")\nprint(df_ohe.columns.tolist())\nprint(f\"\\n원본 컬럼 수: {len(df.columns)}\")\nprint(f\"인코딩 후 컬럼 수: {len(df_ohe.columns)}\")\n\n# 결과 샘플 확인\nprint(\"\\n인코딩 결과 (상위 5개):\")\nprint(df_ohe.head())\n\nOne-Hot Encoding 결과 (컬럼 목록):\n['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'species_Adelie', 'species_Chinstrap', 'species_Gentoo', 'island_Biscoe', 'island_Dream', 'island_Torgersen']\n\n원본 컬럼 수: 7\n인코딩 후 컬럼 수: 11\n\n인코딩 결과 (상위 5개):\n   bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  \\\n0            39.1           18.7              181.0       3750.0    Male   \n1            39.5           17.4              186.0       3800.0  Female   \n2            40.3           18.0              195.0       3250.0  Female   \n3             NaN            NaN                NaN          NaN     NaN   \n4            36.7           19.3              193.0       3450.0  Female   \n\n   species_Adelie  species_Chinstrap  species_Gentoo  island_Biscoe  \\\n0            True              False           False          False   \n1            True              False           False          False   \n2            True              False           False          False   \n3            True              False           False          False   \n4            True              False           False          False   \n\n   island_Dream  island_Torgersen  \n0         False              True  \n1         False              True  \n2         False              True  \n3         False              True  \n4         False              True  \n\n\n\n\n7.2.2 drop_first 옵션\ndrop_first=True로 설정하면 첫 번째 더미 변수를 제거하여 다중공선성(multicollinearity)을 방지할 수 있다. 예를 들어, 3개의 종이 있으면 2개의 컬럼만으로도 모든 정보를 표현할 수 있다.\n예제: drop_first 옵션 사용\n\n# 첫 번째 더미 변수 제거\ndf_ohe_drop = pd.get_dummies(\n    df,\n    columns=[\"species\"],\n    drop_first=True\n)\n\nprint(\"drop_first=True 결과:\")\nprint(df_ohe_drop.filter(like='species').columns.tolist())\n\ndrop_first=True 결과:\n['species_Chinstrap', 'species_Gentoo']\n\n\n다중공선성은 독립 변수들 간에 높은 상관관계가 존재하는 상태로, 선형 회귀에서 계수 추정을 불안정하게 만든다. k개의 범주를 모두 포함하면 완벽한 선형 종속 관계가 생기므로, 하나를 제거하여 이를 방지한다.\n사용 시점\n\n선형 회귀, 로지스틱 회귀 모델 사용 시\n거리 기반 모델(KNN, SVM) 사용 시\nPCA 등 선형 차원 축소 기법 사용 시\n범주의 개수가 적을 때 (일반적으로 10개 이하)\n\n장단점\n\n\n\n\n\n\n\n장점\n단점\n\n\n\n\n범주 간 순서 관계를 만들지 않음\n범주가 많으면 컬럼 수가 급격히 증가 (차원의 저주)\n\n\n선형 모델과 거리 기반 모델에 적합\n메모리 사용량 증가\n\n\n해석이 명확함\n희소 행렬(sparse matrix) 생성",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#ordinal-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.3 Ordinal Encoding",
    "text": "7.3 Ordinal Encoding\nOrdinal Encoding은 순서가 있는 범주형 변수에 사용하는 방법이다. 사용자가 명시적으로 순서를 정의하여 각 범주에 정수를 할당한다. 예를 들어, “small” &lt; “medium” &lt; “large”와 같은 순서가 있는 경우에 적합하다.\n변환 예시\n\n\n\n원본\n인코딩 결과\n\n\n\n\nsmall\n0\n\n\nmedium\n1\n\n\nlarge\n2\n\n\n\n\n7.3.1 예제 코드\n예제: Ordinal Encoding 적용\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# 순서 정의 (작은 것부터 큰 것 순서)\noe = OrdinalEncoder(\n    categories=[[\"small\", \"medium\", \"large\"]]\n)\n\n# 예제 데이터 생성\nsize_data = pd.DataFrame({\n    \"size\": [\"small\", \"large\", \"medium\", \"small\", \"large\"]\n})\n\n# 인코딩 수행\nsize_encoded = oe.fit_transform(size_data)\n\n# 결과 확인\nresult = pd.DataFrame({\n    \"원본\": size_data[\"size\"],\n    \"인코딩\": size_encoded.flatten()\n})\n\nprint(\"Ordinal Encoding 결과:\")\nprint(result)\n\nOrdinal Encoding 결과:\n       원본  인코딩\n0   small  0.0\n1   large  2.0\n2  medium  1.0\n3   small  0.0\n4   large  2.0\n\n\n사용 시점\n\n명확한 순서가 있는 범주형 변수 (예: 학력, 등급, 크기, 만족도)\n순서 정보가 모델 학습에 중요한 경우\n선형 관계를 가정할 수 있는 경우\n\n주의사항\n순서를 잘못 정의하면 모델 성능이 저하될 수 있으므로, 도메인 지식을 바탕으로 신중하게 순서를 결정해야 한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#target-encoding-mean-encoding",
    "href": "part1/07. 범주형 데이터 인코딩.html#target-encoding-mean-encoding",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.4 Target Encoding (Mean Encoding)",
    "text": "7.4 Target Encoding (Mean Encoding)\nTarget Encoding은 각 범주를 타겟 변수의 평균값으로 치환하는 방법이다. 범주와 타겟 간의 관계를 직접적으로 반영하므로 정보량이 크지만, 데이터 누수(data leakage) 위험이 있어 주의가 필요하다.\n변환 예시\n\n\n\nspecies\nbody_mass_g 평균\n\n\n\n\nAdelie\n3700.66\n\n\nChinstrap\n3733.09\n\n\nGentoo\n5076.02\n\n\n\n\n7.4.1 예제 코드\n예제: Target Encoding 적용\n\n# 데이터 복사\ndf_te = df.copy()\n\n# 타겟 변수 및 인코딩할 컬럼 지정\ntarget = \"body_mass_g\"\ncol = \"species\"\n\n# 범주별 타겟 평균 계산\nmean_map = df_te.groupby(col)[target].mean()\n\nprint(\"범주별 타겟 평균:\")\nprint(mean_map)\n\n# 범주를 평균값으로 치환\ndf_te[\"species_enc\"] = df_te[col].map(mean_map)\n\n# 결과 확인\nprint(\"\\nTarget Encoding 결과:\")\nprint(df_te[[col, target, \"species_enc\"]].head(10))\n\n범주별 타겟 평균:\nspecies\nAdelie       3700.662252\nChinstrap    3733.088235\nGentoo       5076.016260\nName: body_mass_g, dtype: float64\n\nTarget Encoding 결과:\n  species  body_mass_g  species_enc\n0  Adelie       3750.0  3700.662252\n1  Adelie       3800.0  3700.662252\n2  Adelie       3250.0  3700.662252\n3  Adelie          NaN  3700.662252\n4  Adelie       3450.0  3700.662252\n5  Adelie       3650.0  3700.662252\n6  Adelie       3625.0  3700.662252\n7  Adelie       4675.0  3700.662252\n8  Adelie       3475.0  3700.662252\n9  Adelie       4250.0  3700.662252\n\n\n데이터 누수 방지\nTarget Encoding을 사용할 때는 반드시 다음 규칙을 지켜야 한다.\n\n학습 데이터만 사용: 평균 계산 시 학습 데이터만 사용하고, 테스트 데이터는 절대 포함하지 않음\n교차 검증: 학습 데이터 내에서도 fold별로 다른 fold의 평균을 사용 (out-of-fold encoding)\n스무딩(Smoothing): 샘플 수가 적은 범주는 전체 평균에 가깝게 조정\n\n예제: 안전한 Target Encoding (학습/테스트 분리)\n\nfrom sklearn.model_selection import train_test_split\n\n# 학습/테스트 분리\ndf_clean = df.dropna(subset=[target, col])\ntrain_df, test_df = train_test_split(df_clean, test_size=0.2, random_state=42)\n\n# 학습 데이터로만 평균 계산\ntrain_mean_map = train_df.groupby(col)[target].mean()\n\n# 학습/테스트 데이터에 적용\ntrain_encoded = train_df[col].map(train_mean_map)\ntest_encoded = test_df[col].map(train_mean_map)\n\nprint(\"학습 데이터 인코딩 결과:\")\nprint(train_encoded.head())\n\n학습 데이터 인코딩 결과:\n115    3689.224138\n8      3689.224138\n138    3689.224138\n333    5071.039604\n305    5071.039604\nName: species, dtype: float64\n\n\n사용 시점\n\n고급 트리 기반 모델(XGBoost, LightGBM, CatBoost) 사용 시\n범주와 타겟 간 강한 관계가 있을 때\n범주의 개수가 매우 많아 One-Hot Encoding이 비효율적일 때\n\n장단점\n\n\n\n\n\n\n\n장점\n단점\n\n\n\n\n타겟과의 관계를 직접 반영하여 정보량이 큼\n데이터 누수 위험 (올바른 방법 필수)\n\n\n고차원 범주형 변수에 효과적\n과적합 위험 (스무딩 필요)\n\n\n컬럼 수 증가 없음\n구현이 복잡함",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#frequency-encoding-빈도-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#frequency-encoding-빈도-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.5 Frequency Encoding (빈도 인코딩)",
    "text": "7.5 Frequency Encoding (빈도 인코딩)\nFrequency Encoding은 각 범주를 해당 범주의 출현 빈도로 치환하는 방법이다. 희귀한 범주와 자주 나타나는 범주를 구분할 수 있어, 희귀 범주가 많은 경우 유용하다.\n변환 예시\n\n\n\nisland\n빈도 (비율)\n\n\n\n\nBiscoe\n0.488\n\n\nDream\n0.360\n\n\nTorgersen\n0.151\n\n\n\n\n7.5.1 예제 코드\n예제: Frequency Encoding 적용\n\n# 범주별 빈도 계산 (비율로 정규화)\nfreq_map = df[\"island\"].value_counts(normalize=True)\n\nprint(\"범주별 빈도:\")\nprint(freq_map)\n\n# 빈도로 치환\ndf_freq = df.copy()\ndf_freq[\"island_enc\"] = df_freq[\"island\"].map(freq_map)\n\n# 결과 확인\nprint(\"\\nFrequency Encoding 결과:\")\nprint(df_freq[[\"island\", \"island_enc\"]].head(10))\n\n범주별 빈도:\nisland\nBiscoe       0.488372\nDream        0.360465\nTorgersen    0.151163\nName: proportion, dtype: float64\n\nFrequency Encoding 결과:\n      island  island_enc\n0  Torgersen    0.151163\n1  Torgersen    0.151163\n2  Torgersen    0.151163\n3  Torgersen    0.151163\n4  Torgersen    0.151163\n5  Torgersen    0.151163\n6  Torgersen    0.151163\n7  Torgersen    0.151163\n8  Torgersen    0.151163\n9  Torgersen    0.151163\n\n\n사용 시점\n\n희귀 범주가 많을 때\n범주의 빈도가 타겟과 관련이 있을 때\n고차원 범주형 변수를 간단히 처리하고 싶을 때\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n구현이 간단함\n서로 다른 범주가 같은 빈도를 가질 수 있음\n\n\n희귀 범주를 효과적으로 표현\n빈도가 타겟과 관련 없으면 효과 제한적\n\n\n컬럼 수 증가 없음\n정보 손실 가능",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "href": "part1/07. 범주형 데이터 인코딩.html#여러-컬럼을-한-번에-인코딩",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.6 여러 컬럼을 한 번에 인코딩",
    "text": "7.6 여러 컬럼을 한 번에 인코딩\n실무에서는 여러 범주형 컬럼을 동시에 처리해야 하는 경우가 많다. 각 인코딩 방법을 여러 컬럼에 적용하는 방법을 살펴본다.\n\n7.6.1 One-Hot Encoding (여러 컬럼)\n예제: 여러 컬럼 One-Hot Encoding\n\n# 인코딩할 범주형 컬럼 목록\ncat_cols = [\"species\", \"island\", \"sex\"]\n\n# 여러 컬럼에 One-Hot Encoding 적용\ndf_ohe_multi = pd.get_dummies(\n    df, \n    columns=cat_cols,\n    drop_first=True  # 다중공선성 방지\n)\n\nprint(\"인코딩된 컬럼 목록:\")\nprint([col for col in df_ohe_multi.columns if any(cat in col for cat in cat_cols)])\nprint(f\"\\n원본 컬럼 수: {len(df.columns)}\")\nprint(f\"인코딩 후 컬럼 수: {len(df_ohe_multi.columns)}\")\n\n인코딩된 컬럼 목록:\n['species_Chinstrap', 'species_Gentoo', 'island_Dream', 'island_Torgersen', 'sex_Male']\n\n원본 컬럼 수: 7\n인코딩 후 컬럼 수: 9\n\n\n\n\n7.6.2 Label Encoding (여러 컬럼)\n예제: 여러 컬럼 Label Encoding\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# 데이터 복사\ndf_label_multi = df.copy()\n\n# 범주형 컬럼 목록\ncat_cols = [\"species\", \"island\", \"sex\"]\n\n# 각 컬럼에 Label Encoding 적용\nfor col in cat_cols:\n    le = LabelEncoder()\n    # 결측치를 문자열로 변환하여 처리\n    df_label_multi[col] = le.fit_transform(df_label_multi[col].astype(str))\n\nprint(\"Label Encoding 결과:\")\nprint(df_label_multi[cat_cols].head())\n\nLabel Encoding 결과:\n   species  island  sex\n0        0       2    1\n1        0       2    0\n2        0       2    0\n3        0       2    2\n4        0       2    0\n\n\n\n\n7.6.3 sklearn ColumnTransformer 사용\n여러 인코딩 방법을 조합하여 사용하는 경우 ColumnTransformer를 사용하면 깔끔하게 처리할 수 있다.\n예제: ColumnTransformer로 여러 인코딩 조합\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# 인코딩 파이프라인 구성\nct = ColumnTransformer(\n    transformers=[\n        (\"ohe\", OneHotEncoder(drop='first', sparse_output=False), [\"species\", \"island\"]),\n        # LabelEncoder는 ColumnTransformer와 직접 사용 불가\n    ],\n    remainder='passthrough'  # 나머지 컬럼은 그대로 유지\n)\n\n# 수치형 컬럼 선택\nnum_cols = [\n    \"bill_length_mm\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\"\n]\n\ndf_transformed = ct.fit_transform(df[[\"species\", \"island\", \"sex\"] + num_cols].dropna())\n\nprint(\"ColumnTransformer 변환 결과 shape:\")\nprint(df_transformed.shape)\n\nColumnTransformer 변환 결과 shape:\n(333, 9)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/07. 범주형 데이터 인코딩.html#요약",
    "href": "part1/07. 범주형 데이터 인코딩.html#요약",
    "title": "7  범주형 데이터 인코딩",
    "section": "7.7 요약",
    "text": "7.7 요약\n이 장에서는 범주형 데이터를 수치형으로 변환하는 다양한 인코딩 기법을 학습했다. 주요 내용은 다음과 같다.\n인코딩 방법 비교\n\n\n\n\n\n\n\n\n\n\n\n인코딩\n순서 필요\n컬럼 수 증가\n장점\n단점\n추천 모델\n\n\n\n\nLabel\nX\nX\n간단, 효율적\n순서 관계 생성\n트리 기반 모델\n\n\nOne-Hot\nX\nO\n순서 없음, 안전\n고차원 문제\n선형, 거리 기반 모델\n\n\nOrdinal\nO\nX\n순서 정보 반영\n순서 정의 필요\n순서형 데이터\n\n\nTarget\nX\nX\n정보량 큼\n데이터 누수 위험\n고급 트리 모델 (주의 필요)\n\n\nFrequency\nX\nX\n희귀 범주 처리\n정보 손실 가능\n희귀 범주가 많을 때\n\n\n\n인코딩 방법 선택 가이드\n\n트리 기반 모델: Label Encoding (간단하고 효율적)\n선형/거리 기반 모델: One-Hot Encoding (가장 안전)\n순서가 있는 범주: Ordinal Encoding (순서 정보 활용)\n고차원 범주: Target Encoding 또는 Frequency Encoding (차원 증가 방지)\n희귀 범주 많음: Frequency Encoding (희귀도 반영)\n\n주의사항\n\n데이터 누수 방지: Target Encoding 사용 시 학습/테스트 분리 철저히 지킬 것\n다중공선성: One-Hot Encoding에서 선형 모델 사용 시 drop_first=True 권장\n결측치 처리: 인코딩 전 결측치를 별도 범주로 처리하거나 제거할 것\n새로운 범주: 테스트 데이터에 학습 시 없던 범주가 나타나면 처리 전략 필요\n\n실무 인코딩 순서\n\n범주형 변수 식별 및 순서 여부 확인\n모델 유형에 따른 인코딩 방법 선택\n학습/테스트 데이터 분리 (데이터 누수 방지)\n학습 데이터로 인코더 학습(fit)\n학습/테스트 데이터에 동일한 인코더 적용(transform)\n\n범주형 데이터 인코딩은 전처리의 마지막 단계로, 적절한 방법을 선택하면 모델 성능을 크게 향상시킬 수 있다. 데이터의 특성, 모델의 요구사항, 범주의 개수와 순서 관계를 종합적으로 고려하여 최적의 인코딩 방법을 선택하는 것이 중요하다. 이제 데이터 전처리의 전 과정을 마쳤으며, 다음 단계로 정제된 데이터를 활용한 모델링을 진행할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>범주형 데이터 인코딩</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html",
    "href": "part1/08. 연속형 데이터 범주화.html",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "8.1 범주화의 필요성과 효과\n연속형 데이터 범주화(Discretization 또는 Binning)는 연속형 변수를 구간별로 나누어 범주형 변수로 변환하는 과정이다. 이 과정은 정보 손실을 수반하지만, 모델의 해석력을 높이고 비선형 관계를 단순화하며 노이즈를 감소시키는 효과가 있다. 특히 도메인 전문가와의 커뮤니케이션이나 규칙 기반 의사결정 시스템에서 유용하다. 이 장에서는 등간격 구간화, 등빈도 구간화, 사용자 정의 구간화, k-means 기반 구간화, Decision Tree 기반 구간화 등 다양한 범주화 기법을 학습한다.\n예제: 데이터 로드\n연속형 데이터를 범주화하면 다음과 같은 효과를 얻을 수 있다.\n범주화의 주요 효과\n다만, 정보 손실이 불가피하므로 데이터가 충분히 많고 모델의 예측 성능보다 해석력이 중요한 경우에 사용하는 것이 적절하다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화의-필요성과-효과",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화의-필요성과-효과",
    "title": "8  연속형 데이터 범주화",
    "section": "",
    "text": "효과\n설명\n예시\n\n\n\n\n해석력 증가\n숫자 대신 의미 있는 구간명 사용\n“40.5mm” → “보통 길이”\n\n\n노이즈 감소\n작은 변동을 무시하고 큰 패턴에 집중\n측정 오차 완화\n\n\n비선형 관계 단순화\n복잡한 관계를 구간별 규칙으로 표현\n나이-위험도 관계\n\n\n이상치 영향 완화\n극단값을 구간 경계로 제한\n최상위 구간으로 포함\n\n\n규칙 추출 용이\n의사결정 규칙 명확화\n“A구간이면 승인”",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등간격-구간화-equal-width-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.2 등간격 구간화 (Equal-width Binning)",
    "text": "8.2 등간격 구간화 (Equal-width Binning)\n등간격 구간화는 전체 값의 범위를 동일한 간격으로 k개의 구간으로 나누는 방법이다. 구현이 간단하고 직관적이지만, 데이터의 분포를 고려하지 않으므로 일부 구간에 데이터가 몰리거나 거의 없을 수 있다.\n구간 너비 계산 공식\n\\[\n\\text{bin width} = \\frac{\\max(x) - \\min(x)}{k}\n\\]\n여기서 k는 구간의 개수이다.\n예제: 등간격 구간화\n\n# 4개의 등간격 구간으로 분할\ndf[\"bill_length_bin\"] = pd.cut(df[\"bill_length_mm\"], bins=4)\n\nprint(\"등간격 구간화 결과:\")\nprint(df[\"bill_length_bin\"].value_counts().sort_index())\nprint(\"\\n구간 정보:\")\nprint(df[\"bill_length_bin\"].unique())\n\n등간격 구간화 결과:\nbill_length_bin\n(32.072, 38.975]     79\n(38.975, 45.85]     124\n(45.85, 52.725]     129\n(52.725, 59.6]       10\nName: count, dtype: int64\n\n구간 정보:\n[(38.975, 45.85], NaN, (32.072, 38.975], (45.85, 52.725], (52.725, 59.6]]\nCategories (4, interval[float64, right]): [(32.072, 38.975] &lt; (38.975, 45.85] &lt; (45.85, 52.725] &lt; (52.725, 59.6]]\n\n\n\n8.2.1 범주 이름 지정\n숫자 구간 대신 의미 있는 이름을 부여하면 해석력이 더욱 향상된다.\n예제: 구간에 라벨 지정\n\n# 구간에 의미 있는 이름 부여\ndf[\"bill_length_bin\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=4,\n    labels=[\"짧음\", \"보통\", \"김\", \"매우 김\"]\n)\n\nprint(\"라벨이 지정된 구간화 결과:\")\nprint(df[\"bill_length_bin\"].value_counts())\n\n라벨이 지정된 구간화 결과:\nbill_length_bin\n김       129\n보통      124\n짧음       79\n매우 김     10\nName: count, dtype: int64\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n구현이 간단하고 직관적\n이상치에 민감 (범위가 크게 늘어남)\n\n\n구간 너비가 일정하여 해석 용이\n데이터 밀도 불균형 발생 가능\n\n\n계산 속도가 빠름\n데이터 분포를 고려하지 않음\n\n\n\n적용 상황\n\n데이터가 비교적 균등하게 분포된 경우\n구간 너비가 도메인상 의미가 있는 경우 (예: 10년 단위 연령 구간)\n빠른 프로토타이핑이 필요한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-binning-quantile-binning",
    "href": "part1/08. 연속형 데이터 범주화.html#등빈도-구간화-equal-frequency-binning-quantile-binning",
    "title": "8  연속형 데이터 범주화",
    "section": "8.3 등빈도 구간화 (Equal-frequency Binning, Quantile Binning)",
    "text": "8.3 등빈도 구간화 (Equal-frequency Binning, Quantile Binning)\n등빈도 구간화는 각 구간에 동일한 개수의 데이터가 포함되도록 분할하는 방법이다. 데이터의 분포를 고려하여 구간을 나누므로, 등간격 구간화의 불균형 문제를 해결할 수 있다.\n예제: 등빈도 구간화\n\n# 4개의 등빈도 구간으로 분할 (사분위수)\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4\n)\n\nprint(\"등빈도 구간화 결과:\")\nprint(df[\"bill_length_qbin\"].value_counts().sort_index())\nprint(\"\\n각 구간의 데이터 개수:\")\nfor bin_range in df[\"bill_length_qbin\"].unique():\n    count = (df[\"bill_length_qbin\"] == bin_range).sum()\n    print(f\"{bin_range}: {count}개\")\n\n등빈도 구간화 결과:\nbill_length_qbin\n(32.099000000000004, 39.225]    86\n(39.225, 44.45]                 85\n(44.45, 48.5]                   87\n(48.5, 59.6]                    84\nName: count, dtype: int64\n\n각 구간의 데이터 개수:\n(32.099000000000004, 39.225]: 86개\n(39.225, 44.45]: 85개\nnan: 0개\n(44.45, 48.5]: 87개\n(48.5, 59.6]: 84개\n\n\n\n8.3.1 라벨 지정\n사분위수 구간에 Q1, Q2, Q3, Q4와 같은 표준 라벨을 부여할 수 있다.\n예제: 사분위수 라벨 지정\n\n# 사분위수 라벨 지정\ndf[\"bill_length_qbin\"] = pd.qcut(\n    df[\"bill_length_mm\"],\n    q=4,\n    labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n)\n\nprint(\"사분위수 구간 분포:\")\nprint(df[\"bill_length_qbin\"].value_counts())\n\n사분위수 구간 분포:\nbill_length_qbin\nQ3    87\nQ1    86\nQ2    85\nQ4    84\nName: count, dtype: int64\n\n\n장단점\n\n\n\n\n\n\n\n장점\n단점\n\n\n\n\n데이터 불균형 문제 해결\n동일한 값이 많으면 에러 발생 가능\n\n\n분포를 자동으로 반영\n구간 너비가 일정하지 않아 해석이 복잡할 수 있음\n\n\n이상치의 영향이 적음\n도메인 지식과 무관하게 분할됨\n\n\n\n적용 상황\n\n데이터가 편향되어 있는 경우\n각 구간에 충분한 샘플이 필요한 경우\n순위 기반 분석이 중요한 경우\n\n주의사항\n동일한 값이 많이 나타나면 qcut이 정확히 같은 개수로 나누지 못해 오류가 발생할 수 있다. 이 경우 duplicates='drop' 옵션을 사용하거나 구간 개수를 줄인다.\n\n# 중복값이 많을 때 처리\ntry:\n    df[\"bill_length_qbin_safe\"] = pd.qcut(\n        df[\"bill_length_mm\"],\n        q=10,\n        duplicates='drop'\n    )\nexcept Exception as e:\n    print(f\"오류 발생: {e}\")",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#사용자-정의-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.4 사용자 정의 구간화",
    "text": "8.4 사용자 정의 구간화\n사용자 정의 구간화는 도메인 지식이나 비즈니스 규칙에 따라 직접 구간 경계를 설정하는 방법이다. 가장 해석력이 높지만, 주관이 개입되므로 기준에 대한 명확한 설명이 필요하다.\n예제: 도메인 지식 기반 구간화\n\n# 도메인 전문가가 정의한 구간 경계\nbins = [30, 40, 45, 50, 60]\nlabels = [\"매우 짧음\", \"짧음\", \"보통\", \"김\"]\n\ndf[\"bill_length_custom\"] = pd.cut(\n    df[\"bill_length_mm\"],\n    bins=bins,\n    labels=labels\n)\n\nprint(\"사용자 정의 구간화 결과:\")\nprint(df[\"bill_length_custom\"].value_counts())\n\n# 구간 통계\nprint(\"\\n구간별 평균 체중:\")\nprint(df.groupby(\"bill_length_custom\")[\"body_mass_g\"].mean())\n\n사용자 정의 구간화 결과:\nbill_length_custom\n보통       113\n매우 짧음    100\n짧음        77\n김         52\nName: count, dtype: int64\n\n구간별 평균 체중:\nbill_length_custom\n매우 짧음    3558.000000\n짧음       4114.935065\n보통       4657.079646\n김        4578.846154\nName: body_mass_g, dtype: float64\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n해석력이 가장 높음\n주관이 개입될 수 있음\n\n\n도메인 지식을 직접 반영\n데이터 분포를 무시할 수 있음\n\n\n비즈니스 규칙과 일치\n구간 설정 근거 설명 필요\n\n\n\n적용 상황\n\n명확한 도메인 기준이 있는 경우 (예: 의학적 기준, 법적 기준)\n비즈니스 규칙이 정해진 경우 (예: 신용등급, 고객 등급)\n기존 시스템과의 호환성이 필요한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#k-means-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.5 k-means 기반 구간화",
    "text": "8.5 k-means 기반 구간화\nk-means 기반 구간화는 값의 분포를 고려하여 군집 중심을 기준으로 구간을 나누는 방법이다. 비선형적인 데이터 구조를 반영할 수 있어 복잡한 분포에서도 효과적이다.\n예제: k-means 기반 구간화\n\nfrom sklearn.cluster import KMeans\n\n# 결측치 제거\nx = df[[\"bill_length_mm\"]].dropna()\n\n# k-means 군집화 (3개 구간)\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(x)\n\n# 원본 데이터프레임에 결과 저장\ndf.loc[x.index, \"bill_length_kbin\"] = clusters\n\nprint(\"k-means 기반 구간화 결과:\")\nprint(df[\"bill_length_kbin\"].value_counts().sort_index())\n\n# 각 군집의 중심값\nprint(\"\\n군집 중심값:\")\nfor i, center in enumerate(kmeans.cluster_centers_):\n    print(f\"군집 {i}: {center[0]:.2f}mm\")\n\n# 군집별 통계\nprint(\"\\n군집별 범위:\")\nfor i in range(3):\n    cluster_data = x[clusters == i]\n    print(f\"군집 {i}: {cluster_data['bill_length_mm'].min():.2f} ~ {cluster_data['bill_length_mm'].max():.2f}mm\")\n\nk-means 기반 구간화 결과:\nbill_length_kbin\n0.0    114\n1.0     94\n2.0    134\nName: count, dtype: int64\n\n군집 중심값:\n군집 0: 45.09mm\n군집 1: 50.66mm\n군집 2: 38.20mm\n\n군집별 범위:\n군집 0: 41.70 ~ 47.80mm\n군집 1: 48.10 ~ 59.60mm\n군집 2: 32.10 ~ 41.60mm\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n데이터 분포를 자동으로 반영\n해석이 상대적으로 어려움\n\n\n비선형 구조를 포착 가능\n랜덤 초기화로 인한 변동성\n\n\n이상치 영향 완화\n구간 개수 결정 필요\n\n\n\n적용 상황\n\n데이터의 자연스러운 군집이 예상되는 경우\n분포가 복잡하거나 다봉형(multi-modal)인 경우\n데이터 기반 자동화가 필요한 경우",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "href": "part1/08. 연속형 데이터 범주화.html#decision-tree-기반-구간화",
    "title": "8  연속형 데이터 범주화",
    "section": "8.6 Decision Tree 기반 구간화",
    "text": "8.6 Decision Tree 기반 구간화\nDecision Tree 기반 구간화는 타겟 변수를 기준으로 정보이득을 최대화하는 구간을 찾는 지도학습 방법이다. 타겟과의 관계를 직접 반영하므로 예측 모델에 효과적이지만, 데이터 누수에 주의해야 한다.\n예제: 타겟 기반 구간화\n\nfrom sklearn.tree import DecisionTreeRegressor\n\n# 결측치 제거\nx = df[[\"bill_length_mm\"]].dropna()\ny = df.loc[x.index, \"body_mass_g\"]\n\n# Decision Tree로 최적 구간 찾기\ntree = DecisionTreeRegressor(\n    max_leaf_nodes=4,  # 4개 구간\n    random_state=42\n)\ntree.fit(x, y)\n\n# 리프 노드 ID를 구간으로 사용\ndf.loc[x.index, \"bill_length_treebin\"] = tree.apply(x)\n\nprint(\"Decision Tree 기반 구간화 결과:\")\nprint(df[\"bill_length_treebin\"].value_counts().sort_index())\n\n# 각 구간의 타겟 평균\nprint(\"\\n구간별 평균 체중:\")\nprint(df.groupby(\"bill_length_treebin\")[\"body_mass_g\"].mean().sort_index())\n\n# 구간 경계 확인 (트리 분할 지점)\nprint(\"\\n트리 분할 정보:\")\nprint(f\"사용된 특성: {tree.feature_importances_}\")\n\nDecision Tree 기반 구간화 결과:\nbill_length_treebin\n3.0     82\n4.0     65\n5.0     88\n6.0    107\nName: count, dtype: int64\n\n구간별 평균 체중:\nbill_length_treebin\n3.0    3480.792683\n4.0    3901.923077\n5.0    4406.534091\n6.0    4767.990654\nName: body_mass_g, dtype: float64\n\n트리 분할 정보:\n사용된 특성: [1.]\n\n\n데이터 누수 주의\nDecision Tree 기반 구간화는 타겟 정보를 사용하므로, 반드시 학습 데이터로만 구간을 결정하고 테스트 데이터에 적용해야 한다.\n예제: 데이터 누수 방지\n\nfrom sklearn.model_selection import train_test_split\n\n# 학습/테스트 분리\ndf_clean = df[[\"bill_length_mm\", \"body_mass_g\"]].dropna()\nX_train, X_test, y_train, y_test = train_test_split(\n    df_clean[[\"bill_length_mm\"]],\n    df_clean[\"body_mass_g\"],\n    test_size=0.2,\n    random_state=42\n)\n\n# 학습 데이터로만 트리 학습\ntree_safe = DecisionTreeRegressor(max_leaf_nodes=4, random_state=42)\ntree_safe.fit(X_train, y_train)\n\n# 학습/테스트 데이터에 동일한 구간 적용\ntrain_bins = tree_safe.apply(X_train)\ntest_bins = tree_safe.apply(X_test)\n\nprint(\"학습 데이터 구간 분포:\")\nprint(pd.Series(train_bins).value_counts().sort_index())\nprint(\"\\n테스트 데이터 구간 분포:\")\nprint(pd.Series(test_bins).value_counts().sort_index())\n\n학습 데이터 구간 분포:\n3    66\n4    50\n5    68\n6    89\nName: count, dtype: int64\n\n테스트 데이터 구간 분포:\n3    16\n4    18\n5    17\n6    18\nName: count, dtype: int64\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n타겟 정보를 직접 반영\n데이터 누수 위험 (주의 필요)\n\n\n예측 성능 향상 가능\n과적합 위험\n\n\n비선형 관계 포착\n해석이 복잡할 수 있음\n\n\n\n적용 상황\n\n예측 모델의 성능이 중요한 경우\n타겟과의 비선형 관계가 예상되는 경우\n특성 공학(feature engineering)의 일환으로 사용",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩-연결",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-후-인코딩-연결",
    "title": "8  연속형 데이터 범주화",
    "section": "8.7 범주화 후 인코딩 연결",
    "text": "8.7 범주화 후 인코딩 연결\n범주화된 변수는 여전히 범주형이므로, 모델 학습을 위해 인코딩이 필요하다.\n예제: 범주화 후 One-Hot Encoding\n\n# 범주화된 변수를 One-Hot Encoding\ndf_bin_encoded = pd.get_dummies(\n    df,\n    columns=[\"bill_length_bin\"],\n    drop_first=True\n)\n\nprint(\"인코딩된 컬럼:\")\nprint([col for col in df_bin_encoded.columns if \"bill_length_bin\" in col])\nprint(\"\\n인코딩 결과 샘플:\")\nprint(df_bin_encoded.filter(like='bill_length_bin').head())\n\n인코딩된 컬럼:\n['bill_length_bin_보통', 'bill_length_bin_김', 'bill_length_bin_매우 김']\n\n인코딩 결과 샘플:\n   bill_length_bin_보통  bill_length_bin_김  bill_length_bin_매우 김\n0                True              False                 False\n1                True              False                 False\n2                True              False                 False\n3               False              False                 False\n4               False              False                 False\n\n\n범주화와 인코딩을 조합하면 연속형 변수를 비선형적으로 활용하면서도 선형 모델에 적용할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#범주화-vs-스케일링",
    "href": "part1/08. 연속형 데이터 범주화.html#범주화-vs-스케일링",
    "title": "8  연속형 데이터 범주화",
    "section": "8.8 범주화 vs 스케일링",
    "text": "8.8 범주화 vs 스케일링\n범주화와 스케일링은 서로 다른 목적과 효과를 가진 전처리 방법이다.\n범주화와 스케일링 비교\n\n\n\n항목\n스케일링\n범주화\n\n\n\n\n목적\n변수 간 크기 통일\n연속형을 범주형으로 변환\n\n\n정보 손실\n없음 (변환만)\n있음 (구간화로 인한 손실)\n\n\n해석력\n낮음 (숫자 그대로)\n높음 (의미 있는 구간명)\n\n\n모델 안정성\n보통\n높음 (노이즈 감소)\n\n\n주 용도\n거리 기반 모델, 경사하강법\n규칙 기반 의사결정, 해석\n\n\n적합 모델\n선형, KNN, SVM, 신경망\n트리, 규칙 기반 시스템\n\n\n\n선택 기준\n\n예측 성능 우선: 스케일링 사용 (정보 손실 없음)\n해석력 우선: 범주화 사용 (도메인 전문가 커뮤니케이션)\n노이즈가 많은 경우: 범주화 고려 (노이즈 감소 효과)\n데이터 충분: 스케일링 사용 (범주화는 정보 손실)\n규칙 추출 필요: 범주화 사용 (명확한 의사결정 규칙)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/08. 연속형 데이터 범주화.html#요약",
    "href": "part1/08. 연속형 데이터 범주화.html#요약",
    "title": "8  연속형 데이터 범주화",
    "section": "8.9 요약",
    "text": "8.9 요약\n이 장에서는 연속형 데이터를 범주형으로 변환하는 다양한 범주화 기법을 학습했다. 주요 내용은 다음과 같다.\n범주화 방법 비교\n\n\n\n\n\n\n\n\n\n\n방법\n원리\n장점\n단점\n적용 상황\n\n\n\n\n등간격\n동일한 구간 너비\n간단, 직관적\n데이터 불균형\n균등 분포, 도메인 기준 너비\n\n\n등빈도\n동일한 데이터 개수\n분포 반영, 균형\n너비 불균등, 중복값 문제\n편향 분포, 순위 기반 분석\n\n\n사용자 정의\n도메인 지식 기반\n해석력 최고, 비즈니스 반영\n주관 개입, 근거 필요\n명확한 기준 존재\n\n\nk-means\n군집 중심 기반\n비선형 구조 반영\n해석 어려움, 변동성\n복잡한 분포, 자동화\n\n\nDecision Tree\n타겟 정보이득 기반\n타겟 관계 반영, 성능 향상\n데이터 누수 위험, 과적합\n예측 성능 중요, 비선형 관계\n\n\n\n범주화 적용 상황\n\n\n\n상황\n범주화 권장 여부\n이유\n\n\n\n\n모델 해석이 중요할 때\n✓\n의미 있는 구간명으로 설명 가능\n\n\n비선형 관계 단순화\n✓\n복잡한 관계를 구간별 규칙으로 표현\n\n\n데이터 노이즈가 큼\n✓\n작은 변동 무시, 큰 패턴 포착\n\n\n데이터가 충분히 많음\n✗\n정보 손실이 아까움, 스케일링 선호\n\n\n딥러닝 사용\n✗\n연속형 그대로 사용이 더 효과적\n\n\n규칙 추출 필요\n✓\n명확한 의사결정 규칙 생성\n\n\n\n범주화 의사결정 프로세스\n\nEDA 수행: 데이터 분포, 타겟과의 관계 파악\n목적 확인: 예측 성능 vs 해석력 중 무엇이 우선인지 결정\n방법 선택: 상황에 맞는 범주화 방법 선택\n검증: 범주화 전후 모델 성능 및 해석력 비교\n문서화: 구간 설정 근거와 결과 명확히 기록\n\n범주화는 정보 손실을 감수하고 해석력을 얻는 선택이다. EDA 결과를 바탕으로 신중하게 결정해야 하며, 특히 도메인 전문가와의 협업이 중요한 경우 사용자 정의 구간화를 고려해야 한다. 다음 단계로는 범주화된 변수를 인코딩하여 모델에 적용하거나, 스케일링을 통해 연속형 변수를 그대로 활용하는 방법을 선택할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>연속형 데이터 범주화</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html",
    "href": "part1/09. 불균형 데이터 처리.html",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "9.1 불균형 데이터의 문제점\n불균형 데이터(Imbalanced Data)는 타겟 클래스 간 샘플 수의 차이가 큰 데이터셋을 의미한다. 예를 들어, 사기 거래 탐지 문제에서 정상 거래가 99%, 사기 거래가 1%인 경우가 대표적이다. 불균형 데이터로 학습된 모델은 다수 클래스에 편향되어 소수 클래스를 제대로 예측하지 못하는 문제가 발생한다. 이 장에서는 불균형 데이터의 문제점을 진단하고, 데이터 수준 처리(언더샘플링, 오버샘플링, SMOTE)와 모델 수준 처리(클래스 가중치 조정) 방법을 학습한다.\n예제: 데이터 로드\n불균형 데이터로 학습된 모델은 다수 클래스만 예측하는 경향이 있다. 이 경우 전체 정확도(Accuracy)는 높아 보이지만, 실제로 중요한 소수 클래스는 거의 예측하지 못한다.\n문제 상황 예시\n의료 진단 모델에서 질병이 있는 경우가 5%, 없는 경우가 95%라고 가정하자.\n이는 “잘 맞춘 것처럼 보이지만 중요한 건 다 틀림” 상황으로, 정확도만으로는 모델의 실제 성능을 평가할 수 없음을 보여준다.\n불균형으로 인한 문제점",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-데이터의-문제점",
    "title": "9  불균형 데이터 처리",
    "section": "",
    "text": "지표\n값\n해석\n\n\n\n\n전체 정확도\n95%\n모든 환자를 “질병 없음”으로 예측해도 달성\n\n\n질병 재현율\n10%\n실제 질병 환자 중 10%만 탐지\n\n\n결과\n잘못된 모델\n높은 정확도에도 불구하고 실제로는 쓸모없음\n\n\n\n\n\n\n다수 클래스에 편향된 학습\n소수 클래스의 패턴을 학습하지 못함\n정확도 지표가 실제 성능을 반영하지 못함\n비용이 큰 오분류(예: 질병 미탐지) 발생",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "href": "part1/09. 불균형 데이터 처리.html#불균형-여부-진단",
    "title": "9  불균형 데이터 처리",
    "section": "9.2 불균형 여부 진단",
    "text": "9.2 불균형 여부 진단\n모델 학습 전에 타겟 변수의 클래스 분포를 확인하여 불균형 정도를 파악해야 한다.\n\n9.2.1 클래스 분포 확인\n예제: 클래스별 샘플 수 확인\n\n# 클래스별 샘플 수\nclass_counts = df[\"species\"].value_counts()\nprint(\"클래스별 샘플 수:\")\nprint(class_counts)\nprint(f\"\\n총 샘플 수: {len(df)}\")\n\n클래스별 샘플 수:\nspecies\nAdelie       152\nGentoo       124\nChinstrap     68\nName: count, dtype: int64\n\n총 샘플 수: 344\n\n\n\n\n9.2.2 비율로 확인\n절대 개수보다 비율로 확인하면 불균형 정도를 더 명확히 파악할 수 있다.\n예제: 클래스별 비율 확인\n\n# 클래스별 비율\nclass_ratios = df[\"species\"].value_counts(normalize=True)\nprint(\"클래스별 비율:\")\nprint(class_ratios.round(3))\n\n# 불균형 비율 계산\nmax_ratio = class_ratios.max()\nmin_ratio = class_ratios.min()\nimbalance_ratio = max_ratio / min_ratio\n\nprint(f\"\\n불균형 비율: {imbalance_ratio:.2f}:1\")\nprint(f\"(가장 많은 클래스가 가장 적은 클래스의 {imbalance_ratio:.2f}배)\")\n\n클래스별 비율:\nspecies\nAdelie       0.442\nGentoo       0.360\nChinstrap    0.198\nName: proportion, dtype: float64\n\n불균형 비율: 2.24:1\n(가장 많은 클래스가 가장 적은 클래스의 2.24배)\n\n\n불균형 정도 기준\n\n\n\n비율\n불균형 정도\n조치\n\n\n\n\n1:1 ~ 3:1\n균형\n특별한 조치 불필요\n\n\n3:1 ~ 10:1\n경미한 불균형\n클래스 가중치 고려\n\n\n10:1 ~ 100:1\n심각한 불균형\n샘플링 기법 필수\n\n\n100:1 이상\n극심한 불균형\n복합적 접근 필요\n\n\n\n\n\n9.2.3 시각화\n시각적으로 불균형을 확인하면 직관적으로 파악할 수 있다.\n예제: 클래스 분포 시각화\n\nimport matplotlib.pyplot as plt\n\n# 막대 그래프\nplt.figure(figsize=(8, 5))\ndf[\"species\"].value_counts().plot(kind=\"bar\")\nplt.title(\"Class Distribution\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "href": "part1/09. 불균형 데이터 처리.html#평가-지표-변경",
    "title": "9  불균형 데이터 처리",
    "section": "9.3 평가 지표 변경",
    "text": "9.3 평가 지표 변경\n불균형 데이터에서는 정확도(Accuracy)가 모델 성능을 제대로 반영하지 못하므로, 다른 평가 지표를 사용해야 한다.\n적절한 평가 지표\n\n\n\n\n\n\n\n\n\n지표\n계산 방식\n의미\n사용 상황\n\n\n\n\nPrecision\nTP / (TP + FP)\n양성 예측 중 실제 양성 비율\n거짓 양성(FP) 비용이 클 때\n\n\nRecall\nTP / (TP + FN)\n실제 양성 중 예측한 비율\n거짓 음성(FN) 비용이 클 때\n\n\nF1-score\n2 × (Precision × Recall) / (Precision + Recall)\nPrecision과 Recall의 조화평균\n균형 잡힌 평가 필요\n\n\nROC-AUC\n곡선 아래 면적\n클래스 구분 능력\n이진 분류, 확률 출력 가능\n\n\nPR-AUC\nPrecision-Recall 곡선 아래 면적\n불균형 데이터에 강건\n극심한 불균형\n\n\n\n예제: 다양한 평가 지표 사용\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# 데이터 준비\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\ndf_clean = df[['species'] + num_cols].dropna()\n\nX = df_clean[num_cols]\ny = df_clean[\"species\"]\n\n# 학습/테스트 분리\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 간단한 모델 학습\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# 평가 지표 출력\nprint(\"분류 보고서:\")\nprint(classification_report(y_test, y_pred))\n\nprint(\"\\n혼동 행렬:\")\nprint(confusion_matrix(y_test, y_pred))\n\n분류 보고서:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n혼동 행렬:\n[[30  0  0]\n [ 0 14  0]\n [ 0  0 25]]\n\n\n불균형 데이터에서는 특히 소수 클래스의 Recall(재현율)을 주의 깊게 확인해야 한다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#데이터-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.4 데이터 수준 처리 방법",
    "text": "9.4 데이터 수준 처리 방법\n데이터 수준 처리는 학습 데이터의 클래스 분포를 직접 조정하는 방법이다. 크게 언더샘플링과 오버샘플링으로 나뉜다.\n\n9.4.1 언더샘플링 (Under-sampling)\n언더샘플링은 다수 클래스의 샘플 일부를 제거하여 클래스 균형을 맞추는 방법이다. 구현이 간단하고 빠르지만, 정보 손실이 발생한다.\n예제: 랜덤 언더샘플링\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# 데이터 준비\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\ndf_dropna = df[['species'] + num_cols].dropna()\n\nX = df_dropna[num_cols]\ny = df_dropna[\"species\"]\n\n# 언더샘플링 적용\nrus = RandomUnderSampler(random_state=42)\nX_under, y_under = rus.fit_resample(X, y)\n\nprint(\"언더샘플링 전:\")\nprint(y.value_counts())\nprint(f\"총 샘플 수: {len(y)}\")\n\nprint(\"\\n언더샘플링 후:\")\nprint(y_under.value_counts())\nprint(f\"총 샘플 수: {len(y_under)}\")\nprint(f\"제거된 샘플 수: {len(y) - len(y_under)}\")\n\n언더샘플링 전:\nspecies\nAdelie       151\nGentoo       123\nChinstrap     68\nName: count, dtype: int64\n총 샘플 수: 342\n\n언더샘플링 후:\nspecies\nAdelie       68\nChinstrap    68\nGentoo       68\nName: count, dtype: int64\n총 샘플 수: 204\n제거된 샘플 수: 138\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n구현이 간단하고 빠름\n정보 손실 (유용한 샘플도 제거)\n\n\n학습 속도 향상 (데이터 감소)\n데이터가 적으면 성능 저하\n\n\n과적합 위험 감소\n다수 클래스의 패턴을 놓칠 수 있음\n\n\n\n적용 상황\n\n데이터가 충분히 많을 때 (수만 개 이상)\n학습 속도가 중요할 때\n노이즈가 많아 일부 제거가 유익할 때\n\n\n\n9.4.2 오버샘플링 (Over-sampling)\n오버샘플링은 소수 클래스의 샘플을 복제하거나 생성하여 클래스 균형을 맞추는 방법이다.\n\n9.4.2.1 단순 복제 (Random Over-sampling)\n예제: 랜덤 오버샘플링\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n# 오버샘플링 적용\nros = RandomOverSampler(random_state=42)\nX_over, y_over = ros.fit_resample(X, y)\n\nprint(\"오버샘플링 전:\")\nprint(y.value_counts())\nprint(f\"총 샘플 수: {len(y)}\")\n\nprint(\"\\n오버샘플링 후:\")\nprint(y_over.value_counts())\nprint(f\"총 샘플 수: {len(y_over)}\")\nprint(f\"생성된 샘플 수: {len(y_over) - len(y)}\")\n\n오버샘플링 전:\nspecies\nAdelie       151\nGentoo       123\nChinstrap     68\nName: count, dtype: int64\n총 샘플 수: 342\n\n오버샘플링 후:\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n총 샘플 수: 453\n생성된 샘플 수: 111\n\n\n문제점\n단순 복제는 동일한 샘플을 여러 번 학습하므로 과적합(overfitting) 위험이 크다. 모델이 복제된 샘플을 암기하게 되어 일반화 성능이 저하될 수 있다.\n\n\n\n9.4.3 SMOTE (Synthetic Minority Over-sampling Technique)\nSMOTE는 소수 클래스의 기존 샘플 사이에 가상의 샘플을 생성하는 방법이다. 단순 복제보다 과적합 위험이 적고, 소수 클래스의 특성 공간을 확장한다.\nSMOTE 작동 원리\n\n소수 클래스의 각 샘플에 대해 k개의 최근접 이웃(KNN) 찾기\n이웃 중 하나를 무작위로 선택\n선택된 샘플과의 사이에 새로운 샘플 생성 (보간)\n\n예제: SMOTE 적용\n\nfrom imblearn.over_sampling import SMOTE\n\n# SMOTE 적용\nsmote = SMOTE(random_state=42)\nX_smote, y_smote = smote.fit_resample(X, y)\n\nprint(\"SMOTE 전:\")\nprint(y.value_counts())\nprint(f\"총 샘플 수: {len(y)}\")\n\nprint(\"\\nSMOTE 후:\")\nprint(y_smote.value_counts())\nprint(f\"총 샘플 수: {len(y_smote)}\")\nprint(f\"생성된 합성 샘플 수: {len(y_smote) - len(y)}\")\n\nSMOTE 전:\nspecies\nAdelie       151\nGentoo       123\nChinstrap     68\nName: count, dtype: int64\n총 샘플 수: 342\n\nSMOTE 후:\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n총 샘플 수: 453\n생성된 합성 샘플 수: 111\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n정보 손실 없음 (새로운 샘플 생성)\n노이즈가 있으면 노이즈도 증폭\n\n\n과적합 위험 낮음 (단순 복제보다)\n계산 비용이 높음 (KNN 사용)\n\n\n특성 공간 확장\n범주형 변수 처리 어려움\n\n\n\n적용 상황\n\n데이터가 적을 때\n정보 손실을 최소화하고 싶을 때\n연속형 변수가 많을 때\n\n\n\n9.4.4 SMOTE 변형 기법\nSMOTE의 한계를 보완한 다양한 변형 기법이 존재한다.\nSMOTE 변형 기법 비교\n\n\n\n\n\n\n\n\n\n기법\n특징\n장점\n적용 상황\n\n\n\n\nBorderline-SMOTE\n경계 근처 샘플만 증강\n분류 경계 강화\n클래스 간 겹침이 많을 때\n\n\nSMOTE-NC\n수치형 + 범주형 혼합\n범주형 변수 처리 가능\n범주형 변수 포함 데이터\n\n\nADASYN\n학습이 어려운 영역 집중\n어려운 샘플에 집중\n클래스 내 분포 복잡\n\n\nSMOTE-ENN\nSMOTE + 노이즈 제거\n경계 정리\n노이즈가 많을 때\n\n\n\n예제: Borderline-SMOTE\n\nfrom imblearn.over_sampling import BorderlineSMOTE\n\n# Borderline-SMOTE 적용\nbsmote = BorderlineSMOTE(random_state=42)\nX_bsmote, y_bsmote = bsmote.fit_resample(X, y)\n\nprint(\"Borderline-SMOTE 결과:\")\nprint(y_bsmote.value_counts())\n\nBorderline-SMOTE 결과:\nspecies\nAdelie       151\nChinstrap    151\nGentoo       151\nName: count, dtype: int64\n\n\n예제: SMOTE-NC (범주형 포함)\n\nfrom imblearn.over_sampling import SMOTENC\n\n# 범주형 변수가 포함된 경우 (예시)\n# categorical_features 파라미터에 범주형 변수의 인덱스 지정\n# smotenc = SMOTENC(categorical_features=[0, 1], random_state=42)\n# X_smotenc, y_smotenc = smotenc.fit_resample(X_mixed, y)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "href": "part1/09. 불균형 데이터 처리.html#모델-수준-처리-방법",
    "title": "9  불균형 데이터 처리",
    "section": "9.5 모델 수준 처리 방법",
    "text": "9.5 모델 수준 처리 방법\n모델 수준 처리는 데이터를 변경하지 않고 모델의 학습 과정에서 클래스 가중치를 조정하는 방법이다.\n\n9.5.1 클래스 가중치 자동 조정\n많은 머신러닝 알고리즘은 class_weight 파라미터를 제공한다. class_weight='balanced'로 설정하면 클래스 비율의 역수로 가중치를 자동 계산한다.\n가중치 계산 공식\n\\[\nw_i = \\frac{n_{\\text{samples}}}{n_{\\text{classes}} \\times n_{\\text{samples}_i}}\n\\]\n예제: 자동 클래스 가중치\n\nfrom sklearn.linear_model import LogisticRegression\n\n# 클래스 가중치 자동 조정\nmodel_balanced = LogisticRegression(\n    class_weight=\"balanced\",\n    max_iter=1000,\n    random_state=42\n)\n\n# 학습\nmodel_balanced.fit(X_train, y_train)\ny_pred_balanced = model_balanced.predict(X_test)\n\nprint(\"가중치 조정 모델 평가:\")\nprint(classification_report(y_test, y_pred_balanced))\n\n가중치 조정 모델 평가:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n\n\n\n9.5.2 수동 가중치 설정\n도메인 지식에 따라 클래스별 가중치를 수동으로 설정할 수 있다. 특정 클래스의 오분류 비용이 클 때 유용하다.\n예제: 수동 클래스 가중치\n\n# 수동 가중치 설정 (중요한 클래스에 더 높은 가중치)\nweights = {\n    \"Adelie\": 1.0,\n    \"Gentoo\": 1.5,\n    \"Chinstrap\": 2.0  # 가장 희귀한 클래스에 높은 가중치\n}\n\nmodel_weighted = LogisticRegression(\n    class_weight=weights,\n    max_iter=1000,\n    random_state=42\n)\n\nmodel_weighted.fit(X_train, y_train)\ny_pred_weighted = model_weighted.predict(X_test)\n\nprint(\"수동 가중치 모델 평가:\")\nprint(classification_report(y_test, y_pred_weighted))\n\n수동 가중치 모델 평가:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n\n장단점\n\n\n\n장점\n단점\n\n\n\n\n데이터 변경 없음 (원본 유지)\n모든 알고리즘이 지원하지 않음\n\n\n구현이 간단함\n최적 가중치 찾기 어려움\n\n\n학습 시간 증가 없음\n극심한 불균형에는 한계 있음\n\n\n\n적용 상황\n\n데이터가 충분히 많을 때\n샘플링으로 인한 정보 손실을 피하고 싶을 때\n트리 기반 모델 사용 시",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "href": "part1/09. 불균형 데이터 처리.html#앙상블-불균형-대응",
    "title": "9  불균형 데이터 처리",
    "section": "9.6 앙상블 + 불균형 대응",
    "text": "9.6 앙상블 + 불균형 대응\n앙상블 기법과 불균형 처리를 결합한 방법이다. 여러 모델을 조합하여 성능을 향상시킨다.\n\n9.6.1 Balanced Random Forest\n각 트리를 학습할 때 다수 클래스를 언더샘플링하여 균형을 맞춘다.\n예제: Balanced Random Forest\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n# Balanced Random Forest 모델 생성\nbrf = BalancedRandomForestClassifier(\n    n_estimators=100,\n    random_state=42,\n    n_jobs=-1\n)\n\n# 학습 및 예측\nbrf.fit(X_train, y_train)\ny_pred_brf = brf.predict(X_test)\n\nprint(\"Balanced Random Forest 평가:\")\nprint(classification_report(y_test, y_pred_brf))\n\nBalanced Random Forest 평가:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n\n\n\n9.6.2 EasyEnsemble\n여러 번 언더샘플링하여 다양한 서브셋을 만들고, 각각에 대해 모델을 학습한 후 앙상블한다.\n예제: EasyEnsemble\n\nfrom imblearn.ensemble import EasyEnsembleClassifier\n\n# EasyEnsemble 모델 생성\neec = EasyEnsembleClassifier(\n    n_estimators=10,\n    random_state=42,\n    n_jobs=-1\n)\n\neec.fit(X_train, y_train)\ny_pred_eec = eec.predict(X_test)\n\nprint(\"EasyEnsemble 평가:\")\nprint(classification_report(y_test, y_pred_eec))\n\nEasyEnsemble 평가:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      0.90      0.95        30\n   Chinstrap       0.82      1.00      0.90        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           0.96        69\n   macro avg       0.94      0.97      0.95        69\nweighted avg       0.96      0.96      0.96        69\n\n\n\n앙상블 기법 장점\n\n여러 모델의 강점을 결합하여 강건성 향상\n언더샘플링의 정보 손실을 앙상블로 보완\n과적합 위험 감소",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "href": "part1/09. 불균형 데이터 처리.html#pipeline으로-안전하게-적용",
    "title": "9  불균형 데이터 처리",
    "section": "9.7 Pipeline으로 안전하게 적용",
    "text": "9.7 Pipeline으로 안전하게 적용\n샘플링 기법을 적용할 때 가장 중요한 원칙은 학습 데이터에만 적용하는 것이다. 테스트 데이터에 샘플링을 적용하면 데이터 누수(data leakage)가 발생하여 성능이 과대평가된다.\n예제: Pipeline을 사용한 안전한 샘플링\n\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\n\n# Pipeline 구성 (SMOTE는 학습 데이터에만 적용됨)\npipeline = ImbPipeline([\n    (\"scaler\", StandardScaler()),\n    (\"smote\", SMOTE(random_state=42)),\n    (\"model\", LogisticRegression(max_iter=1000))\n])\n\n# 학습 (SMOTE는 fit 시에만 적용)\npipeline.fit(X_train, y_train)\n\n# 예측 (SMOTE는 적용되지 않음)\ny_pred_pipeline = pipeline.predict(X_test)\n\nprint(\"Pipeline 모델 평가:\")\nprint(classification_report(y_test, y_pred_pipeline))\n\nPipeline 모델 평가:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        30\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        25\n\n    accuracy                           1.00        69\n   macro avg       1.00      1.00      1.00        69\nweighted avg       1.00      1.00      1.00        69\n\n\n\nPipeline을 사용하면 다음과 같은 장점이 있다.\n\n데이터 누수 방지 (샘플링은 학습에만 적용)\n교차 검증 시 자동으로 안전하게 처리\n코드 간결성 및 재사용성 향상",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/09. 불균형 데이터 처리.html#요약",
    "href": "part1/09. 불균형 데이터 처리.html#요약",
    "title": "9  불균형 데이터 처리",
    "section": "9.8 요약",
    "text": "9.8 요약\n이 장에서는 불균형 데이터의 문제점과 다양한 처리 방법을 학습했다. 주요 내용은 다음과 같다.\n불균형 데이터 처리 방법 비교\n\n\n\n\n\n\n\n\n\n\n방법\n원리\n장점\n단점\n적용 상황\n\n\n\n\n언더샘플링\n다수 클래스 제거\n빠름, 과적합 감소\n정보 손실\n데이터 충분, 노이즈 많음\n\n\n오버샘플링\n소수 클래스 복제\n정보 손실 없음\n과적합 위험\n데이터 적음\n\n\nSMOTE\n합성 샘플 생성\n과적합 감소, 공간 확장\n노이즈 증폭\n데이터 적음, 연속형 변수\n\n\n클래스 가중치\n모델 학습 조정\n데이터 변경 없음\n극심한 불균형 한계\n데이터 충분, 원본 유지\n\n\n앙상블\n여러 모델 결합\n강건성 높음\n계산 비용 높음\n성능 중요, 트리 모델\n\n\n\n권장 처리 전략\n\n\n\n\n\n\n\n\n\n상황\n1순위 권장\n2순위 권장\n이유\n\n\n\n\n데이터 적음 (수천 개 이하)\nSMOTE\n오버샘플링\n정보 손실 최소화\n\n\n데이터 많음 (수만 개 이상)\n클래스 가중치\n언더샘플링\n원본 보존, 학습 속도\n\n\n노이즈 많음\n언더샘플링\nBorderline-SMOTE\n노이즈 제거 효과\n\n\n범주형 변수 포함\nSMOTE-NC\n클래스 가중치\n범주형 처리 가능\n\n\n트리 기반 모델\n클래스 가중치\nBalanced RF\n가중치 기본 지원\n\n\n극심한 불균형 (100:1 이상)\nSMOTE + 가중치\n앙상블\n복합적 접근\n\n\n\n불균형 데이터 처리 프로세스\n\n불균형 진단: 클래스 분포 확인 및 비율 계산\n평가 지표 설정: Precision, Recall, F1-score, ROC-AUC 등 선택\n처리 방법 선택: 데이터 크기, 특성, 모델에 따라 결정\nPipeline 구성: 데이터 누수 방지를 위한 안전한 적용\n교차 검증: Stratified K-Fold로 성능 평가\n하이퍼파라미터 튜닝: 처리 방법별 최적 파라미터 찾기\n\n주의사항\n\n데이터 누수 방지: 샘플링은 반드시 학습 데이터에만 적용\n평가 지표: 정확도 대신 Recall, F1-score 등 사용\nStratified Split: 학습/테스트 분리 시 클래스 비율 유지\n과적합 모니터링: 학습/검증 성능 차이 확인\n\n불균형 데이터 처리는 실무에서 매우 흔하게 마주치는 문제이다. 데이터의 특성과 비즈니스 목표를 고려하여 적절한 방법을 선택하고, Pipeline을 사용하여 안전하게 적용하는 것이 중요하다. 이제 전처리의 모든 과정을 마쳤으며, 다음 단계로 정제된 데이터를 활용한 모델링을 진행할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>불균형 데이터 처리</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html",
    "href": "part1/10. 피처 엔지니어링.html",
    "title": "10  피처 엔지니어링",
    "section": "",
    "text": "10.1 피처 엔지니어링의 중요성\n피처 엔지니어링(Feature Engineering)은 기존 데이터를 변형하거나 조합하여 모델이 더 잘 학습할 수 있는 새로운 입력 변수를 만드는 과정이다. 머신러닝에서 “좋은 피처”는 복잡한 알고리즘보다 더 큰 영향을 미치며, 실무 데이터 과학자의 역량이 가장 잘 드러나는 단계이다. 피처 엔지니어링은 도메인 지식과 데이터 탐색을 결합하여 모델이 패턴을 더 쉽게 학습할 수 있도록 돕는다. 이 장에서는 비율 피처, 조합 피처, 변환 피처, 집계 피처 등 다양한 피처 생성 기법과 모델별 전략을 학습한다.\n예제: 데이터 로드\n피처 엔지니어링이 중요한 이유는 다음과 같다.\n피처 엔지니어링의 영향력\n실무 관점",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#피처-엔지니어링의-중요성",
    "href": "part1/10. 피처 엔지니어링.html#피처-엔지니어링의-중요성",
    "title": "10  피처 엔지니어링",
    "section": "",
    "text": "관점\n설명\n예시\n\n\n\n\n성능 차이의 핵심\n같은 모델도 피처에 따라 성능 천차만별\n기본 피처 80% → 파생 피처 95%\n\n\n단순함의 강력함\n좋은 피처 + 단순 모델 &gt; 나쁜 피처 + 복잡 모델\n선형 회귀 + 좋은 피처 &gt; 복잡한 신경망\n\n\n도메인 지식 반영\n전문가 지식이 모델에 직접 반영되는 단계\n의료: 체질량지수(BMI), 금융: 부채비율\n\n\n해석력 향상\n의미 있는 피처는 모델 설명에 도움\n“연령대”가 “나이 제곱”보다 이해 쉬움\n\n\n차원 축소 효과\n여러 변수의 관계를 하나로 압축\n길이/너비 대신 종횡비 사용\n\n\n\n\n\n캐글 경진대회에서 상위권 진입의 핵심 요소\n프로덕션 환경에서 모델 유지보수성 향상\n컴퓨팅 리소스 절약 (단순 모델 사용 가능)\n비즈니스 이해관계자와의 커뮤니케이션 개선",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#파생-변수-생성-기법",
    "href": "part1/10. 피처 엔지니어링.html#파생-변수-생성-기법",
    "title": "10  피처 엔지니어링",
    "section": "10.2 파생 변수 생성 기법",
    "text": "10.2 파생 변수 생성 기법\n파생 변수는 기존 변수를 수학적으로 변환하거나 조합하여 새로운 정보를 추출하는 방법이다.\n\n10.2.1 비율(Ratio) 피처\n두 변수의 비율은 각 변수의 절대값보다 더 의미 있는 정보를 담을 수 있다. 특히 크기가 다른 개체를 비교할 때 유용하다.\n예제: 부리 비율 피처 생성\n\n# 부리 길이 대 깊이 비율\ndf[\"bill_ratio\"] = df[\"bill_length_mm\"] / df[\"bill_depth_mm\"]\n\nprint(\"부리 비율 피처:\")\nprint(df[\"bill_ratio\"].describe())\n\n# 종별 비율 비교\nprint(\"\\n종별 평균 부리 비율:\")\nprint(df.groupby(\"species\")[\"bill_ratio\"].mean().sort_values())\n\n부리 비율 피처:\ncount    333.000000\nmean       2.607228\nstd        0.495436\nmin        1.639810\n25%        2.162651\n50%        2.576531\n75%        3.096970\nmax        3.612676\nName: bill_ratio, dtype: float64\n\n종별 평균 부리 비율:\nspecies\nAdelie       2.121478\nChinstrap    2.653756\nGentoo       3.176602\nName: bill_ratio, dtype: float64\n\n\n비율 피처는 다음과 같은 상황에서 효과적이다.\n\n신체 비례나 형태를 나타낼 때 (예: 부리 형태, 체형)\n스케일이 다른 개체를 비교할 때\n효율성이나 밀도를 나타낼 때 (예: 연비, 인구밀도)\n\n추가 예시\n\n# 체중 대 날개 길이 비율 (체중 효율성)\ndf[\"body_flipper_ratio\"] = df[\"body_mass_g\"] / df[\"flipper_length_mm\"]\n\n# 전체 부리 크기 대비 길이 비율\ndf[\"bill_total\"] = df[\"bill_length_mm\"] + df[\"bill_depth_mm\"]\ndf[\"bill_length_proportion\"] = df[\"bill_length_mm\"] / df[\"bill_total\"]\n\nprint(\"\\n생성된 비율 피처:\")\nprint(df[[\"bill_ratio\", \"body_flipper_ratio\", \"bill_length_proportion\"]].head())\n\n\n생성된 비율 피처:\n   bill_ratio  body_flipper_ratio  bill_length_proportion\n0    2.090909           20.718232                0.676471\n1    2.270115           20.430108                0.694200\n2    2.238889           16.666667                0.691252\n4    1.901554           17.875648                0.655357\n5    1.907767           19.210526                0.656093\n\n\n\n\n10.2.2 조합 피처 (곱셈, 덧셈)\n여러 변수를 곱하거나 더하면 변수 간 상호작용이나 전체 크기를 나타낼 수 있다.\n예제: 상호작용 피처 생성\n\n# 체중과 날개 길이의 상호작용 (전체적인 신체 크기)\ndf[\"body_flipper_interaction\"] = df[\"body_mass_g\"] * df[\"flipper_length_mm\"]\n\nprint(\"상호작용 피처:\")\nprint(df[\"body_flipper_interaction\"].describe())\n\n# 종별 상호작용 피처 비교\nprint(\"\\n종별 평균 상호작용:\")\nprint(df.groupby(\"species\")[\"body_flipper_interaction\"].mean())\n\n상호작용 피처:\ncount    3.330000e+02\nmean     8.553021e+05\nstd      2.206116e+05\nmin      5.158500e+05\n25%      6.796500e+05\n50%      8.075000e+05\n75%      1.021250e+06\nmax      1.392300e+06\nName: body_flipper_interaction, dtype: float64\n\n종별 평균 상호작용:\nspecies\nAdelie       7.059329e+05\nChinstrap    7.327592e+05\nGentoo       1.108586e+06\nName: body_flipper_interaction, dtype: float64\n\n\n조합 피처 유형\n\n\n\n연산\n의미\n예시\n\n\n\n\n곱셈 (A × B)\n상호작용, 면적, 부피\n가로 × 세로 = 면적\n\n\n덧셈 (A + B)\n전체 크기, 합계\n총 점수, 총 길이\n\n\n차이 (A - B)\n변화량, 격차\n가격 변동, 수입-지출\n\n\n평균 ((A + B) / 2)\n중심 경향\n평균 성적, 평균 온도\n\n\n\n추가 예시\n\n# 전체 부리 크기 (길이 + 깊이)\ndf[\"bill_total_size\"] = df[\"bill_length_mm\"] + df[\"bill_depth_mm\"]\n\n# 날개와 부리 길이 차이\ndf[\"flipper_bill_diff\"] = df[\"flipper_length_mm\"] - df[\"bill_length_mm\"]\n\nprint(\"\\n조합 피처 샘플:\")\nprint(df[[\"bill_total_size\", \"flipper_bill_diff\"]].head())\n\n\n조합 피처 샘플:\n   bill_total_size  flipper_bill_diff\n0             57.8              141.9\n1             56.9              146.5\n2             58.3              154.7\n4             56.0              156.3\n5             59.9              150.7\n\n\n\n\n10.2.3 다항식 피처\n변수의 제곱이나 고차항은 비선형 관계를 선형 모델에서 포착할 수 있게 한다.\n예제: 제곱 피처 생성\n\n# 체중의 제곱 (비선형 관계 표현)\ndf[\"body_mass_squared\"] = df[\"body_mass_g\"] ** 2\n\n# 부리 길이의 제곱근 (분포 정규화 효과)\ndf[\"bill_length_sqrt\"] = np.sqrt(df[\"bill_length_mm\"])\n\nprint(\"다항식 피처 샘플:\")\nprint(df[[\"body_mass_g\", \"body_mass_squared\", \"bill_length_sqrt\"]].head())\n\n다항식 피처 샘플:\n   body_mass_g  body_mass_squared  bill_length_sqrt\n0       3750.0         14062500.0          6.252999\n1       3800.0         14440000.0          6.284903\n2       3250.0         10562500.0          6.348228\n4       3450.0         11902500.0          6.058052\n5       3650.0         13322500.0          6.268971",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#변환-기반-피처",
    "href": "part1/10. 피처 엔지니어링.html#변환-기반-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.3 변환 기반 피처",
    "text": "10.3 변환 기반 피처\n변환은 분포를 정규화하거나 스케일을 안정화하여 모델 학습을 돕는다.\n\n10.3.1 로그 변환 피처\n로그 변환은 왜곡된 분포를 정규화하고 큰 값의 영향을 줄인다.\n예제: 로그 변환 피처\n\n# 체중의 로그 변환\ndf[\"log_body_mass\"] = np.log(df[\"body_mass_g\"])\n\nprint(\"로그 변환 전후 비교:\")\nprint(\"원본 왜도:\", df[\"body_mass_g\"].skew().round(3))\nprint(\"로그 왜도:\", df[\"log_body_mass\"].skew().round(3))\n\n# 분포 비교\nprint(\"\\n원본 통계:\")\nprint(df[\"body_mass_g\"].describe())\nprint(\"\\n로그 변환 통계:\")\nprint(df[\"log_body_mass\"].describe())\n\n로그 변환 전후 비교:\n원본 왜도: 0.472\n로그 왜도: 0.175\n\n원본 통계:\ncount     333.000000\nmean     4207.057057\nstd       805.215802\nmin      2700.000000\n25%      3550.000000\n50%      4050.000000\n75%      4775.000000\nmax      6300.000000\nName: body_mass_g, dtype: float64\n\n로그 변환 통계:\ncount    333.000000\nmean       8.326667\nstd        0.188465\nmin        7.901007\n25%        8.174703\n50%        8.306472\n75%        8.471149\nmax        8.748305\nName: log_body_mass, dtype: float64\n\n\n변환의 효과\n\n스케일 안정화: 큰 값과 작은 값의 차이 완화\n거리 기반 모델 성능 향상: KNN, SVM 등에서 중요\n선형 관계 강화: 지수적 관계를 선형으로 변환\n이상치 영향 감소: 극단값을 상대적으로 축소\n\n\n\n10.3.2 표준화/정규화 피처\n변수를 일정 범위로 변환하여 스케일을 통일한다.\n예제: 표준화 피처 생성\n\nfrom sklearn.preprocessing import StandardScaler\n\n# 주요 수치형 변수들을 표준화\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n\nscaler = StandardScaler()\ndf_scaled = pd.DataFrame(\n    scaler.fit_transform(df[num_cols]),\n    columns=[f\"{col}_scaled\" for col in num_cols],\n    index=df.index\n)\n\n# 원본 데이터프레임에 추가\ndf = pd.concat([df, df_scaled], axis=1)\n\nprint(\"표준화 피처 샘플:\")\nprint(df[[num_cols[0], f\"{num_cols[0]}_scaled\"]].head())\n\n표준화 피처 샘플:\n   bill_length_mm  bill_length_mm_scaled\n0            39.1              -0.896042\n1            39.5              -0.822788\n2            40.3              -0.676280\n4            36.7              -1.335566\n5            39.3              -0.859415",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#구간-기반-피처-binning",
    "href": "part1/10. 피처 엔지니어링.html#구간-기반-피처-binning",
    "title": "10  피처 엔지니어링",
    "section": "10.4 구간 기반 피처 (Binning)",
    "text": "10.4 구간 기반 피처 (Binning)\n연속형 변수를 구간으로 나누어 범주화하면 비선형 관계를 포착하고 해석력을 높일 수 있다.\n예제: 체중 구간 피처\n\n# 체중을 3개 구간으로 분류\ndf[\"body_mass_group\"] = pd.cut(\n    df[\"body_mass_g\"],\n    bins=[0, 3500, 4500, 7000],\n    labels=[\"small\", \"medium\", \"large\"]\n)\n\nprint(\"체중 구간 분포:\")\nprint(df[\"body_mass_group\"].value_counts())\n\n# 구간별 평균 날개 길이\nprint(\"\\n구간별 평균 날개 길이:\")\nprint(df.groupby(\"body_mass_group\")[\"flipper_length_mm\"].mean())\n\n체중 구간 분포:\nbody_mass_group\nmedium    146\nlarge     112\nsmall      75\nName: count, dtype: int64\n\n구간별 평균 날개 길이:\nbody_mass_group\nsmall     188.506667\nmedium    195.383562\nlarge     216.589286\nName: flipper_length_mm, dtype: float64\n\n\n구간화의 효과\n\n연속값을 해석 가능한 범주로 변환\n비선형 관계를 단계적으로 표현\n트리 모델에서 분할 힌트 제공\n이상치의 영향 완화\n\n추가 예시: 등빈도 구간화\n\n# 사분위수 기준 구간화\ndf[\"body_mass_quantile\"] = pd.qcut(\n    df[\"body_mass_g\"],\n    q=4,\n    labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n)\n\nprint(\"사분위수 구간 분포:\")\nprint(df[\"body_mass_quantile\"].value_counts())\n\n사분위수 구간 분포:\nbody_mass_quantile\nQ1    86\nQ2    86\nQ4    83\nQ3    78\nName: count, dtype: int64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#집계-기반-피처-aggregation-features",
    "href": "part1/10. 피처 엔지니어링.html#집계-기반-피처-aggregation-features",
    "title": "10  피처 엔지니어링",
    "section": "10.5 집계 기반 피처 (Aggregation Features)",
    "text": "10.5 집계 기반 피처 (Aggregation Features)\n그룹별 통계량을 계산하여 개체가 그룹 내에서 어떤 위치에 있는지 나타낸다.\n\n10.5.1 그룹 평균 대비 차이\n예제: 종별 평균 대비 편차\n\n# 종별 평균 체중 계산\nspecies_mean = df.groupby(\"species\")[\"body_mass_g\"].transform(\"mean\")\n\n# 개체별 편차 계산\ndf[\"body_mass_diff_species\"] = df[\"body_mass_g\"] - species_mean\n\nprint(\"종별 평균 대비 편차:\")\nprint(df[[\"species\", \"body_mass_g\", \"body_mass_diff_species\"]].head(10))\n\n# 편차의 분포\nprint(\"\\n종별 편차 통계:\")\nprint(df.groupby(\"species\")[\"body_mass_diff_species\"].describe())\n\n종별 평균 대비 편차:\n   species  body_mass_g  body_mass_diff_species\n0   Adelie       3750.0               43.835616\n1   Adelie       3800.0               93.835616\n2   Adelie       3250.0             -456.164384\n4   Adelie       3450.0             -256.164384\n5   Adelie       3650.0              -56.164384\n6   Adelie       3625.0              -81.164384\n7   Adelie       4675.0              968.835616\n12  Adelie       3200.0             -506.164384\n13  Adelie       3800.0               93.835616\n14  Adelie       4400.0              693.835616\n\n종별 편차 통계:\n           count          mean         std          min         25%  \\\nspecies                                                               \nAdelie     146.0  1.183589e-13  458.620135  -856.164384 -343.664384   \nChinstrap   68.0  8.024953e-14  384.335081 -1033.088235 -245.588235   \nGentoo     119.0 -3.821406e-13  501.476154 -1142.436975 -392.436975   \n\n                 50%         75%          max  \nspecies                                        \nAdelie     -6.164384  293.835616  1068.835616  \nChinstrap -33.088235  216.911765  1066.911765  \nGentoo    -42.436975  407.563025  1207.563025  \n\n\n\n\n10.5.2 그룹별 다양한 통계량\n예제: 종별 순위와 백분위수\n\n# 종 내 체중 순위\ndf[\"body_mass_rank_species\"] = df.groupby(\"species\")[\"body_mass_g\"].rank(ascending=False)\n\n# 종 내 체중 백분위수\ndf[\"body_mass_pct_species\"] = df.groupby(\"species\")[\"body_mass_g\"].rank(pct=True)\n\nprint(\"종 내 순위와 백분위수:\")\nprint(df[[\"species\", \"body_mass_g\", \"body_mass_rank_species\", \"body_mass_pct_species\"]].head(10))\n\n종 내 순위와 백분위수:\n   species  body_mass_g  body_mass_rank_species  body_mass_pct_species\n0   Adelie       3750.0                    65.5               0.558219\n1   Adelie       3800.0                    58.5               0.606164\n2   Adelie       3250.0                   121.5               0.174658\n4   Adelie       3450.0                   100.5               0.318493\n5   Adelie       3650.0                    76.5               0.482877\n6   Adelie       3625.0                    78.0               0.472603\n7   Adelie       4675.0                     4.0               0.979452\n12  Adelie       3200.0                   124.5               0.154110\n13  Adelie       3800.0                    58.5               0.606164\n14  Adelie       4400.0                    13.0               0.917808\n\n\n집계 피처의 유형\n\n\n\n통계량\n의미\n활용 예시\n\n\n\n\nmean\n그룹 평균\n평균 대비 큰지 작은지\n\n\nstd\n그룹 분산\n그룹 내 변동성\n\n\nmin, max\n그룹 범위\n극단값과의 거리\n\n\nrank\n그룹 내 순위\n상대적 위치\n\n\ncount\n그룹 크기\n희귀도 판단",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#통계-요약-피처",
    "href": "part1/10. 피처 엔지니어링.html#통계-요약-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.6 통계 요약 피처",
    "text": "10.6 통계 요약 피처\n여러 변수의 통계량을 계산하여 개체의 전반적인 특성을 요약한다.\n예제: 행 단위 통계 피처\n\n# 주요 수치형 변수 선택\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n\n# 행 단위 평균 (개체의 전반적인 크기)\ndf[\"num_mean\"] = df[num_cols].mean(axis=1)\n\n# 행 단위 표준편차 (측정값의 변동성)\ndf[\"num_std\"] = df[num_cols].std(axis=1)\n\n# 행 단위 최댓값\ndf[\"num_max\"] = df[num_cols].max(axis=1)\n\n# 행 단위 최솟값\ndf[\"num_min\"] = df[num_cols].min(axis=1)\n\n# 행 단위 범위 (최댓값 - 최솟값)\ndf[\"num_range\"] = df[\"num_max\"] - df[\"num_min\"]\n\nprint(\"통계 요약 피처:\")\nprint(df[[\"num_mean\", \"num_std\", \"num_range\"]].describe())\n\n통계 요약 피처:\n          num_mean      num_std    num_range\ncount   333.000000   333.000000   333.000000\nmean   1117.295420  2061.465442  4189.892192\nstd     204.948885   400.090276   806.147181\nmin     738.875000  1309.655694  2683.400000\n25%     951.400000  1736.198835  3532.500000\n50%    1075.325000  1981.053650  4030.100000\n75%    1258.550000  2345.624922  4756.000000\nmax    1646.350000  3103.740721  6284.800000\n\n\n통계 요약의 효과\n\n개체의 전체적인 특성을 하나의 값으로 압축\n변수 간 일관성 또는 변동성 파악\n차원 축소 효과\n노이즈 감소 (평균화 효과)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#범주형-조합-피처",
    "href": "part1/10. 피처 엔지니어링.html#범주형-조합-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.7 범주형 조합 피처",
    "text": "10.7 범주형 조합 피처\n여러 범주형 변수를 결합하여 세분화된 그룹을 만든다.\n예제: 종과 성별 조합\n\n# 종과 성별을 결합한 새로운 범주\ndf[\"species_sex\"] = df[\"species\"].astype(str) + \"_\" + df[\"sex\"].astype(str)\n\nprint(\"종-성별 조합 분포:\")\nprint(df[\"species_sex\"].value_counts())\n\n# 조합별 평균 체중\nprint(\"\\n종-성별 조합별 평균 체중:\")\nprint(df.groupby(\"species_sex\")[\"body_mass_g\"].mean().sort_values())\n\n종-성별 조합 분포:\nspecies_sex\nAdelie_Male         73\nAdelie_Female       73\nGentoo_Male         61\nGentoo_Female       58\nChinstrap_Female    34\nChinstrap_Male      34\nName: count, dtype: int64\n\n종-성별 조합별 평균 체중:\nspecies_sex\nAdelie_Female       3368.835616\nChinstrap_Female    3527.205882\nChinstrap_Male      3938.970588\nAdelie_Male         4043.493151\nGentoo_Female       4679.741379\nGentoo_Male         5484.836066\nName: body_mass_g, dtype: float64\n\n\n조합 피처 사용 시 주의사항\n\n빈도가 너무 낮은 조합은 과적합 위험 (최소 30개 이상 권장)\n조합 수가 너무 많으면 차원이 폭발적으로 증가\n도메인 지식상 의미 있는 조합만 생성\nOne-Hot Encoding 시 컬럼 수 급증 주의\n\n추가 예시: 삼원 조합\n\n# 종, 성별, 체중 구간의 삼원 조합\ndf[\"species_sex_mass\"] = (\n    df[\"species\"].astype(str) + \"_\" + \n    df[\"sex\"].astype(str) + \"_\" + \n    df[\"body_mass_group\"].astype(str)\n)\n\nprint(\"\\n삼원 조합 샘플:\")\nprint(df[\"species_sex_mass\"].value_counts().head(10))\n\n\n삼원 조합 샘플:\nspecies_sex_mass\nAdelie_Male_medium         61\nGentoo_Male_large          61\nAdelie_Female_small        51\nGentoo_Female_large        42\nChinstrap_Male_medium      28\nAdelie_Female_medium       22\nChinstrap_Female_medium    19\nGentoo_Female_medium       16\nChinstrap_Female_small     15\nAdelie_Male_large           7\nName: count, dtype: int64",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#시간-및-순서형-데이터-피처",
    "href": "part1/10. 피처 엔지니어링.html#시간-및-순서형-데이터-피처",
    "title": "10  피처 엔지니어링",
    "section": "10.8 시간 및 순서형 데이터 피처",
    "text": "10.8 시간 및 순서형 데이터 피처\n시계열 데이터나 순서가 있는 데이터에서는 추가적인 피처 생성 기법을 사용한다.\n시간 데이터 피처 예시\n\n# 예시: 날짜 데이터가 있다고 가정\n# df[\"date\"] = pd.to_datetime(df[\"date\"])\n# \n# # 날짜에서 추출\n# df[\"year\"] = df[\"date\"].dt.year\n# df[\"month\"] = df[\"date\"].dt.month\n# df[\"day_of_week\"] = df[\"date\"].dt.dayofweek\n# df[\"quarter\"] = df[\"date\"].dt.quarter\n# df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n# \n# # 주기성 표현 (원형 인코딩)\n# df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n# df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n# \n# # 시간 경과\n# df[\"days_since_start\"] = (df[\"date\"] - df[\"date\"].min()).dt.days\n# \n# # 이동 평균 (윈도우 = 7일)\n# df[\"rolling_mean_7d\"] = df[\"value\"].rolling(window=7).mean()\n\n시간 피처 유형\n\n\n\n피처 유형\n예시\n설명\n\n\n\n\n추출\n연, 월, 요일, 시간\n날짜/시간에서 구성 요소 분리\n\n\n주기\nsin/cos 변환\n원형 데이터의 연속성 표현\n\n\n경과\n시작일로부터 일수\n시간의 흐름을 수치로 표현\n\n\n집계\n이동평균, 누적합\n과거 패턴 요약\n\n\n차분\n전일 대비 변화량\n변화율이나 추세",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#피처-선택과의-연결",
    "href": "part1/10. 피처 엔지니어링.html#피처-선택과의-연결",
    "title": "10  피처 엔지니어링",
    "section": "10.9 피처 선택과의 연결",
    "text": "10.9 피처 선택과의 연결\n모든 피처를 사용하는 것이 정답은 아니다. 중복되거나 불필요한 피처는 모델 성능을 저하시킬 수 있다.\n예제: 상관관계 분석\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 주요 피처들의 상관관계\nfeature_cols = [\n    \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\",\n    \"bill_ratio\", \"body_flipper_interaction\", \"log_body_mass\"\n]\n\ncorr_matrix = df[feature_cols].corr()\n\n# 히트맵 시각화\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", center=0, fmt=\".2f\")\nplt.title(\"Feature Correlation Matrix\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n피처 선택 기준\n\n높은 상관관계 (|r| &gt; 0.9): 중복 정보, 하나만 선택\n타겟과의 상관관계: 타겟과 관련 없는 피처 제거\n다중공선성: VIF(Variance Inflation Factor) &gt; 10이면 문제\n피처 중요도: 트리 모델의 feature_importances_ 활용\n\n예제: 중요도 기반 선택\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 간단한 모델로 피처 중요도 확인\nX = df[feature_cols].dropna()\ny = df.loc[X.index, \"species\"]\n\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# 피처 중요도 출력\nimportance_df = pd.DataFrame({\n    \"feature\": feature_cols,\n    \"importance\": rf.feature_importances_\n}).sort_values(\"importance\", ascending=False)\n\nprint(\"피처 중요도:\")\nprint(importance_df)\n\n피처 중요도:\n                    feature  importance\n4                bill_ratio    0.326713\n0            bill_length_mm    0.184320\n2         flipper_length_mm    0.160196\n5  body_flipper_interaction    0.151781\n1             bill_depth_mm    0.112097\n6             log_body_mass    0.037838\n3               body_mass_g    0.027055",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#모델별-피처-엔지니어링-전략",
    "href": "part1/10. 피처 엔지니어링.html#모델별-피처-엔지니어링-전략",
    "title": "10  피처 엔지니어링",
    "section": "10.10 모델별 피처 엔지니어링 전략",
    "text": "10.10 모델별 피처 엔지니어링 전략\n모델의 특성에 따라 효과적인 피처가 다르다.\n모델별 피처 전략\n\n\n\n\n\n\n\n\n\n모델 유형\n중요 피처\n이유\n권장 기법\n\n\n\n\n선형 모델 (회귀, 로지스틱)\n스케일링, 상호작용\n변수 간 관계를 명시적으로 표현 필요\n표준화, 다항식, 상호작용\n\n\nKNN, SVM\n스케일링, 분포 정규화\n거리 기반이라 스케일에 민감\nMin-Max, 로그 변환\n\n\n트리 계열 (RF, XGBoost)\n비선형 파생, 범주화\n분할 힌트 제공, 비선형 자동 처리\n비율, Binning, 집계\n\n\n신경망\n단순 + 대량 데이터\n자체적으로 복잡한 피처 학습\n정규화, 임베딩\n\n\nNaive Bayes\n독립 피처\n피처 간 독립성 가정\n단순 변환, 중복 제거\n\n\n\n모델별 예시\n\n# 선형 모델용: 상호작용 피처\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(df[[\"bill_length_mm\", \"bill_depth_mm\"]])\nprint(\"다항식 피처 개수:\", X_poly.shape[1])\n\n# 트리 모델용: 범주화 + 집계\ntree_features = [\"body_mass_group\", \"body_mass_diff_species\", \"bill_ratio\"]\nprint(\"\\n트리 모델 추천 피처:\", tree_features)\n\n다항식 피처 개수: 5\n\n트리 모델 추천 피처: ['body_mass_group', 'body_mass_diff_species', 'bill_ratio']",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#전처리-전체-흐름에서의-위치",
    "href": "part1/10. 피처 엔지니어링.html#전처리-전체-흐름에서의-위치",
    "title": "10  피처 엔지니어링",
    "section": "10.11 전처리 전체 흐름에서의 위치",
    "text": "10.11 전처리 전체 흐름에서의 위치\n피처 엔지니어링은 전처리의 후반부에 위치하며, 정제된 데이터를 바탕으로 수행한다.\n데이터 전처리 전체 흐름\n1. 데이터 로드\n   ↓\n2. 결측치 처리 (제거/대체)\n   ↓\n3. 이상치 탐지 및 처리\n   ↓\n4. 스케일링 (변수 크기 통일)\n   ↓\n5. 분포 변환 (왜도 정규화)\n   ↓\n6. 인코딩 (범주형 → 수치형)\n   ↓\n7. 불균형 데이터 처리\n   ↓\n8. 피처 엔지니어링 ← 현재 장\n   ↓\n9. 피처 선택 (중요 피처 추출)\n   ↓\n10. 모델링\n단계별 피처 생성 시점\n\n결측치 처리 전: X (결측치가 계산을 방해)\n이상치 처리 전: △ (이상치가 비율/평균에 영향)\n스케일링 전: O (원본 값으로 비율 계산 후 스케일링)\n인코딩 후: O (범주형 조합 피처 생성 가능)\n피처 선택 전: O (모든 후보 피처 생성 후 선택)",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part1/10. 피처 엔지니어링.html#요약",
    "href": "part1/10. 피처 엔지니어링.html#요약",
    "title": "10  피처 엔지니어링",
    "section": "10.12 요약",
    "text": "10.12 요약\n이 장에서는 피처 엔지니어링의 개념과 다양한 기법을 학습했다. 주요 내용은 다음과 같다.\n피처 엔지니어링 기법 정리\n\n\n\n\n\n\n\n\n\n기법\n방법\n효과\n예시\n\n\n\n\n비율\nA / B\n상대적 크기, 형태\n부리 길이/깊이 비율\n\n\n조합\nA × B, A + B\n상호작용, 전체 크기\n체중 × 날개 길이\n\n\n변환\nlog, sqrt, 제곱\n분포 정규화, 비선형 관계\nlog(체중)\n\n\n구간화\ncut, qcut\n비선형 포착, 해석력\n체중 그룹(소/중/대)\n\n\n집계\ngroupby + transform\n그룹 내 상대적 위치\n종별 평균 대비 편차\n\n\n통계 요약\nmean, std, max\n전체 특성 압축\n측정값 평균\n\n\n범주 조합\n문자열 결합\n세분화된 그룹\n종_성별 조합\n\n\n시계열\nlag, rolling, diff\n시간 패턴, 추세\n7일 이동평균\n\n\n\n피처 엔지니어링 모범 사례\n\nEDA 먼저: 데이터 탐색을 통해 패턴 발견 후 피처 생성\n도메인 지식 활용: 전문가 의견이나 논문의 아이디어 반영\n단순하게 시작: 복잡한 피처보다 직관적인 피처부터\n반복적 개선: 모델 성능을 확인하며 점진적으로 추가\n과적합 주의: 학습 데이터로만 통계량 계산, 테스트 누수 방지\n문서화: 피처 생성 논리와 의미를 명확히 기록\n재현성 확보: 파이프라인으로 자동화하여 일관성 유지\n\n피처 엔지니어링 체크리스트\n\nEDA를 통해 변수 간 관계 파악했는가?\n도메인 지식을 반영한 피처를 고려했는가?\n비율, 조합, 변환 등 다양한 기법을 시도했는가?\n생성한 피처의 분포와 타겟과의 관계를 확인했는가?\n중복되거나 불필요한 피처를 제거했는가?\n학습/테스트 데이터 누수를 방지했는가?\n피처 생성 과정을 파이프라인으로 자동화했는가?\n\n피처 엔지니어링은 데이터 과학의 핵심 기술이자 예술이다. 도메인 지식, 창의성, 그리고 반복적인 실험을 통해 모델 성능을 크게 향상시킬 수 있다. 좋은 피처는 복잡한 모델보다 강력하며, 실무에서 데이터 과학자의 역량을 가장 잘 드러내는 단계이다. 다음 단계로는 생성된 피처를 바탕으로 모델을 학습하고 평가하는 과정을 진행할 수 있다.",
    "crumbs": [
      "Ⅰ. 데이터 전처리 및 변환",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>피처 엔지니어링</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html",
    "href": "part2/01. 확률분포와 표본.html",
    "title": "11  확률분포와 표본",
    "section": "",
    "text": "11.1 확률분포의 개념\n통계적 추론의 출발점은 데이터가 어떤 확률분포에서 생성되었는지를 이해하는 것이다. 확률분포는 데이터의 불확실성을 수학적으로 모델링하며, 이를 통해 데이터의 패턴을 파악하고 미래를 예측할 수 있다. 또한 표본과 모집단의 관계를 이해하면, 제한된 데이터로부터 전체에 대한 결론을 도출할 수 있다. 이 장에서는 대표적인 확률분포와 표본 개념, 그리고 통계적 추론의 핵심인 중심극한정리를 학습한다.\n예제: 데이터 로드\n확률분포(Probability Distribution)는 확률변수가 취할 수 있는 값과 그 값이 나타날 확률의 구조를 나타낸다. 모든 통계 분석은 데이터가 특정 확률분포를 따른다는 가정에서 출발한다.\n확률분포의 핵심 개념\n확률분포의 분류\n확률분포는 변수의 유형에 따라 크게 두 가지로 구분된다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#확률분포의-개념",
    "href": "part2/01. 확률분포와 표본.html#확률분포의-개념",
    "title": "11  확률분포와 표본",
    "section": "",
    "text": "개념\n설명\n예시\n\n\n\n\n확률변수 (Random Variable)\n결과가 확률적으로 결정되는 변수\n주사위 눈, 키, 몸무게\n\n\n확률질량함수 (PMF)\n이산형 변수의 확률 분포\nP(X = k)\n\n\n확률밀도함수 (PDF)\n연속형 변수의 확률 밀도\nf(x)\n\n\n누적분포함수 (CDF)\n특정 값 이하일 확률\nP(X ≤ x)\n\n\n기댓값 (Expected Value)\n평균적으로 기대되는 값\nE[X] = μ\n\n\n분산 (Variance)\n값의 퍼짐 정도\nVar[X] = σ²\n\n\n\n\n\n\n이산형 확률분포: 셀 수 있는 값들의 분포 (예: 0, 1, 2, 3, …)\n연속형 확률분포: 실수 구간의 모든 값을 가질 수 있는 분포 (예: 키, 몸무게)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#연속형-확률분포-continuous-distributions",
    "href": "part2/01. 확률분포와 표본.html#연속형-확률분포-continuous-distributions",
    "title": "11  확률분포와 표본",
    "section": "11.2 연속형 확률분포 (Continuous Distributions)",
    "text": "11.2 연속형 확률분포 (Continuous Distributions)\n연속형 확률분포는 실수 구간의 모든 값을 가질 수 있는 변수의 분포이다. 특정 점에서의 확률은 0이며, 구간의 확률을 확률밀도함수(PDF)의 적분으로 계산한다.\n\n11.2.1 대표적인 연속형 분포\n연속형 분포 비교\n\n\n\n\n\n\n\n\n\n\n분포\n형태\n특징\n파라미터\n사용 예시\n\n\n\n\n정규분포 (Normal)\n종 모양, 대칭\n중심극한정리, 자연 현상\nμ (평균), σ² (분산)\n신체 측정값, 시험 점수\n\n\n지수분포 (Exponential)\n오른쪽 꼬리\n무기억성, 대기 시간\nλ (비율)\n고장 시간, 서비스 대기\n\n\n감마분포 (Gamma)\n비대칭, 양수\n지수분포의 일반화\nα (형태), β (척도)\n생존 시간, 보험 청구액\n\n\n로그정규분포 (Lognormal)\n오른쪽 꼬리\n로그 변환 시 정규분포\nμ (로그 평균), σ² (로그 분산)\n소득, 주택 가격, 입자 크기\n\n\nt-분포 (Student’s t)\n정규와 유사, 두꺼운 꼬리\n작은 표본 추론\nν (자유도)\n소표본 가설검정\n\n\n카이제곱분포 (Chi-squared)\n오른쪽 치우침\n정규분포 제곱합\nk (자유도)\n분산 검정, 적합도 검정\n\n\n\n\n\n11.2.2 정규분포 (Normal Distribution)\n정규분포는 자연 현상에서 가장 흔히 나타나는 분포로, 평균을 중심으로 좌우 대칭인 종 모양을 띤다.\n정규분포 확률밀도함수\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\\]\n여기서 μ는 평균(중심 위치), σ²는 분산(퍼짐 정도)을 나타낸다.\n정규분포의 특성\n\n평균, 중앙값, 최빈값이 모두 같음\n68-95-99.7 규칙: μ±σ에 68%, μ±2σ에 95%, μ±3σ에 99.7% 포함\n두 정규분포의 합도 정규분포\n중심극한정리의 핵심\n\n예제: 연속형 변수의 분포 시각화\n\n# 체중 분포 확인\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# 히스토그램 + KDE\nsns.histplot(df[\"body_mass_g\"], kde=True, ax=axes[0])\naxes[0].set_title(\"Body Mass Distribution\")\naxes[0].set_xlabel(\"Body Mass (g)\")\naxes[0].set_ylabel(\"Frequency\")\n\n# Q-Q 플롯 (정규성 확인)\nfrom scipy import stats\nstats.probplot(df[\"body_mass_g\"], dist=\"norm\", plot=axes[1])\naxes[1].set_title(\"Q-Q Plot\")\n\nplt.tight_layout()\nplt.show()\n\n# 기술 통계량\nprint(\"\\n체중 기술 통계량:\")\nprint(df[\"body_mass_g\"].describe())\nprint(f\"\\n왜도(Skewness): {df['body_mass_g'].skew():.3f}\")\nprint(f\"첨도(Kurtosis): {df['body_mass_g'].kurtosis():.3f}\")\n\n\n\n\n\n\n\n\n\n체중 기술 통계량:\ncount     333.000000\nmean     4207.057057\nstd       805.215802\nmin      2700.000000\n25%      3550.000000\n50%      4050.000000\n75%      4775.000000\nmax      6300.000000\nName: body_mass_g, dtype: float64\n\n왜도(Skewness): 0.472\n첨도(Kurtosis): -0.733\n\n\nQ-Q 플롯이 직선에 가까우면 정규분포에 가깝다고 판단할 수 있다.\n예제: 정규성 검정\n\n# Shapiro-Wilk 검정 (정규성 검정)\nstatistic, p_value = stats.shapiro(df[\"body_mass_g\"])\n\nprint(\"Shapiro-Wilk 정규성 검정:\")\nprint(f\"검정 통계량: {statistic:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nif p_value &gt; 0.05:\n    print(\"결론: 정규분포를 따른다고 볼 수 있음 (유의수준 0.05)\")\nelse:\n    print(\"결론: 정규분포를 따르지 않음 (유의수준 0.05)\")\n\nShapiro-Wilk 정규성 검정:\n검정 통계량: 0.9580\np-value: 0.0000\n결론: 정규분포를 따르지 않음 (유의수준 0.05)\n\n\n\n\n11.2.3 기타 연속형 분포\n예제: 다양한 연속형 분포 시각화\n\n# 이론적 분포 시각화\nx = np.linspace(0, 10, 1000)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# 정규분포\naxes[0, 0].plot(x, stats.norm.pdf(x, 5, 1), label='μ=5, σ=1')\naxes[0, 0].plot(x, stats.norm.pdf(x, 5, 2), label='μ=5, σ=2')\naxes[0, 0].set_title(\"Normal Distribution\")\naxes[0, 0].legend()\n\n# 지수분포\naxes[0, 1].plot(x, stats.expon.pdf(x, scale=1), label='λ=1')\naxes[0, 1].plot(x, stats.expon.pdf(x, scale=2), label='λ=0.5')\naxes[0, 1].set_title(\"Exponential Distribution\")\naxes[0, 1].legend()\n\n# 감마분포\naxes[1, 0].plot(x, stats.gamma.pdf(x, a=2, scale=1), label='α=2, β=1')\naxes[1, 0].plot(x, stats.gamma.pdf(x, a=5, scale=1), label='α=5, β=1')\naxes[1, 0].set_title(\"Gamma Distribution\")\naxes[1, 0].legend()\n\n# 로그정규분포\naxes[1, 1].plot(x, stats.lognorm.pdf(x, s=0.5), label='σ=0.5')\naxes[1, 1].plot(x, stats.lognorm.pdf(x, s=1), label='σ=1')\naxes[1, 1].set_title(\"Lognormal Distribution\")\naxes[1, 1].legend()\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#이산형-확률분포-discrete-distributions",
    "href": "part2/01. 확률분포와 표본.html#이산형-확률분포-discrete-distributions",
    "title": "11  확률분포와 표본",
    "section": "11.3 이산형 확률분포 (Discrete Distributions)",
    "text": "11.3 이산형 확률분포 (Discrete Distributions)\n이산형 확률분포는 셀 수 있는 값들만 가질 수 있는 변수의 분포이다. 각 값에 대한 확률을 확률질량함수(PMF)로 표현한다.\n\n11.3.1 대표적인 이산형 분포\n이산형 분포 비교\n\n\n\n\n\n\n\n\n\n\n분포\n특징\n파라미터\n확률질량함수\n사용 예시\n\n\n\n\n베르누이 (Bernoulli)\n성공/실패 1회 시행\np (성공 확률)\nP(X=1)=p, P(X=0)=1-p\n동전 던지기, 합격 여부\n\n\n이항분포 (Binomial)\nn번 독립 시행 성공 횟수\nn (시행 횟수), p (성공 확률)\n\\(\\binom{n}{k}p^k(1-p)^{n-k}\\)\n불량품 개수, 설문 응답\n\n\n포아송분포 (Poisson)\n단위 시간/공간 발생 횟수\nλ (평균 발생률)\n\\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\)\n고객 방문 수, 교통사고 건수\n\n\n기하분포 (Geometric)\n첫 성공까지 시행 횟수\np (성공 확률)\n\\((1-p)^{k-1}p\\)\n제품 판매까지 시도 횟수\n\n\n\n\n\n11.3.2 베르누이 분포와 이항분포\n베르누이 분포는 성공(1) 또는 실패(0)의 이진 결과를 가지며, 이항분포는 n번의 독립적인 베르누이 시행에서 성공 횟수의 분포이다.\n이항분포 확률질량함수\n\\[\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\\]\n여기서 n은 시행 횟수, k는 성공 횟수, p는 각 시행의 성공 확률이다.\n예제: 이산형 변수의 분포\n\n# 종별 개체 수 확인\nspecies_counts = df[\"species\"].value_counts()\n\nprint(\"종별 개체 수:\")\nprint(species_counts)\nprint(f\"\\n총 개체 수: {len(df)}\")\n\n# 시각화\nplt.figure(figsize=(8, 5))\nsns.countplot(x=\"species\", data=df, order=species_counts.index)\nplt.title(\"Species Distribution\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# 비율 계산\nspecies_prob = species_counts / len(df)\nprint(\"\\n종별 확률 (비율):\")\nprint(species_prob.round(3))\n\n종별 개체 수:\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64\n\n총 개체 수: 333\n\n\n\n\n\n\n\n\n\n\n종별 확률 (비율):\nspecies\nAdelie       0.438\nGentoo       0.357\nChinstrap    0.204\nName: count, dtype: float64\n\n\n예제: 이항분포 시뮬레이션\n\n# 동전 던지기 100번을 1000회 반복\nn_trials = 100  # 각 실험에서 동전을 던지는 횟수\np_success = 0.5  # 앞면이 나올 확률\nn_experiments = 1000  # 실험 반복 횟수\n\n# 시뮬레이션\nresults = np.random.binomial(n_trials, p_success, n_experiments)\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.hist(results, bins=30, density=True, alpha=0.7, edgecolor='black')\nplt.axvline(n_trials * p_success, color='red', linestyle='--', \n            label=f'Expected (np={n_trials * p_success})')\nplt.title(f\"Binomial Distribution: n={n_trials}, p={p_success}\")\nplt.xlabel(\"Number of Successes\")\nplt.ylabel(\"Probability Density\")\nplt.legend()\nplt.show()\n\nprint(f\"이론적 평균: {n_trials * p_success}\")\nprint(f\"시뮬레이션 평균: {results.mean():.2f}\")\nprint(f\"이론적 표준편차: {np.sqrt(n_trials * p_success * (1 - p_success)):.2f}\")\nprint(f\"시뮬레이션 표준편차: {results.std():.2f}\")\n\n\n\n\n\n\n\n\n이론적 평균: 50.0\n시뮬레이션 평균: 49.81\n이론적 표준편차: 5.00\n시뮬레이션 표준편차: 4.99",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#표본과-모집단",
    "href": "part2/01. 확률분포와 표본.html#표본과-모집단",
    "title": "11  확률분포와 표본",
    "section": "11.4 표본과 모집단",
    "text": "11.4 표본과 모집단\n통계 분석에서 가장 중요한 개념은 표본과 모집단의 구분이다.\n표본과 모집단의 개념\n\n\n\n\n\n\n\n\n\n개념\n정의\n특성\n예시\n\n\n\n\n모집단 (Population)\n연구 대상 전체 집합\n파라미터(parameter)로 표현: μ, σ²\n한국의 모든 성인\n\n\n표본 (Sample)\n모집단에서 추출한 일부\n통계량(statistic)으로 표현: x̄, s²\n1000명의 설문 응답자\n\n\n\n표본 추출의 필요성\n\n비용 절감: 전수 조사는 시간과 비용이 막대함\n실용성: 모집단이 너무 크거나 무한할 수 있음\n파괴적 검사: 제품 수명 테스트처럼 전수 조사가 불가능한 경우\n시의성: 빠른 의사결정이 필요한 경우\n\n\n실무에서는 거의 항상 표본 데이터를 기반으로 분석하며, 표본으로부터 모집단에 대한 결론을 추론한다.\n\n표본 추출 방법\n\n\n\n\n\n\n\n\n\n방법\n설명\n장점\n단점\n\n\n\n\n단순무작위추출\n모든 개체가 동일한 선택 확률\n편향 최소화\n모집단 전체 접근 필요\n\n\n층화추출\n그룹별로 나누어 추출\n그룹별 대표성 확보\n사전 정보 필요\n\n\n군집추출\n군집 단위로 추출\n비용 효율적\n군집 내 유사성 높으면 정보 손실\n\n\n계통추출\n일정 간격으로 추출\n구현 간단\n주기성 존재 시 편향",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#표본-분포-sampling-distribution",
    "href": "part2/01. 확률분포와 표본.html#표본-분포-sampling-distribution",
    "title": "11  확률분포와 표본",
    "section": "11.5 표본 분포 (Sampling Distribution)",
    "text": "11.5 표본 분포 (Sampling Distribution)\n표본 분포는 표본 통계량(평균, 분산, 비율 등)의 확률분포를 의미한다. 이는 데이터 자체의 분포와는 다른 개념이다.\n중요한 구분\n\n데이터의 분포: 개별 관측값의 분포 (예: 펭귄 체중 분포)\n표본 평균의 분포: 여러 표본에서 계산한 평균들의 분포\n\n예제: 표본 평균 분포 시뮬레이션\n\n# 원본 데이터 (모집단으로 가정)\nbody_mass = df[\"body_mass_g\"].values\n\nprint(f\"모집단 크기: {len(body_mass)}\")\nprint(f\"모집단 평균: {body_mass.mean():.2f}\")\nprint(f\"모집단 표준편차: {body_mass.std():.2f}\")\n\n# 표본 크기와 반복 횟수 설정\nsample_size = 30\nn_samples = 1000\n\n# 표본 평균 수집\nsample_means = []\nfor _ in range(n_samples):\n    sample = np.random.choice(body_mass, size=sample_size, replace=True)\n    sample_means.append(sample.mean())\n\nsample_means = np.array(sample_means)\n\nprint(f\"\\n표본 평균의 평균: {sample_means.mean():.2f}\")\nprint(f\"표본 평균의 표준편차(표준오차): {sample_means.std():.2f}\")\nprint(f\"이론적 표준오차: {body_mass.std() / np.sqrt(sample_size):.2f}\")\n\n모집단 크기: 333\n모집단 평균: 4207.06\n모집단 표준편차: 804.01\n\n표본 평균의 평균: 4198.39\n표본 평균의 표준편차(표준오차): 144.53\n이론적 표준오차: 146.79\n\n\n예제: 원본 데이터 vs 표본 평균 분포\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 원본 데이터 분포\naxes[0].hist(body_mass, bins=30, density=True, alpha=0.7, edgecolor='black')\naxes[0].axvline(body_mass.mean(), color='red', linestyle='--', \n                label=f'Mean = {body_mass.mean():.2f}')\naxes[0].set_title(\"Original Data Distribution\")\naxes[0].set_xlabel(\"Body Mass (g)\")\naxes[0].set_ylabel(\"Density\")\naxes[0].legend()\n\n# 표본 평균 분포\naxes[1].hist(sample_means, bins=30, density=True, alpha=0.7, edgecolor='black')\naxes[1].axvline(sample_means.mean(), color='red', linestyle='--', \n                label=f'Mean = {sample_means.mean():.2f}')\n\n# 정규분포 곡선 추가\nx = np.linspace(sample_means.min(), sample_means.max(), 100)\naxes[1].plot(x, stats.norm.pdf(x, sample_means.mean(), sample_means.std()), \n             'g-', linewidth=2, label='Normal fit')\naxes[1].set_title(\"Sampling Distribution of the Mean\")\naxes[1].set_xlabel(\"Sample Mean\")\naxes[1].set_ylabel(\"Density\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n위 그래프에서 확인할 수 있듯이, 원본 데이터는 다소 비대칭적이지만 표본 평균의 분포는 정규분포에 가깝다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#중심극한정리-central-limit-theorem-clt",
    "href": "part2/01. 확률분포와 표본.html#중심극한정리-central-limit-theorem-clt",
    "title": "11  확률분포와 표본",
    "section": "11.6 중심극한정리 (Central Limit Theorem, CLT)",
    "text": "11.6 중심극한정리 (Central Limit Theorem, CLT)\n중심극한정리는 통계학에서 가장 중요한 정리 중 하나로, 표본 크기가 충분히 크면 표본 평균의 분포가 원래 모집단의 분포와 관계없이 정규분포에 수렴한다는 내용이다.\n중심극한정리의 내용\n모집단의 분포와 무관하게, 표본 크기 n이 충분히 크면 표본 평균 \\(\\bar{X}\\)는 다음과 같이 근사적으로 정규분포를 따른다.\n\\[\n\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\n\\]\n여기서 μ는 모집단 평균, σ²는 모집단 분산, n은 표본 크기이다.\n중심극한정리의 조건\n\n표본 크기: 일반적으로 n ≥ 30이면 충분 (분포가 심하게 왜곡되지 않은 경우)\n독립성: 각 관측값이 서로 독립적으로 추출\n동일 분포: 같은 모집단에서 추출 (i.i.d.: independent and identically distributed)\n\n예제: 표본 크기에 따른 정규성 개선\n\n# 다양한 표본 크기로 실험\nsample_sizes = [5, 10, 30, 100]\nn_samples = 1000\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, sample_size in enumerate(sample_sizes):\n    sample_means = []\n    for _ in range(n_samples):\n        sample = np.random.choice(body_mass, size=sample_size, replace=True)\n        sample_means.append(sample.mean())\n    \n    # 히스토그램\n    axes[i].hist(sample_means, bins=30, density=True, alpha=0.7, edgecolor='black')\n    \n    # 정규분포 곡선\n    x = np.linspace(min(sample_means), max(sample_means), 100)\n    axes[i].plot(x, stats.norm.pdf(x, np.mean(sample_means), np.std(sample_means)), \n                 'r-', linewidth=2, label='Normal fit')\n    \n    axes[i].set_title(f\"Sample Size = {sample_size}\")\n    axes[i].set_xlabel(\"Sample Mean\")\n    axes[i].set_ylabel(\"Density\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n표본 크기가 증가할수록 표본 평균 분포가 정규분포에 더 가까워지는 것을 확인할 수 있다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#중심극한정리의-중요성",
    "href": "part2/01. 확률분포와 표본.html#중심극한정리의-중요성",
    "title": "11  확률분포와 표본",
    "section": "11.7 중심극한정리의 중요성",
    "text": "11.7 중심극한정리의 중요성\n중심극한정리는 대부분의 통계적 추론 방법의 이론적 기반이다.\n중심극한정리가 적용되는 통계 기법\n\n\n\n기법\n설명\n중심극한정리의 역할\n\n\n\n\nt-검정\n두 그룹 평균 비교\n표본 평균이 정규분포를 따름\n\n\nANOVA\n여러 그룹 평균 비교\n그룹별 평균이 정규분포를 따름\n\n\n신뢰구간\n모수 추정 범위\n표본 평균의 분포가 정규분포\n\n\n선형 회귀\n변수 간 관계 모델링\n잔차의 정규성 가정\n\n\n가설검정\n통계적 의사결정\n검정 통계량의 분포\n\n\n\n실무적 의미\n\n모집단 분포 불필요: 모집단이 정규분포가 아니어도 분석 가능\n신뢰구간 구성: 표본으로부터 모수의 범위 추정\n가설검정: p-value 계산의 이론적 근거\n예측 구간: 미래 값의 불확실성 정량화\n\n예제: 비정규 분포에서의 중심극한정리\n\n# 극단적으로 왜곡된 분포 (지수분포) 생성\nexponential_data = np.random.exponential(scale=2, size=10000)\n\n# 원본 데이터 분포\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(exponential_data, bins=50, density=True, alpha=0.7, edgecolor='black')\naxes[0].set_title(\"Original Exponential Distribution\")\naxes[0].set_xlabel(\"Value\")\naxes[0].set_ylabel(\"Density\")\n\n# 표본 평균 분포 (n=30)\nsample_means_exp = []\nfor _ in range(1000):\n    sample = np.random.choice(exponential_data, size=30, replace=True)\n    sample_means_exp.append(sample.mean())\n\naxes[1].hist(sample_means_exp, bins=30, density=True, alpha=0.7, edgecolor='black')\nx = np.linspace(min(sample_means_exp), max(sample_means_exp), 100)\naxes[1].plot(x, stats.norm.pdf(x, np.mean(sample_means_exp), np.std(sample_means_exp)), \n             'r-', linewidth=2, label='Normal fit')\naxes[1].set_title(\"Sampling Distribution (n=30)\")\naxes[1].set_xlabel(\"Sample Mean\")\naxes[1].set_ylabel(\"Density\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"원본 데이터 왜도: {stats.skew(exponential_data):.3f}\")\nprint(f\"표본 평균 왜도: {stats.skew(sample_means_exp):.3f}\")\n\n\n\n\n\n\n\n\n원본 데이터 왜도: 2.078\n표본 평균 왜도: 0.194\n\n\n원본 데이터는 극도로 왜곡되어 있지만, 표본 평균의 분포는 정규분포에 매우 가깝다는 것을 확인할 수 있다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/01. 확률분포와 표본.html#요약",
    "href": "part2/01. 확률분포와 표본.html#요약",
    "title": "11  확률분포와 표본",
    "section": "11.8 요약",
    "text": "11.8 요약\n이 장에서는 확률분포와 표본의 개념, 그리고 통계적 추론의 핵심인 중심극한정리를 학습했다. 주요 내용은 다음과 같다.\n확률분포 요약\n\n\n\n\n\n\n\n\n\n구분\n대표 분포\n특징\n주요 사용처\n\n\n\n\n연속형\n정규분포\n대칭, 종 모양\n신체 측정, 자연 현상\n\n\n연속형\n지수분포, 감마분포\n양수, 오른쪽 치우침\n대기 시간, 생존 분석\n\n\n연속형\n로그정규분포\n로그 변환 시 정규\n소득, 가격, 크기\n\n\n이산형\n이항분포\n성공 횟수\n불량품 개수, 설문 응답\n\n\n이산형\n포아송분포\n발생 횟수\n고객 방문, 사고 건수\n\n\n\n표본과 모집단\n\n모집단: 연구 대상 전체 (파라미터: μ, σ²)\n표본: 모집단에서 추출한 일부 (통계량: x̄, s²)\n표본 추출의 목적: 비용 절감, 실용성, 시의성\n\n중심극한정리 핵심\n\n내용: 표본 크기가 충분히 크면 표본 평균은 정규분포에 수렴\n조건: n ≥ 30 (일반적), 독립성, 동일 분포\n의미: 모집단 분포와 무관하게 정규분포 기반 추론 가능\n적용: t-검정, ANOVA, 신뢰구간, 회귀분석 등\n\n실무 적용\n\n데이터 분포 확인: 히스토그램, Q-Q 플롯, 정규성 검정\n표본 설계: 적절한 표본 크기와 추출 방법 선택\n통계 분석: 중심극한정리를 바탕으로 가설검정과 신뢰구간 구성\n결과 해석: 표본 통계량으로 모집단 파라미터 추정\n\n확률분포와 표본의 개념은 통계학과 데이터 분석의 기초이다. 중심극한정리를 이해하면 제한된 표본으로부터 모집단에 대한 타당한 결론을 도출할 수 있으며, 이는 모든 통계적 추론의 출발점이 된다. 다음 장에서는 이러한 이론을 바탕으로 실제 가설검정과 신뢰구간 추정을 학습할 것이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>확률분포와 표본</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html",
    "href": "part2/02. 정규성 검정.html",
    "title": "12  정규성 검정",
    "section": "",
    "text": "12.1 정규성 검정의 필요성\n정규성 검정(Normality Test)은 데이터가 정규분포를 따르는지 확인하는 과정이다. 많은 통계 기법이 정규성을 가정하므로, 분석 전에 이 가정이 타당한지 검증해야 한다. 다만, 정규성 검정의 목적은 “데이터가 완벽히 정규분포인가?”를 판단하는 것이 아니라 “정규분포 가정을 사용해도 분석 결과가 타당한가?”를 확인하는 것이다. 이 장에서는 시각적 방법과 통계적 검정을 통해 정규성을 평가하고, 정규성 가정이 위배될 때의 대응 전략을 학습한다.\n예제: 데이터 로드\n많은 통계 기법이 정규성 가정에 의존한다. 이 가정이 심각하게 위배되면 분석 결과가 부정확해질 수 있다.\n정규성 가정이 필요한 통계 기법\n정규성 검사가 필요한 상황\n정규성 검정의 핵심은 “정규 가정을 사용해도 되는가?”를 판단하는 것이지, “데이터가 완벽히 정규분포인가?”를 확인하는 것이 아니다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성-검정의-필요성",
    "href": "part2/02. 정규성 검정.html#정규성-검정의-필요성",
    "title": "12  정규성 검정",
    "section": "",
    "text": "기법\n정규성 가정 대상\n중요도\n이유\n\n\n\n\nt-검정\n각 그룹의 데이터 또는 표본 평균\n높음\n검정 통계량이 t-분포를 따름\n\n\nANOVA\n각 그룹의 잔차\n높음\nF-통계량의 분포 가정\n\n\n선형 회귀\n잔차(residuals)\n매우 높음\n추론과 예측 구간의 정확도\n\n\n상관 분석\nPearson 상관계수\n중간\n검정 통계량의 분포\n\n\n주성분 분석(PCA)\n데이터\n낮음\n정규성 불필요하지만 해석 도움\n\n\n\n\n\n\n\n\n\n\n\n\n상황\n정규성 필요 여부\n이유\n\n\n\n\n원 데이터 (대표본, n ≥ 30)\n낮음\n중심극한정리로 표본 평균이 정규분포 수렴\n\n\n원 데이터 (소표본, n &lt; 30)\n매우 높음\n중심극한정리 효과 미약\n\n\n잔차(residuals)\n매우 높음\n회귀분석의 핵심 가정\n\n\n비모수 검정 사용 시\n불필요\n분포 가정 없음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#시각적-정규성-검정",
    "href": "part2/02. 정규성 검정.html#시각적-정규성-검정",
    "title": "12  정규성 검정",
    "section": "12.2 시각적 정규성 검정",
    "text": "12.2 시각적 정규성 검정\n통계적 검정 전에 시각적 방법으로 데이터의 분포를 확인하는 것이 중요하다. 시각적 방법은 직관적이고 분포의 특성(왜도, 첨도, 이상치)을 파악하기 쉽다.\n\n12.2.1 히스토그램과 KDE (Kernel Density Estimation)\n히스토그램은 데이터의 분포 형태를 가장 간단하게 확인할 수 있는 방법이다. KDE 곡선을 함께 표시하면 연속적인 분포 형태를 더 잘 파악할 수 있다.\n예제: 히스토그램 + KDE\n\n# 체중 분포 시각화\nplt.figure(figsize=(10, 5))\nsns.histplot(df[\"body_mass_g\"], kde=True, bins=30, edgecolor='black')\nplt.axvline(df[\"body_mass_g\"].mean(), color='red', linestyle='--', \n            linewidth=2, label=f'Mean = {df[\"body_mass_g\"].mean():.1f}')\nplt.axvline(df[\"body_mass_g\"].median(), color='green', linestyle='--', \n            linewidth=2, label=f'Median = {df[\"body_mass_g\"].median():.1f}')\nplt.title(\"Body Mass Distribution with KDE\")\nplt.xlabel(\"Body Mass (g)\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# 기술 통계량\nprint(\"기술 통계량:\")\nprint(df[\"body_mass_g\"].describe())\nprint(f\"\\n왜도(Skewness): {df['body_mass_g'].skew():.3f}\")\nprint(f\"첨도(Kurtosis): {df['body_mass_g'].kurtosis():.3f}\")\n\n\n\n\n\n\n\n\n기술 통계량:\ncount     333.000000\nmean     4207.057057\nstd       805.215802\nmin      2700.000000\n25%      3550.000000\n50%      4050.000000\n75%      4775.000000\nmax      6300.000000\nName: body_mass_g, dtype: float64\n\n왜도(Skewness): 0.472\n첨도(Kurtosis): -0.733\n\n\n정규성 판단 기준\n\n종 모양: 중앙이 높고 양 끝으로 갈수록 낮아지는 형태\n대칭성: 평균과 중앙값이 유사하고 좌우 대칭\n왜도: 0에 가까울수록 대칭 (|skewness| &lt; 0.5 권장)\n첨도: 3에 가까울수록 정규분포 (정규분포의 첨도 = 3)\n꼬리: 양쪽 꼬리가 너무 두껍거나 얇지 않음\n\n\n\n12.2.2 Q-Q Plot (Quantile-Quantile Plot)\nQ-Q 플롯은 데이터의 분위수를 이론적 정규분포의 분위수와 비교하는 그래프이다. 가장 신뢰할 수 있는 시각적 정규성 검정 방법이다.\n예제: Q-Q Plot\n\n# Q-Q 플롯 생성\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 체중 Q-Q 플롯\nstats.probplot(df[\"body_mass_g\"], dist=\"norm\", plot=axes[0])\naxes[0].set_title(\"Q-Q Plot: Body Mass\")\naxes[0].grid(True, alpha=0.3)\n\n# 부리 길이 Q-Q 플롯\nstats.probplot(df[\"bill_length_mm\"], dist=\"norm\", plot=axes[1])\naxes[1].set_title(\"Q-Q Plot: Bill Length\")\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nQ-Q Plot 해석 가이드\n\n\n\n\n\n\n\n\n패턴\n의미\n조치\n\n\n\n\n점들이 직선에 근접\n정규분포에 가까움\n정규성 가정 사용 가능\n\n\n왼쪽 끝이 위로 이탈\n왼쪽 꼬리가 두꺼움 (음의 왜도)\n로그 변환 고려\n\n\n오른쪽 끝이 위로 이탈\n오른쪽 꼬리가 두꺼움 (양의 왜도)\n로그 또는 제곱근 변환\n\n\nS자 곡선\n왜도가 있음\n변환 또는 비모수 검정\n\n\n양쪽 끝이 벗어남\n첨도 문제 (꼬리가 얇거나 두꺼움)\n이상치 확인\n\n\n\n\n\n12.2.3 박스플롯\n박스플롯은 이상치와 분포의 대칭성을 한눈에 확인할 수 있다.\n예제: 박스플롯으로 대칭성 확인\n\n# 박스플롯\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\naxes[0].boxplot(df[\"body_mass_g\"], vert=True)\naxes[0].set_ylabel(\"Body Mass (g)\")\naxes[0].set_title(\"Body Mass Boxplot\")\naxes[0].grid(True, alpha=0.3)\n\naxes[1].boxplot(df[\"bill_length_mm\"], vert=True)\naxes[1].set_ylabel(\"Bill Length (mm)\")\naxes[1].set_title(\"Bill Length Boxplot\")\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n박스가 중앙선을 중심으로 대칭이고 수염(whisker)의 길이가 비슷하면 정규분포에 가깝다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#통계적-정규성-검정",
    "href": "part2/02. 정규성 검정.html#통계적-정규성-검정",
    "title": "12  정규성 검정",
    "section": "12.3 통계적 정규성 검정",
    "text": "12.3 통계적 정규성 검정\n시각적 방법은 주관적일 수 있으므로, 통계적 검정으로 객관적인 판단을 보완한다.\n\n12.3.1 주요 정규성 검정 비교\n정규성 검정 방법 비교\n\n\n\n\n\n\n\n\n\n\n\n검정\n귀무가설\n특징\n장점\n단점\n권장 상황\n\n\n\n\nShapiro-Wilk\n정규분포를 따름\n가장 널리 사용\n강력한 검정력\nn &gt; 5000에서 느림\n일반적 상황 (n ≤ 5000)\n\n\nKolmogorov-Smirnov\n두 분포가 같음\n일반 분포 검정\n다양한 분포 비교 가능\n검정력 낮음\n이론 분포와 비교\n\n\nAnderson-Darling\n정규분포를 따름\n꼬리에 민감\n꼬리 이상 탐지\n해석 복잡\n꼬리가 중요한 경우\n\n\nD’Agostino-Pearson\n정규분포를 따름\n왜도·첨도 기반\n중대형 표본에 적합\n소표본에서 불안정\nn ≥ 50\n\n\nJarque-Bera\n정규분포를 따름\n시계열 데이터용\n계산 간단\n대표본에서만 유효\n시계열 분석\n\n\n\n\n\n12.3.2 Shapiro-Wilk 검정\nShapiro-Wilk 검정은 가장 널리 사용되는 정규성 검정으로, 소표본부터 중표본까지 우수한 검정력을 보인다.\n가설 설정\n\nH₀ (귀무가설): 데이터가 정규분포를 따른다\nH₁ (대립가설): 데이터가 정규분포를 따르지 않는다\n\n예제: Shapiro-Wilk 검정\n\nfrom scipy.stats import shapiro\n\n# 체중에 대한 정규성 검정\nstat, p_value = shapiro(df[\"body_mass_g\"])\n\nprint(\"Shapiro-Wilk Test: Body Mass\")\nprint(f\"검정 통계량(W): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"정규분포를 따른다고 볼 수 있음 (H₀ 채택)\")\nelse:\n    print(\"정규분포를 따르지 않음 (H₀ 기각)\")\n\nShapiro-Wilk Test: Body Mass\n검정 통계량(W): 0.9580\np-value: 0.0000\n유의수준 0.05 기준: 정규분포를 따르지 않음 (H₀ 기각)\n\n\n여러 변수에 대한 검정\n\n# 여러 변수 검정\nnumeric_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n\nprint(\"\\n=== 정규성 검정 결과 요약 ===\")\nresults = []\nfor col in numeric_cols:\n    stat, p_value = shapiro(df[col])\n    results.append({\n        \"Variable\": col,\n        \"W-statistic\": round(stat, 4),\n        \"p-value\": round(p_value, 4),\n        \"Normal?\": \"Yes\" if p_value &gt; 0.05 else \"No\"\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df.to_string(index=False))\n\n\n=== 정규성 검정 결과 요약 ===\n         Variable  W-statistic  p-value Normal?\n   bill_length_mm       0.9743      0.0      No\n    bill_depth_mm       0.9733      0.0      No\nflipper_length_mm       0.9517      0.0      No\n      body_mass_g       0.9580      0.0      No\n\n\n\n\n12.3.3 Kolmogorov-Smirnov 검정\nKolmogorov-Smirnov (K-S) 검정은 데이터의 누적분포함수(CDF)를 이론적 분포와 비교한다.\n예제: Kolmogorov-Smirnov 검정\n\nfrom scipy.stats import kstest\n\n# 정규분포와 비교 (표준화 필요)\ndata_standardized = (df[\"body_mass_g\"] - df[\"body_mass_g\"].mean()) / df[\"body_mass_g\"].std()\nstat, p_value = kstest(data_standardized, 'norm')\n\nprint(\"Kolmogorov-Smirnov Test: Body Mass\")\nprint(f\"검정 통계량(D): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"정규분포를 따른다고 볼 수 있음\")\nelse:\n    print(\"정규분포를 따르지 않음\")\n\nKolmogorov-Smirnov Test: Body Mass\n검정 통계량(D): 0.1057\np-value: 0.0011\n유의수준 0.05 기준: 정규분포를 따르지 않음\n\n\n\n\n12.3.4 Anderson-Darling 검정\nAnderson-Darling 검정은 분포의 꼬리 부분에 더 많은 가중치를 두어 극단값의 정규성을 엄격히 평가한다.\n예제: Anderson-Darling 검정\n\nfrom scipy.stats import anderson\n\n# Anderson-Darling 검정\nresult = anderson(df[\"body_mass_g\"], dist=\"norm\")\n\nprint(\"Anderson-Darling Test: Body Mass\")\nprint(f\"검정 통계량: {result.statistic:.4f}\")\nprint(\"\\n유의수준별 임계값:\")\nfor i, (sig_level, crit_val) in enumerate(zip(result.significance_level, result.critical_values)):\n    print(f\"  {sig_level}%: {crit_val:.4f} \", end=\"\")\n    if result.statistic &lt; crit_val:\n        print(\"(정규분포 가정 가능)\")\n    else:\n        print(\"(정규분포 기각)\")\n\nAnderson-Darling Test: Body Mass\n검정 통계량: 4.6146\n\n유의수준별 임계값:\n  15.0%: 0.5600 (정규분포 기각)\n  10.0%: 0.6300 (정규분포 기각)\n  5.0%: 0.7500 (정규분포 기각)\n  2.5%: 0.8710 (정규분포 기각)\n  1.0%: 1.0330 (정규분포 기각)\n\n\nAnderson-Darling 검정은 p-value 대신 임계값과 비교한다. 검정 통계량이 임계값보다 작으면 정규성을 가정할 수 있다.\n\n\n12.3.5 D’Agostino-Pearson 검정\nD’Agostino-Pearson 검정은 왜도와 첨도를 동시에 고려하여 정규성을 평가한다.\n예제: D’Agostino-Pearson 검정\n\nfrom scipy.stats import normaltest\n\n# D'Agostino-Pearson 검정\nstat, p_value = normaltest(df[\"body_mass_g\"])\n\nprint(\"D'Agostino-Pearson Test: Body Mass\")\nprint(f\"검정 통계량(K²): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"정규분포를 따른다고 볼 수 있음\")\nelse:\n    print(\"정규분포를 따르지 않음\")\n\nD'Agostino-Pearson Test: Body Mass\n검정 통계량(K²): 30.4642\np-value: 0.0000\n유의수준 0.05 기준: 정규분포를 따르지 않음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#표본-크기와-p-value의-함정",
    "href": "part2/02. 정규성 검정.html#표본-크기와-p-value의-함정",
    "title": "12  정규성 검정",
    "section": "12.4 표본 크기와 p-value의 함정",
    "text": "12.4 표본 크기와 p-value의 함정\n표본 크기가 커지면 정규성 검정이 작은 편차에도 민감하게 반응하여 거의 항상 귀무가설을 기각한다. 이는 통계적으로 유의하지만 실무적으로는 무의미한 결과일 수 있다.\n예제: 표본 크기 효과\n\n# 다양한 표본 크기로 검정\nsample_sizes = [30, 100, 300]\n\nprint(\"=== 표본 크기에 따른 Shapiro-Wilk 검정 ===\\n\")\nfor n in sample_sizes:\n    sample = df[\"body_mass_g\"].sample(n, random_state=42)\n    stat, p_value = shapiro(sample)\n    print(f\"n = {n:3d}: W = {stat:.4f}, p-value = {p_value:.4f} \", end=\"\")\n    print(\"→ 정규\" if p_value &gt; 0.05 else \"→ 비정규\")\n\n=== 표본 크기에 따른 Shapiro-Wilk 검정 ===\n\nn =  30: W = 0.9579, p-value = 0.2734 → 정규\nn = 100: W = 0.9458, p-value = 0.0004 → 비정규\nn = 300: W = 0.9592, p-value = 0.0000 → 비정규\n\n\n큰 표본에서의 대응\n\n\n\n표본 크기\np-value 해석\n권장 조치\n\n\n\n\nn &lt; 30\np-value 중요\n통계적 검정 결과 우선\n\n\n30 ≤ n &lt; 100\np-value + 시각화\n둘 다 고려\n\n\nn ≥ 100\n시각화 중요\n실무적 판단 우선\n\n\nn ≥ 300\n거의 항상 기각\n중심극한정리 의존\n\n\n\n\n핵심: 큰 표본에서는 통계적으로 비정규라도 중심극한정리에 의해 표본 평균이 정규분포에 수렴하므로 실무적으로 문제없는 경우가 많다.\n\n예제: 시각적 vs 통계적 판단\n\n# 큰 표본에서의 시각적 검정\nlarge_sample = df[\"body_mass_g\"].sample(300, random_state=42)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 히스토그램\nsns.histplot(large_sample, kde=True, ax=axes[0])\naxes[0].set_title(f\"Histogram (n={len(large_sample)})\")\n\n# Q-Q Plot\nstats.probplot(large_sample, dist=\"norm\", plot=axes[1])\naxes[1].set_title(f\"Q-Q Plot (n={len(large_sample)})\")\n\nplt.tight_layout()\nplt.show()\n\n# 통계적 검정\nstat, p_value = shapiro(large_sample)\nprint(f\"\\nShapiro-Wilk: p-value = {p_value:.4f}\")\nprint(\"→ 통계적으로는 비정규이지만 시각적으로는 정규에 가까움\")\nprint(\"→ 실무적 판단: 정규성 가정 사용 가능\")\n\n\n\n\n\n\n\n\n\nShapiro-Wilk: p-value = 0.0000\n→ 통계적으로는 비정규이지만 시각적으로는 정규에 가까움\n→ 실무적 판단: 정규성 가정 사용 가능",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#정규성이-위배될-때의-대응-전략",
    "href": "part2/02. 정규성 검정.html#정규성이-위배될-때의-대응-전략",
    "title": "12  정규성 검정",
    "section": "12.5 정규성이 위배될 때의 대응 전략",
    "text": "12.5 정규성이 위배될 때의 대응 전략\n정규성 검정 결과가 유의하게 비정규라면, 다음 전략 중 하나를 선택한다.\n\n12.5.1 전략 1: 분포 변환\n데이터를 변환하여 정규분포에 가깝게 만든다. 이는 5장(분포 변환)에서 학습한 내용이다.\n예제: 로그 변환 후 정규성 검정\n\n# 로그 변환\nlog_mass = np.log(df[\"body_mass_g\"])\n\n# 변환 전후 비교\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 원본: 히스토그램\nsns.histplot(df[\"body_mass_g\"], kde=True, ax=axes[0, 0])\naxes[0, 0].set_title(\"Original: Histogram\")\n\n# 원본: Q-Q Plot\nstats.probplot(df[\"body_mass_g\"], dist=\"norm\", plot=axes[0, 1])\naxes[0, 1].set_title(\"Original: Q-Q Plot\")\n\n# 로그 변환: 히스토그램\nsns.histplot(log_mass, kde=True, ax=axes[1, 0])\naxes[1, 0].set_title(\"Log Transformed: Histogram\")\n\n# 로그 변환: Q-Q Plot\nstats.probplot(log_mass, dist=\"norm\", plot=axes[1, 1])\naxes[1, 1].set_title(\"Log Transformed: Q-Q Plot\")\n\nplt.tight_layout()\nplt.show()\n\n# 검정 비교\nprint(\"=== 변환 전후 Shapiro-Wilk 검정 ===\")\nprint(f\"원본 데이터: W = {shapiro(df['body_mass_g'])[0]:.4f}, p = {shapiro(df['body_mass_g'])[1]:.4f}\")\nprint(f\"로그 변환: W = {shapiro(log_mass)[0]:.4f}, p = {shapiro(log_mass)[1]:.4f}\")\n\n\n\n\n\n\n\n\n=== 변환 전후 Shapiro-Wilk 검정 ===\n원본 데이터: W = 0.9580, p = 0.0000\n로그 변환: W = 0.9755, p = 0.0000\n\n\n주요 변환 방법\n\n로그 변환: 양의 왜도를 줄임 (가장 흔히 사용)\n제곱근 변환: 로그보다 약한 변환\nBox-Cox 변환: 최적의 λ 자동 추정\nYeo-Johnson 변환: 0과 음수 포함 데이터 가능\n\n\n\n12.5.2 전략 2: 비모수 검정 사용\n정규성을 가정하지 않는 비모수 검정을 사용한다.\n모수 검정 vs 비모수 검정\n\n\n\n상황\n모수 검정 (정규성 가정)\n비모수 검정 (분포 자유)\n\n\n\n\n두 그룹 평균 비교\nIndependent t-test\nMann-Whitney U test\n\n\n대응 표본 비교\nPaired t-test\nWilcoxon signed-rank test\n\n\n다집단 비교\nOne-way ANOVA\nKruskal-Wallis test\n\n\n상관 분석\nPearson correlation\nSpearman correlation\n\n\n회귀 분석\nLinear regression\nQuantile regression\n\n\n\n예제: t-test vs Mann-Whitney U test\n\nfrom scipy.stats import ttest_ind, mannwhitneyu\n\n# 두 종의 체중 비교 (Adelie vs Gentoo)\nadelie = df[df[\"species\"] == \"Adelie\"][\"body_mass_g\"]\ngentoo = df[df[\"species\"] == \"Gentoo\"][\"body_mass_g\"]\n\n# 모수 검정 (t-test)\nt_stat, t_pval = ttest_ind(adelie, gentoo)\n\n# 비모수 검정 (Mann-Whitney U)\nu_stat, u_pval = mannwhitneyu(adelie, gentoo)\n\nprint(\"=== 두 그룹 비교: Adelie vs Gentoo ===\")\nprint(f\"\\nt-test (모수):          t = {t_stat:.4f}, p = {t_pval:.4f}\")\nprint(f\"Mann-Whitney (비모수): U = {u_stat:.4f}, p = {u_pval:.4f}\")\nprint(\"\\n→ 두 검정 모두 유의한 차이 발견\")\n\n=== 두 그룹 비교: Adelie vs Gentoo ===\n\nt-test (모수):          t = -23.4668, p = 0.0000\nMann-Whitney (비모수): U = 358.5000, p = 0.0000\n\n→ 두 검정 모두 유의한 차이 발견\n\n\n비모수 검정은 검정력이 약간 낮지만, 정규성 가정을 위배해도 안전하게 사용할 수 있다.\n\n\n12.5.3 전략 3: 중심극한정리 활용\n표본 크기가 충분히 크면(일반적으로 n ≥ 30), 원 데이터가 비정규라도 표본 평균은 정규분포에 수렴한다.\n중심극한정리 적용 조건\n\n표본 크기 n ≥ 30 (분포가 심하게 왜곡되지 않은 경우)\n독립성 만족\n극단적인 이상치 없음\n\n예제: 표본 평균의 정규성\n\n# 비정규 데이터 생성 (지수분포)\nnon_normal_data = np.random.exponential(scale=2, size=1000)\n\n# 원본 데이터 정규성 검정\nprint(\"=== 원본 데이터 (지수분포) ===\")\nprint(f\"Shapiro-Wilk: p = {shapiro(non_normal_data[:300])[1]:.4f} (비정규)\")\n\n# 표본 평균 분포 생성\nsample_means = [np.random.choice(non_normal_data, size=30).mean() for _ in range(1000)]\n\n# 표본 평균 정규성 검정\nprint(f\"\\n=== 표본 평균 분포 (n=30, 1000회 반복) ===\")\nprint(f\"Shapiro-Wilk: p = {shapiro(sample_means)[1]:.4f} (정규에 가까움)\")\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(non_normal_data[:300], bins=30, density=True, alpha=0.7, edgecolor='black')\naxes[0].set_title(\"Original Data (Exponential)\")\n\naxes[1].hist(sample_means, bins=30, density=True, alpha=0.7, edgecolor='black')\naxes[1].set_title(\"Sampling Distribution of Mean\")\n\nplt.tight_layout()\nplt.show()\n\n=== 원본 데이터 (지수분포) ===\nShapiro-Wilk: p = 0.0000 (비정규)\n\n=== 표본 평균 분포 (n=30, 1000회 반복) ===\nShapiro-Wilk: p = 0.0000 (정규에 가까움)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/02. 정규성 검정.html#요약",
    "href": "part2/02. 정규성 검정.html#요약",
    "title": "12  정규성 검정",
    "section": "12.6 요약",
    "text": "12.6 요약\n이 장에서는 정규성 검정의 개념, 방법, 그리고 정규성 위배 시 대응 전략을 학습했다. 주요 내용은 다음과 같다.\n정규성 검정 방법 요약\n\n\n\n\n\n\n\n\n\n\n방법\n유형\n장점\n단점\n권장 상황\n\n\n\n\n히스토그램 + KDE\n시각적\n직관적, 분포 형태 파악\n주관적\n초기 탐색\n\n\nQ-Q Plot\n시각적\n정규성 판단에 가장 유용\n해석 연습 필요\n모든 상황 (필수)\n\n\nShapiro-Wilk\n통계적\n검정력 우수\n대표본에서 민감\nn ≤ 5000\n\n\nAnderson-Darling\n통계적\n꼬리 민감\n해석 복잡\n꼬리 중요 시\n\n\nK-S\n통계적\n다양한 분포 비교\n검정력 낮음\n이론 분포 비교\n\n\n\n정규성 위배 시 대응 전략\n\n\n\n\n\n\n\n\n\n\n전략\n방법\n장점\n단점\n적용 상황\n\n\n\n\n분포 변환\n로그, Box-Cox 등\n정규성 개선\n해석 복잡해짐\n왜도가 심한 경우\n\n\n비모수 검정\nMann-Whitney, Kruskal-Wallis\n분포 가정 불필요\n검정력 낮음\n소표본, 비정규\n\n\n중심극한정리\n대표본 활용\n추가 조치 불필요\nn ≥ 30 필요\n충분한 표본\n\n\n부트스트랩\n재표본추출\n분포 가정 불필요\n계산 비용 높음\n복잡한 통계량\n\n\n\n실무 판단 프로세스\n\n시각적 확인: 히스토그램, Q-Q Plot으로 분포 확인\n통계적 검정: Shapiro-Wilk 등으로 객관적 평가\n표본 크기 고려:\n\n소표본 (n &lt; 30): 정규성 매우 중요\n중표본 (30 ≤ n &lt; 100): 시각적 + 통계적 판단\n대표본 (n ≥ 100): 중심극한정리 의존 가능\n\n분석 목적 고려: 회귀의 잔차는 엄격, 평균 비교는 유연\n대응 전략 선택: 변환, 비모수 검정, 또는 진행\n\n주요 주의사항\n\n정규성은 절대 조건이 아니며, 실무적 판단이 중요함\np-value 하나로 결정하지 말고 시각화와 함께 종합 판단\n큰 표본에서는 p-value가 과민하게 반응함을 인지\n정규성 검정은 수단이지 목적이 아님\n\n정규성 검정은 통계 분석의 가정을 검증하는 중요한 단계이다. 시각적 방법과 통계적 검정을 종합적으로 활용하고, 표본 크기와 분석 목적을 고려하여 적절한 대응 전략을 선택하는 것이 중요하다. 다음 장에서는 정규성 가정을 바탕으로 한 가설검정과 신뢰구간 추정을 학습할 것이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>정규성 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html",
    "href": "part2/03. 등분산 검정.html",
    "title": "13  등분산 검정",
    "section": "",
    "text": "13.1 등분산성의 개념\n등분산성(Homoscedasticity)은 여러 집단의 분산이 동일하다는 가정으로, 많은 통계 검정의 전제 조건이다. 등분산 검정(Test for Homogeneity of Variance)은 이 가정이 타당한지 확인하는 과정이다. 정규성 검정과 마찬가지로, 등분산성 가정이 위배되면 t-검정이나 ANOVA의 결과가 부정확해질 수 있다. 이 장에서는 시각적 방법과 통계적 검정을 통해 등분산성을 평가하고, 등분산성이 위배될 때의 대응 전략을 학습한다.\n예제: 데이터 로드\n등분산성(Homoscedasticity)은 여러 집단 또는 조건에서 종속 변수의 분산이 동일하다는 가정이다. 수학적으로는 다음과 같이 표현한다.\n\\[\n\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2\n\\]\n여기서 σᵢ²는 i번째 집단의 모분산이다.\n등분산성 vs 이분산성\n이분산성의 영향\n예제: 집단별 분산 확인\n# 종별 체중 분산 계산\nprint(\"=== 종별 체중 분산 ===\")\nvariances = df.groupby(\"species\")[\"body_mass_g\"].var()\nprint(variances)\n\nprint(f\"\\n최대 분산: {variances.max():.2f}\")\nprint(f\"최소 분산: {variances.min():.2f}\")\nprint(f\"분산 비율: {variances.max() / variances.min():.2f}:1\")\n\n=== 종별 체중 분산 ===\nspecies\nAdelie       210332.427964\nChinstrap    147713.454785\nGentoo       251478.332859\nName: body_mass_g, dtype: float64\n\n최대 분산: 251478.33\n최소 분산: 147713.45\n분산 비율: 1.70:1\n일반적으로 최대 분산이 최소 분산의 2배 이상이면 등분산성을 의심해야 한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성의-개념",
    "href": "part2/03. 등분산 검정.html#등분산성의-개념",
    "title": "13  등분산 검정",
    "section": "",
    "text": "개념\n정의\n특징\n영향\n\n\n\n\n등분산성 (Homoscedasticity)\n모든 집단의 분산이 같음\nσ₁² = σ₂² = ⋯ = σₖ²\n표준 t-검정, ANOVA 사용 가능\n\n\n이분산성 (Heteroscedasticity)\n집단 간 분산이 다름\nσ₁² ≠ σ₂² ≠ ⋯ ≠ σₖ²\n검정력 저하, 1종 오류 증가\n\n\n\n\n\n가설검정의 1종 오류율(α) 왜곡\n신뢰구간의 정확도 저하\n검정력(power) 감소\n회귀분석에서 계수 추정의 비효율성",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성-검정의-필요성",
    "href": "part2/03. 등분산 검정.html#등분산성-검정의-필요성",
    "title": "13  등분산 검정",
    "section": "13.2 등분산성 검정의 필요성",
    "text": "13.2 등분산성 검정의 필요성\n등분산성 가정은 여러 통계 기법의 전제 조건이다.\n통계 기법별 등분산성 필요 여부\n\n\n\n분석 기법\n등분산 필요\n중요도\n비고\n\n\n\n\nStudent t-test\n필요\n높음\n등분산 가정 하에 설계됨\n\n\nANOVA\n필요\n매우 높음\nF-통계량이 등분산 가정\n\n\n선형 회귀 (잔차)\n필요\n매우 높음\n이분산성이 계수 추정 왜곡\n\n\nWelch t-test\n불필요\n-\n이분산 허용\n\n\nWelch ANOVA\n불필요\n-\n이분산 허용\n\n\n비모수 검정\n불필요\n-\n분포 가정 없음\n\n\n부트스트랩\n불필요\n-\n재표본추출 기반\n\n\n\n중요한 사실\n\n정규성보다 등분산성 위반이 더 심각한 경우가 많다. 특히 표본 크기가 집단 간 불균형할 때 이분산성은 1종 오류율을 크게 증가시킨다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성-검정-시점",
    "href": "part2/03. 등분산 검정.html#등분산성-검정-시점",
    "title": "13  등분산 검정",
    "section": "13.3 등분산성 검정 시점",
    "text": "13.3 등분산성 검정 시점\n등분산성 검정은 다음 시점에 수행한다.\n검정 시점\n\n집단 간 평균 비교 전: t-검정이나 ANOVA 수행 전 필수 확인\nANOVA 적용 전: 다집단 비교 시 반드시 검정\n회귀 분석의 잔차 진단: 예측값에 따른 잔차 분산 균일성 확인\n반복측정 데이터: 시간이나 조건에 따른 분산 변화 확인\n\n분석 흐름\n데이터 로드\n  ↓\n정규성 검정 (각 그룹)\n  ↓\n등분산성 검정 ← 현재 장\n  ↓\n├─ 등분산 O → Student t-test / ANOVA\n└─ 등분산 X → Welch t-test / Welch ANOVA / 비모수 검정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#시각적-등분산성-점검",
    "href": "part2/03. 등분산 검정.html#시각적-등분산성-점검",
    "title": "13  등분산 검정",
    "section": "13.4 시각적 등분산성 점검",
    "text": "13.4 시각적 등분산성 점검\n통계적 검정 전에 시각적으로 등분산성을 확인하는 것이 중요하다.\n\n13.4.1 박스플롯\n박스플롯은 집단 간 분산의 차이를 시각적으로 비교하는 가장 간단한 방법이다.\n예제: 박스플롯으로 분산 비교\n\n# 종별 체중 박스플롯\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 박스플롯\nsns.boxplot(x=\"species\", y=\"body_mass_g\", data=df, ax=axes[0])\naxes[0].set_title(\"Body Mass by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"Body Mass (g)\")\n\n# 바이올린 플롯 (분포 형태까지 확인)\nsns.violinplot(x=\"species\", y=\"body_mass_g\", data=df, ax=axes[1])\naxes[1].set_title(\"Body Mass Distribution by Species\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_ylabel(\"Body Mass (g)\")\n\nplt.tight_layout()\nplt.show()\n\n# 집단별 통계량\nprint(\"\\n=== 종별 체중 통계량 ===\")\nsummary = df.groupby(\"species\")[\"body_mass_g\"].agg(['mean', 'std', 'var', 'count'])\nprint(summary)\n\n\n\n\n\n\n\n\n\n=== 종별 체중 통계량 ===\n                  mean         std            var  count\nspecies                                                 \nAdelie     3706.164384  458.620135  210332.427964    146\nChinstrap  3733.088235  384.335081  147713.454785     68\nGentoo     5092.436975  501.476154  251478.332859    119\n\n\n박스플롯 관찰 포인트\n\n박스 높이: IQR(사분위수 범위)을 나타내며, 집단 간 유사해야 함\n수염(whisker) 길이: 데이터의 전체 퍼짐 정도, 집단 간 비슷해야 함\n이상치 개수: 집단 간 이상치 비율이 크게 다르면 이분산 의심\n\n\n\n13.4.2 분산 비교 히스토그램\n집단별 분포를 겹쳐서 확인하면 퍼짐의 차이를 직접 비교할 수 있다.\n예제: 히스토그램으로 분포 비교\n\n# 종별 체중 분포\nplt.figure(figsize=(10, 5))\nsns.histplot(data=df, x=\"body_mass_g\", hue=\"species\", kde=True, alpha=0.5)\nplt.title(\"Body Mass Distribution by Species\")\nplt.xlabel(\"Body Mass (g)\")\nplt.ylabel(\"Frequency\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\n분포의 퍼짐(너비)이 비슷하면 등분산성을 만족할 가능성이 높다.\n\n\n13.4.3 잔차 플롯 (회귀분석)\n회귀분석에서는 예측값에 대한 잔차 플롯으로 이분산성을 확인한다.\n예제: 잔차 플롯\n\nfrom sklearn.linear_model import LinearRegression\n\n# 간단한 선형 회귀 (설명 목적)\nX = df[[\"bill_length_mm\"]].values\ny = df[\"body_mass_g\"].values\n\n# 결측치 제거\nmask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\nX_clean = X[mask]\ny_clean = y[mask]\n\n# 회귀 모델\nmodel = LinearRegression()\nmodel.fit(X_clean, y_clean)\ny_pred = model.predict(X_clean)\nresiduals = y_clean - y_pred\n\n# 잔차 플롯\nplt.figure(figsize=(10, 5))\nplt.scatter(y_pred, residuals, alpha=0.5)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2)\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Fitted Values\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\nprint(f\"잔차 평균: {residuals.mean():.4f}\")\nprint(f\"잔차 표준편차: {residuals.std():.2f}\")\n\n\n\n\n\n\n\n\n잔차 평균: -0.0000\n잔차 표준편차: 649.48\n\n\n잔차가 예측값에 관계없이 일정한 분산을 보이면 등분산성을 만족한다. 패턴(예: 깔때기 모양)이 보이면 이분산성을 의심해야 한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#통계적-등분산-검정",
    "href": "part2/03. 등분산 검정.html#통계적-등분산-검정",
    "title": "13  등분산 검정",
    "section": "13.5 통계적 등분산 검정",
    "text": "13.5 통계적 등분산 검정\n시각적 방법을 보완하기 위해 통계적 검정을 수행한다.\n\n13.5.1 주요 등분산 검정 비교\n등분산 검정 방법 비교\n\n\n\n\n\n\n\n\n\n\n\n검정\n원리\n장점\n단점\n전제 조건\n권장 상황\n\n\n\n\nBartlett\n로그 우도비\n검정력 높음 (정규 분포 시)\n정규성에 매우 민감\n정규성 필요\n정규분포 확실 시\n\n\nLevene\n평균 편차\n안정적, 널리 사용\n보통 검정력\n정규성 불필요\n일반적 상황 (권장)\n\n\nBrown-Forsythe\n중앙값 편차\n이상치에 강건\n약간 복잡\n정규성 불필요\n이상치 많을 때\n\n\nFligner-Killeen\n순위 기반\n매우 강건\n검정력 낮음\n정규성 불필요\n비정규 분포\n\n\n\n검정 선택 가이드\n정규성 만족?\n├─ Yes → Bartlett (검정력 우수)\n└─ No  → Levene (가장 안정적, 실무 권장)\n          ├─ 이상치 많음? → Brown-Forsythe\n          └─ 극심한 비정규? → Fligner-Killeen\n\n\n13.5.2 Levene 검정 (가장 권장)\nLevene 검정은 각 관측값과 집단 평균(또는 중앙값)의 절대편차를 비교한다. 정규성에 민감하지 않아 실무에서 가장 널리 사용된다.\n가설 설정\n\nH₀ (귀무가설): 모든 집단의 분산이 같다 (등분산)\nH₁ (대립가설): 적어도 하나의 집단의 분산이 다르다\n\n예제: Levene 검정\n\nfrom scipy.stats import levene\n\n# 종별 체중 데이터 준비\ngroups = [\n    df[df[\"species\"] == sp][\"body_mass_g\"].dropna()\n    for sp in df[\"species\"].unique()\n]\n\n# Levene 검정\nstat, p_value = levene(*groups)\n\nprint(\"=== Levene Test ===\")\nprint(f\"검정 통계량(W): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"등분산성 가정 가능 (H₀ 채택)\")\nelse:\n    print(\"등분산성 위배 (H₀ 기각, 이분산)\")\n\n=== Levene Test ===\n검정 통계량(W): 5.1349\np-value: 0.0064\n\n유의수준 0.05 기준: 등분산성 위배 (H₀ 기각, 이분산)\n\n\nLevene 검정의 변형\n\n# center 파라미터로 평균 vs 중앙값 선택 가능\n# center='mean': 평균 기반 (기본값)\n# center='median': 중앙값 기반 (Brown-Forsythe)\n# center='trimmed': 절사평균 기반\n\nstat_median, p_median = levene(*groups, center='median')\nprint(f\"\\nLevene (중앙값 기반, Brown-Forsythe): p = {p_median:.4f}\")\n\n\nLevene (중앙값 기반, Brown-Forsythe): p = 0.0064\n\n\n\n\n13.5.3 Bartlett 검정\nBartlett 검정은 로그 우도비를 사용하며, 정규분포를 따를 때 가장 강력한 검정이다. 하지만 정규성에 매우 민감하여 실무에서는 잘 사용되지 않는다.\n예제: Bartlett 검정\n\nfrom scipy.stats import bartlett\n\n# Bartlett 검정\nstat, p_value = bartlett(*groups)\n\nprint(\"\\n=== Bartlett Test ===\")\nprint(f\"검정 통계량(T): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"등분산성 가정 가능\")\nelse:\n    print(\"등분산성 위배\")\n\nprint(\"\\n⚠️ 주의: Bartlett 검정은 정규성에 민감함\")\nprint(\"   정규성이 위배되면 잘못된 결과 (과도한 기각)\")\n\n\n=== Bartlett Test ===\n검정 통계량(T): 5.6920\np-value: 0.0581\n\n유의수준 0.05 기준: 등분산성 가정 가능\n\n⚠️ 주의: Bartlett 검정은 정규성에 민감함\n   정규성이 위배되면 잘못된 결과 (과도한 기각)\n\n\n\n\n13.5.4 Fligner-Killeen 검정\nFligner-Killeen 검정은 순위 기반 비모수 검정으로, 극심한 비정규 분포나 두꺼운 꼬리를 가진 분포에서도 안정적이다.\n예제: Fligner-Killeen 검정\n\nfrom scipy.stats import fligner\n\n# Fligner-Killeen 검정\nstat, p_value = fligner(*groups)\n\nprint(\"\\n=== Fligner-Killeen Test ===\")\nprint(f\"검정 통계량(H): {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"등분산성 가정 가능\")\nelse:\n    print(\"등분산성 위배\")\n\nprint(\"\\n✓ 장점: 비모수, 매우 강건\")\nprint(\"  단점: 검정력이 다소 낮음\")\n\n\n=== Fligner-Killeen Test ===\n검정 통계량(H): 9.2507\np-value: 0.0098\n\n유의수준 0.05 기준: 등분산성 위배\n\n✓ 장점: 비모수, 매우 강건\n  단점: 검정력이 다소 낮음\n\n\n\n\n13.5.5 검정 결과 종합 비교\n예제: 모든 검정 결과 비교\n\n# 모든 검정 수행\ntests = {\n    \"Levene\": levene(*groups),\n    \"Levene (median)\": levene(*groups, center='median'),\n    \"Bartlett\": bartlett(*groups),\n    \"Fligner-Killeen\": fligner(*groups)\n}\n\nprint(\"\\n=== 등분산 검정 결과 요약 ===\")\nresults = []\nfor name, (stat, p_val) in tests.items():\n    results.append({\n        \"Test\": name,\n        \"Statistic\": round(stat, 4),\n        \"p-value\": round(p_val, 4),\n        \"Result\": \"등분산\" if p_val &gt; 0.05 else \"이분산\"\n    })\n\nresults_df = pd.DataFrame(results)\nprint(results_df.to_string(index=False))\n\n\n=== 등분산 검정 결과 요약 ===\n           Test  Statistic  p-value Result\n         Levene     5.1349   0.0064    이분산\nLevene (median)     5.1349   0.0064    이분산\n       Bartlett     5.6920   0.0581    등분산\nFligner-Killeen     9.2507   0.0098    이분산",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#등분산성이-위배될-때의-대응-전략",
    "href": "part2/03. 등분산 검정.html#등분산성이-위배될-때의-대응-전략",
    "title": "13  등분산 검정",
    "section": "13.6 등분산성이 위배될 때의 대응 전략",
    "text": "13.6 등분산성이 위배될 때의 대응 전략\n등분산성 검정 결과가 유의하게 이분산이라면, 다음 전략 중 하나를 선택한다.\n\n13.6.1 전략 1: 이분산 허용 검정 사용\n등분산 가정을 필요로 하지 않는 대안 검정을 사용한다.\n대안 검정\n\n\n\n원래 검정\n등분산 가정\n대안 검정\n등분산 불필요\n\n\n\n\nStudent t-test\n필요\nWelch t-test\n불필요\n\n\nOne-way ANOVA\n필요\nWelch ANOVA\n불필요\n\n\n상관 분석\n-\n비모수 상관 (Spearman)\n불필요\n\n\n회귀 분석\n필요\n가중 최소제곱(WLS)\n이분산 고려\n\n\n\n예제: Student t-test vs Welch t-test\n\nfrom scipy.stats import ttest_ind\n\n# 두 종 선택\nadelie = df[df[\"species\"] == \"Adelie\"][\"body_mass_g\"].dropna()\ngentoo = df[df[\"species\"] == \"Gentoo\"][\"body_mass_g\"].dropna()\n\n# Student t-test (등분산 가정)\nt_equal, p_equal = ttest_ind(adelie, gentoo, equal_var=True)\n\n# Welch t-test (이분산 허용)\nt_welch, p_welch = ttest_ind(adelie, gentoo, equal_var=False)\n\nprint(\"=== t-test 비교: Adelie vs Gentoo ===\")\nprint(f\"\\nStudent t-test (등분산 가정):\")\nprint(f\"  t = {t_equal:.4f}, p = {p_equal:.4f}\")\n\nprint(f\"\\nWelch t-test (이분산 허용):\")\nprint(f\"  t = {t_welch:.4f}, p = {p_welch:.4f}\")\n\nprint(\"\\n→ 이분산인 경우 Welch t-test 사용 권장\")\n\n=== t-test 비교: Adelie vs Gentoo ===\n\nStudent t-test (등분산 가정):\n  t = -23.4668, p = 0.0000\n\nWelch t-test (이분산 허용):\n  t = -23.2539, p = 0.0000\n\n→ 이분산인 경우 Welch t-test 사용 권장\n\n\n\n\n13.6.2 전략 2: 분포 변환\n데이터를 변환하여 등분산성을 개선할 수 있다.\n예제: 로그 변환 후 등분산 검정\n\n# 로그 변환\ndf_log = df.copy()\ndf_log[\"log_body_mass\"] = np.log(df[\"body_mass_g\"])\n\n# 변환 후 집단 데이터\ngroups_log = [\n    df_log[df_log[\"species\"] == sp][\"log_body_mass\"].dropna()\n    for sp in df_log[\"species\"].unique()\n]\n\n# 변환 전후 Levene 검정 비교\nstat_orig, p_orig = levene(*groups)\nstat_log, p_log = levene(*groups_log)\n\nprint(\"=== 변환 전후 Levene 검정 비교 ===\")\nprint(f\"원본 데이터: W = {stat_orig:.4f}, p = {p_orig:.4f}\")\nprint(f\"로그 변환: W = {stat_log:.4f}, p = {p_log:.4f}\")\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.boxplot(x=\"species\", y=\"body_mass_g\", data=df, ax=axes[0])\naxes[0].set_title(\"Original Data\")\n\nsns.boxplot(x=\"species\", y=\"log_body_mass\", data=df_log, ax=axes[1])\naxes[1].set_title(\"Log Transformed Data\")\n\nplt.tight_layout()\nplt.show()\n\n=== 변환 전후 Levene 검정 비교 ===\n원본 데이터: W = 5.1349, p = 0.0064\n로그 변환: W = 4.1210, p = 0.0171\n\n\n\n\n\n\n\n\n\n주요 변환 방법\n\n로그 변환: 양의 왜도를 줄이고 분산 안정화\n제곱근 변환: 로그보다 약한 변환\nBox-Cox 변환: 최적의 변환 파라미터 자동 추정\nYeo-Johnson 변환: 0과 음수 포함 데이터 가능\n\n\n\n13.6.3 전략 3: 비모수 검정 사용\n분포 가정을 하지 않는 비모수 검정을 사용한다.\n예제: ANOVA vs Kruskal-Wallis\n\nfrom scipy.stats import f_oneway, kruskal\n\n# 종별 체중 데이터\nspecies_groups = [\n    df[df[\"species\"] == sp][\"body_mass_g\"].dropna()\n    for sp in df[\"species\"].unique()\n]\n\n# 모수 검정 (ANOVA)\nf_stat, p_anova = f_oneway(*species_groups)\n\n# 비모수 검정 (Kruskal-Wallis)\nh_stat, p_kw = kruskal(*species_groups)\n\nprint(\"=== 다집단 비교: 모수 vs 비모수 ===\")\nprint(f\"\\nOne-way ANOVA (등분산 가정):\")\nprint(f\"  F = {f_stat:.4f}, p = {p_anova:.4f}\")\n\nprint(f\"\\nKruskal-Wallis (비모수):\")\nprint(f\"  H = {h_stat:.4f}, p = {p_kw:.4f}\")\n\nprint(\"\\n→ 등분산성 위배 시 Kruskal-Wallis 사용\")\n\n=== 다집단 비교: 모수 vs 비모수 ===\n\nOne-way ANOVA (등분산 가정):\n  F = 341.8949, p = 0.0000\n\nKruskal-Wallis (비모수):\n  H = 212.0851, p = 0.0000\n\n→ 등분산성 위배 시 Kruskal-Wallis 사용\n\n\n\n\n13.6.4 전략 4: 가중 회귀 (회귀분석)\n회귀분석에서 이분산성이 있을 때 가중 최소제곱(Weighted Least Squares)을 사용한다.\n# 가중 최소제곱 예시 (개념적)\n# from statsmodels.regression.linear_model import WLS\n# weights = 1 / residuals**2  # 잔차의 역수로 가중치 계산\n# model_wls = WLS(y, X, weights=weights).fit()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#정규성과-등분산성의-관계",
    "href": "part2/03. 등분산 검정.html#정규성과-등분산성의-관계",
    "title": "13  등분산 검정",
    "section": "13.7 정규성과 등분산성의 관계",
    "text": "13.7 정규성과 등분산성의 관계\n정규성과 등분산성은 독립적인 가정이다.\n정규성 vs 등분산성\n\n\n\n상황\n정규성\n등분산성\n검정 선택\n\n\n\n\n이상적\nO\nO\nStudent t-test, ANOVA\n\n\n일반적\nO\nX\nWelch t-test, Welch ANOVA\n\n\n비정규\nX\nO\n비모수 검정 또는 변환 후 검정\n\n\n최악\nX\nX\n비모수 검정 필수\n\n\n\n검정 간 관계\n\nBartlett는 정규성에 의존: 정규성 위배 시 과도하게 기각\nLevene과 Fligner는 정규성에 독립: 정규성과 무관하게 안정적\n\n실무 권장 조합\n정규성 검정: Q-Q plot + Shapiro-Wilk\n     +\n등분산 검정: 박스플롯 + Levene\n예제: 정규성과 등분산성 동시 검정\n\nprint(\"=== 정규성 및 등분산성 검정 종합 ===\\n\")\n\n# 각 종별 정규성 검정\nfor species in df[\"species\"].unique():\n    data = df[df[\"species\"] == species][\"body_mass_g\"].dropna()\n    stat, p_val = stats.shapiro(data)\n    print(f\"{species} 정규성: W = {stat:.4f}, p = {p_val:.4f} \", end=\"\")\n    print(\"→ 정규\" if p_val &gt; 0.05 else \"→ 비정규\")\n\n# 등분산성 검정\nstat, p_val = levene(*groups)\nprint(f\"\\n등분산성 (Levene): W = {stat:.4f}, p = {p_val:.4f} \", end=\"\")\nprint(\"→ 등분산\" if p_val &gt; 0.05 else \"→ 이분산\")\n\n# 최종 권장\nprint(\"\\n=== 최종 권장 검정 ===\")\nif all([stats.shapiro(df[df[\"species\"] == sp][\"body_mass_g\"].dropna())[1] &gt; 0.05 \n        for sp in df[\"species\"].unique()]) and p_val &gt; 0.05:\n    print(\"✓ Student t-test / ANOVA 사용 가능\")\nelif p_val &gt; 0.05:\n    print(\"△ 정규성 위배 → 비모수 검정 또는 변환 후 검정\")\nelse:\n    print(\"✓ 이분산 → Welch t-test / Welch ANOVA 권장\")\n\n=== 정규성 및 등분산성 검정 종합 ===\n\nAdelie 정규성: W = 0.9812, p = 0.0423 → 비정규\nChinstrap 정규성: W = 0.9845, p = 0.5605 → 정규\nGentoo 정규성: W = 0.9861, p = 0.2605 → 정규\n\n등분산성 (Levene): W = 5.1349, p = 0.0064 → 이분산\n\n=== 최종 권장 검정 ===\n✓ 이분산 → Welch t-test / Welch ANOVA 권장",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/03. 등분산 검정.html#요약",
    "href": "part2/03. 등분산 검정.html#요약",
    "title": "13  등분산 검정",
    "section": "13.8 요약",
    "text": "13.8 요약\n이 장에서는 등분산 검정의 개념, 방법, 그리고 이분산 시 대응 전략을 학습했다. 주요 내용은 다음과 같다.\n등분산 검정 방법 요약\n\n\n\n\n\n\n\n\n\n\n방법\n유형\n장점\n단점\n권장 상황\n\n\n\n\n박스플롯\n시각적\n직관적, 이상치 확인\n주관적\n초기 탐색\n\n\n잔차 플롯\n시각적\n회귀 이분산 확인\n회귀 전용\n회귀분석\n\n\nLevene\n통계적\n안정적, 널리 사용\n보통 검정력\n일반적 상황 (권장)\n\n\nBartlett\n통계적\n검정력 높음\n정규성 민감\n정규분포 확실 시\n\n\nFligner-Killeen\n통계적\n매우 강건\n검정력 낮음\n극심한 비정규\n\n\n\n이분산 시 대응 전략\n\n\n\n\n\n\n\n\n\n\n전략\n방법\n장점\n단점\n적용 상황\n\n\n\n\n대안 검정\nWelch t-test, Welch ANOVA\n변환 불필요\n검정력 약간 낮음\n첫 번째 선택\n\n\n분포 변환\n로그, Box-Cox\n등분산성 개선\n해석 복잡\n변환이 합리적인 경우\n\n\n비모수 검정\nMann-Whitney, Kruskal-Wallis\n분포 가정 불필요\n검정력 낮음\n정규성도 위배\n\n\n가중 회귀\nWLS\n효율적 추정\n구현 복잡\n회귀분석\n\n\n\n실무 점검 프로세스\n\n시각적 확인: 박스플롯으로 분산 차이 확인\n집단별 분산 계산: 최대/최소 비율 확인 (2배 이상 주의)\n통계적 검정: Levene 검정 수행\n정규성 고려: 정규성도 함께 확인\n대응 전략 선택:\n\n등분산 O → Student t-test, ANOVA\n등분산 X → Welch 검정 또는 비모수 검정\n\n\n주요 주의사항\n\n정규성보다 등분산성 위반이 더 심각할 수 있음\nBartlett는 정규성에 민감하므로 실무에서 비권장\nLevene이 가장 안정적이고 널리 사용됨\n표본 크기가 집단 간 불균형하면 이분산성 영향 증가\n시각적 확인과 통계적 검정을 함께 고려\n\n등분산 검정은 집단 간 비교 분석의 필수 단계이다. 시각적 방법과 통계적 검정을 종합적으로 활용하고, 이분산성이 확인되면 적절한 대안 검정이나 변환을 사용하는 것이 중요하다. 다음 장에서는 등분산성 가정을 바탕으로 한 t-검정과 ANOVA를 학습할 것이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>등분산 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html",
    "href": "part2/04. 적합성 및 독립성 검정.html",
    "title": "14  적합성 및 독립성 검정",
    "section": "",
    "text": "14.1 검정 유형과 적용 시점\n적합성 검정(Goodness-of-Fit Test)과 독립성 검정(Test of Independence)은 범주형 데이터의 빈도와 구조를 분석하는 방법이다. 이 검정들은 평균이 아닌 관측 빈도와 기대 빈도의 차이를 통해 데이터의 패턴을 파악한다. 적합성 검정은 “데이터가 특정 이론적 분포를 따르는가?”를, 독립성 검정은 “두 범주형 변수가 서로 관련이 있는가?”를 확인한다. 이 장에서는 카이제곱 검정을 중심으로 범주형 데이터 분석 방법을 학습한다.\n예제: 데이터 로드\n적합성 검정과 독립성 검정은 다음과 같은 질문에 답한다.\n검정 유형별 질문\n이러한 검정은 평균이 아니라 빈도와 구조를 파악하기 위해 수행하며, t-검정이나 ANOVA와는 다른 목적으로 사용된다.\n검정 적용 시점",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#검정-유형과-적용-시점",
    "href": "part2/04. 적합성 및 독립성 검정.html#검정-유형과-적용-시점",
    "title": "14  적합성 및 독립성 검정",
    "section": "",
    "text": "검정 유형\n핵심 질문\n변수 개수\n비교 대상\n\n\n\n\n적합성 검정\n이 데이터는 내가 기대한 분포를 따르는가?\n1개\n관측 분포 vs 이론 분포\n\n\n독립성 검정\n두 범주형 변수는 서로 관련이 있는가?\n2개\n변수 간 연관성\n\n\n동질성 검정\n여러 집단의 분포가 같은가?\n2개\n집단 간 분포\n\n\n\n\n\n\n범주형 변수의 분포를 이론적 분포와 비교할 때\n두 범주형 변수 간 관계를 탐색할 때\n샘플링이 모집단을 대표하는지 확인할 때\n실험 설계의 균형성을 검증할 때",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#카이제곱χ²-검정의-기본-원리",
    "href": "part2/04. 적합성 및 독립성 검정.html#카이제곱χ²-검정의-기본-원리",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.2 카이제곱(χ²) 검정의 기본 원리",
    "text": "14.2 카이제곱(χ²) 검정의 기본 원리\n모든 카이제곱 검정은 관측 빈도와 기대 빈도의 차이를 측정한다.\n카이제곱 검정 통계량\n\\[\n\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n\\]\n여기서: - \\(O_i\\): 관측 빈도(Observed frequency) - \\(E_i\\): 기대 빈도(Expected frequency) - \\(k\\): 범주의 개수\n검정 통계량의 의미\n\n\n\nχ² 값\n의미\n결론\n\n\n\n\n0에 가까움\n관측값 ≈ 기대값\n귀무가설 지지\n\n\n큼\n관측값 ≠ 기대값\n대립가설 지지\n\n\n\nχ² 값이 클수록 관측 데이터가 기대 분포에서 벗어난 정도가 크다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#적합성-검정-goodness-of-fit-test",
    "href": "part2/04. 적합성 및 독립성 검정.html#적합성-검정-goodness-of-fit-test",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.3 적합성 검정 (Goodness-of-Fit Test)",
    "text": "14.3 적합성 검정 (Goodness-of-Fit Test)\n적합성 검정은 관측된 범주 빈도가 미리 정해진 이론적 분포와 일치하는지 확인하는 검정이다.\n가설 설정\n\nH₀ (귀무가설): 관측 분포 = 기대 분포 (데이터가 이론 분포를 따름)\nH₁ (대립가설): 관측 분포 ≠ 기대 분포 (데이터가 이론 분포를 따르지 않음)\n\n\n14.3.1 균등 분포 검정\n가장 기본적인 적합성 검정은 모든 범주가 동일한 비율로 나타나는지 확인하는 것이다.\n예제: 종 분포가 균등한가?\n\n# 종별 관측 빈도\nobserved = df[\"species\"].value_counts().sort_index()\n\nprint(\"=== 관측 빈도 ===\")\nprint(observed)\nprint(f\"\\n총 개체 수: {observed.sum()}\")\n\n# 균등 분포 기대 빈도\nexpected = [observed.sum() / len(observed)] * len(observed)\n\nprint(\"\\n=== 기대 빈도 (균등 분포) ===\")\nfor species, exp in zip(observed.index, expected):\n    print(f\"{species}: {exp:.2f}\")\n\n=== 관측 빈도 ===\nspecies\nAdelie       146\nChinstrap     68\nGentoo       119\nName: count, dtype: int64\n\n총 개체 수: 333\n\n=== 기대 빈도 (균등 분포) ===\nAdelie: 111.00\nChinstrap: 111.00\nGentoo: 111.00\n\n\n예제: 카이제곱 적합성 검정\n\n# 적합성 검정\nstat, p_value = chisquare(f_obs=observed, f_exp=expected)\n\nprint(\"\\n=== 카이제곱 적합성 검정 ===\")\nprint(f\"χ² 통계량: {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"균등 분포를 따른다고 볼 수 있음 (H₀ 채택)\")\nelse:\n    print(\"균등 분포가 아님 (H₀ 기각)\")\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 관측 빈도\naxes[0].bar(observed.index, observed.values, alpha=0.7, edgecolor='black')\naxes[0].set_title(\"Observed Frequencies\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"Count\")\n\n# 관측 vs 기대 비교\nx = np.arange(len(observed))\nwidth = 0.35\naxes[1].bar(x - width/2, observed.values, width, label='Observed', alpha=0.7)\naxes[1].bar(x + width/2, expected, width, label='Expected (Uniform)', alpha=0.7)\naxes[1].set_xlabel(\"Species\")\naxes[1].set_ylabel(\"Count\")\naxes[1].set_title(\"Observed vs Expected\")\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(observed.index)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n=== 카이제곱 적합성 검정 ===\nχ² 통계량: 28.2703\np-value: 0.0000\n\n유의수준 0.05 기준: 균등 분포가 아님 (H₀ 기각)\n\n\n\n\n\n\n\n\n\n\n\n14.3.2 특정 비율 검정\n이론적으로 특정 비율이 예상되는 경우, 그 비율과 관측 데이터를 비교할 수 있다.\n예제: 특정 비율 검정\n\n# 이론적 비율 설정 (예: 4:3:3)\ntheoretical_ratio = [4, 3, 3]\ntotal = observed.sum()\n\n# 비율에 맞는 기대 빈도 계산\nexpected_ratio = [total * r / sum(theoretical_ratio) for r in theoretical_ratio]\n\nprint(\"=== 특정 비율 검정 (4:3:3) ===\")\nprint(f\"기대 비율: {theoretical_ratio}\")\nprint(f\"\\n기대 빈도:\")\nfor species, exp in zip(observed.index, expected_ratio):\n    print(f\"{species}: {exp:.2f}\")\n\n# 검정\nstat, p_value = chisquare(f_obs=observed, f_exp=expected_ratio)\n\nprint(f\"\\nχ² 통계량: {stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"이론적 비율과 일치 (H₀ 채택)\")\nelse:\n    print(\"이론적 비율과 다름 (H₀ 기각)\")\n\n=== 특정 비율 검정 (4:3:3) ===\n기대 비율: [4, 3, 3]\n\n기대 빈도:\nAdelie: 133.20\nChinstrap: 99.90\nGentoo: 99.90\n\nχ² 통계량: 15.0681\np-value: 0.0005\n\n유의수준 0.05 기준: 이론적 비율과 다름 (H₀ 기각)\n\n\n\n\n14.3.3 적합성 검정의 활용 사례\n적용 분야\n\n\n\n분야\n질문\n예시\n\n\n\n\n품질 관리\n제품 불량률이 기준치인가?\n불량품 5% 예상 vs 실제 관측\n\n\n유전학\n멘델의 법칙을 따르는가?\n3:1 비율 검정\n\n\n게임\n주사위가 공정한가?\n각 면이 1/6 확률\n\n\n샘플링\n표본이 모집단을 대표하는가?\n성별/연령 분포 비교\n\n\n마케팅\n광고 효과가 균등한가?\n요일별 방문자 수",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#독립성-검정-test-of-independence",
    "href": "part2/04. 적합성 및 독립성 검정.html#독립성-검정-test-of-independence",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.4 독립성 검정 (Test of Independence)",
    "text": "14.4 독립성 검정 (Test of Independence)\n독립성 검정은 두 범주형 변수가 서로 독립적인지, 즉 하나의 변수를 알았을 때 다른 변수의 분포가 변하는지를 확인하는 검정이다.\n가설 설정\n\nH₀ (귀무가설): 두 변수는 독립이다 (관련이 없다)\nH₁ (대립가설): 두 변수는 독립이 아니다 (관련이 있다)\n\n독립의 의미\n두 변수 A와 B가 독립이라는 것은 다음을 의미한다.\n\\[\nP(A \\cap B) = P(A) \\times P(B)\n\\]\n즉, A와 B가 동시에 일어날 확률이 각각의 확률의 곱과 같다.\n\n14.4.1 교차표 (Contingency Table)\n독립성 검정은 교차표를 기반으로 수행한다.\n예제: 종과 성별의 교차표\n\n# 교차표 생성\ncontingency_table = pd.crosstab(\n    df[\"species\"],\n    df[\"sex\"],\n    margins=True  # 합계 포함\n)\n\nprint(\"=== 교차표: 종 × 성별 ===\")\nprint(contingency_table)\n\n# 비율로 변환 (행 기준)\ncontingency_pct = pd.crosstab(\n    df[\"species\"],\n    df[\"sex\"],\n    normalize='index'\n) * 100\n\nprint(\"\\n=== 종별 성별 비율 (%) ===\")\nprint(contingency_pct.round(1))\n\n=== 교차표: 종 × 성별 ===\nsex        Female  Male  All\nspecies                     \nAdelie         73    73  146\nChinstrap      34    34   68\nGentoo         58    61  119\nAll           165   168  333\n\n=== 종별 성별 비율 (%) ===\nsex        Female  Male\nspecies                \nAdelie       50.0  50.0\nChinstrap    50.0  50.0\nGentoo       48.7  51.3\n\n\n예제: 교차표 시각화\n\n# 교차표 시각화 (합계 제외)\ncontingency_no_margin = pd.crosstab(df[\"species\"], df[\"sex\"])\n\n# 히트맵\nplt.figure(figsize=(8, 5))\nsns.heatmap(contingency_no_margin, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Contingency Table: Species × Sex\")\nplt.xlabel(\"Sex\")\nplt.ylabel(\"Species\")\nplt.show()\n\n# 그룹화 막대 그래프\ncontingency_no_margin.plot(kind='bar', figsize=(10, 5))\nplt.title(\"Species and Sex Distribution\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Count\")\nplt.xticks(rotation=0)\nplt.legend(title=\"Sex\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.4.2 카이제곱 독립성 검정\n예제: 독립성 검정 수행\n\n# 독립성 검정 (합계 제외)\nchi2, p_value, dof, expected_freq = chi2_contingency(contingency_no_margin)\n\nprint(\"=== 카이제곱 독립성 검정 ===\")\nprint(f\"χ² 통계량: {chi2:.4f}\")\nprint(f\"자유도(df): {dof}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"두 변수는 독립이다 (관련 없음, H₀ 채택)\")\nelse:\n    print(\"두 변수는 독립이 아니다 (관련 있음, H₀ 기각)\")\n\n=== 카이제곱 독립성 검정 ===\nχ² 통계량: 0.0486\n자유도(df): 2\np-value: 0.9760\n\n유의수준 0.05 기준: 두 변수는 독립이다 (관련 없음, H₀ 채택)\n\n\n\n\n14.4.3 기대 빈도 확인\n카이제곱 검정의 신뢰도는 기대 빈도에 의존한다.\n예제: 기대 빈도 확인\n\n# 기대 빈도 출력\nexpected_df = pd.DataFrame(\n    expected_freq,\n    index=contingency_no_margin.index,\n    columns=contingency_no_margin.columns\n)\n\nprint(\"\\n=== 기대 빈도 ===\")\nprint(expected_df.round(2))\n\n# 기대 빈도 &lt; 5인 셀 확인\nlow_expected = (expected_df &lt; 5).sum().sum()\ntotal_cells = expected_df.size\n\nprint(f\"\\n기대 빈도 &lt; 5인 셀: {low_expected}/{total_cells}\")\n\nif low_expected &gt; 0:\n    print(\"⚠️ 주의: 기대 빈도가 5 미만인 셀이 있으면 카이제곱 검정 신뢰도 저하\")\n    print(\"   → 범주 통합 또는 Fisher의 정확 검정 고려\")\nelse:\n    print(\"✓ 모든 셀의 기대 빈도가 5 이상으로 검정 신뢰도 양호\")\n\n\n=== 기대 빈도 ===\nsex        Female   Male\nspecies                 \nAdelie      72.34  73.66\nChinstrap   33.69  34.31\nGentoo      58.96  60.04\n\n기대 빈도 &lt; 5인 셀: 0/6\n✓ 모든 셀의 기대 빈도가 5 이상으로 검정 신뢰도 양호\n\n\n기대 빈도 조건\n\n\n\n조건\n권장 조치\n\n\n\n\n모든 셀의 기대 빈도 ≥ 5\n카이제곱 검정 사용 가능\n\n\n20% 이상의 셀이 기대 빈도 &lt; 5\n범주 통합 또는 Fisher 정확 검정\n\n\n2×2 표에서 기대 빈도 &lt; 5\nYates 연속성 수정 또는 Fisher 정확 검정\n\n\n\n\n\n14.4.4 연관성의 강도 측정\n카이제곱 검정은 연관성의 존재 여부만 알려주므로, 연관성의 강도를 측정하려면 추가 지표가 필요하다.\n예제: Cramér’s V 계산\n\n# Cramér's V 계산\nn = contingency_no_margin.sum().sum()\nmin_dim = min(contingency_no_margin.shape[0] - 1, contingency_no_margin.shape[1] - 1)\ncramers_v = np.sqrt(chi2 / (n * min_dim))\n\nprint(f\"\\n=== 연관성 강도 ===\")\nprint(f\"Cramér's V: {cramers_v:.4f}\")\n\n# 해석\nif cramers_v &lt; 0.1:\n    strength = \"매우 약함\"\nelif cramers_v &lt; 0.3:\n    strength = \"약함\"\nelif cramers_v &lt; 0.5:\n    strength = \"중간\"\nelse:\n    strength = \"강함\"\n\nprint(f\"연관성 강도: {strength}\")\n\n\n=== 연관성 강도 ===\nCramér's V: 0.0121\n연관성 강도: 매우 약함\n\n\nCramér’s V 해석\n\n\n\nCramér’s V\n연관성 강도\n\n\n\n\n0.00 ~ 0.10\n매우 약함\n\n\n0.10 ~ 0.30\n약함\n\n\n0.30 ~ 0.50\n중간\n\n\n0.50 이상\n강함",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#f-검정-분산-비교",
    "href": "part2/04. 적합성 및 독립성 검정.html#f-검정-분산-비교",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.5 F 검정 (분산 비교)",
    "text": "14.5 F 검정 (분산 비교)\nF 검정은 두 집단의 분산이 같은지 확인하는 검정이다. 등분산성 검정의 가장 기본적인 형태이지만, 정규성에 매우 민감하여 실무에서는 Levene 검정을 더 많이 사용한다.\nF 통계량\n\\[\nF = \\frac{s_1^2}{s_2^2}\n\\]\n여기서 \\(s_1^2\\)와 \\(s_2^2\\)는 각 집단의 표본 분산이다.\n예제: F 검정\n\n# 두 종 선택\ngroup1 = df[df[\"species\"] == \"Adelie\"][\"body_mass_g\"].dropna()\ngroup2 = df[df[\"species\"] == \"Chinstrap\"][\"body_mass_g\"].dropna()\n\n# F 통계량 계산\nf_stat = group1.var() / group2.var()\n\nprint(\"=== F 검정: Adelie vs Chinstrap ===\")\nprint(f\"\\nAdelie 분산: {group1.var():.2f}\")\nprint(f\"Chinstrap 분산: {group2.var():.2f}\")\nprint(f\"\\nF 통계량: {f_stat:.4f}\")\n\n# 자유도\ndf1 = len(group1) - 1\ndf2 = len(group2) - 1\n\n# p-value 계산 (양측 검정)\np_value = 2 * min(f.cdf(f_stat, df1, df2), 1 - f.cdf(f_stat, df1, df2))\n\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"\\n유의수준 0.05 기준: \", end=\"\")\nif p_value &gt; 0.05:\n    print(\"등분산 (H₀ 채택)\")\nelse:\n    print(\"이분산 (H₀ 기각)\")\n\nprint(\"\\n⚠️ 주의: F 검정은 정규성에 매우 민감\")\nprint(\"   → 실무에서는 Levene 검정 권장\")\n\n=== F 검정: Adelie vs Chinstrap ===\n\nAdelie 분산: 210332.43\nChinstrap 분산: 147713.45\n\nF 통계량: 1.4239\np-value: 0.1047\n\n유의수준 0.05 기준: 등분산 (H₀ 채택)\n\n⚠️ 주의: F 검정은 정규성에 매우 민감\n   → 실무에서는 Levene 검정 권장\n\n\nF 검정 vs Levene 검정\n\n\n\n항목\nF 검정\nLevene 검정\n\n\n\n\n가정\n정규성 필요\n정규성 불필요\n\n\n강건성\n정규성 위배 시 부정확\n안정적\n\n\n용도\n이론 설명용\n실무 권장",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#검정-방법-종합-비교",
    "href": "part2/04. 적합성 및 독립성 검정.html#검정-방법-종합-비교",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.6 검정 방법 종합 비교",
    "text": "14.6 검정 방법 종합 비교\n검정 유형별 정리\n\n\n\n목적\n데이터 유형\n변수 개수\n검정 방법\n주요 사용\n\n\n\n\n분포 적합성\n범주형\n1개\nχ² 적합성 검정\n이론 분포와 비교\n\n\n변수 관계\n범주형 × 범주형\n2개\nχ² 독립성 검정\n범주 간 연관성\n\n\n분산 비교 (이론)\n연속형\n2개\nF 검정\n교육 목적\n\n\n분산 비교 (실무)\n연속형\n2개 이상\nLevene 검정\n등분산성 검정\n\n\n평균 비교\n연속형\n2개\nt-검정\n그룹 간 평균 차이\n\n\n평균 비교\n연속형\n3개 이상\nANOVA\n다집단 평균 비교",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#자주-하는-오해와-주의사항",
    "href": "part2/04. 적합성 및 독립성 검정.html#자주-하는-오해와-주의사항",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.7 자주 하는 오해와 주의사항",
    "text": "14.7 자주 하는 오해와 주의사항\n흔한 오해\n\n\n\n\n\n\n\n오해\n진실\n\n\n\n\n“p &lt; 0.05이면 강한 관계”\np-value는 관계의 존재 여부만 판단, 강도는 별도 측정 필요\n\n\n“카이제곱은 평균을 비교한다”\n카이제곱은 빈도를 비교하는 검정\n\n\n“카이제곱은 모든 데이터에 적용 가능”\n표본 크기와 기대 빈도 조건이 중요\n\n\n“독립성 검정에서 귀무가설 채택 = 변수가 독립”\n귀무가설 기각 실패 ≠ 독립 증명\n\n\n\n주의사항\n\n기대 빈도 확인: 기대 빈도가 5 미만인 셀이 많으면 검정 신뢰도 저하\n표본 크기: 표본이 너무 크면 실무적으로 의미 없는 차이도 유의하게 나옴\n연관성 강도: p-value만으로 연관성 강도를 판단하지 말 것\n인과관계: 독립성 검정은 상관관계만 확인, 인과관계는 아님\n다중 비교: 여러 검정을 동시에 수행하면 1종 오류 증가",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/04. 적합성 및 독립성 검정.html#요약",
    "href": "part2/04. 적합성 및 독립성 검정.html#요약",
    "title": "14  적합성 및 독립성 검정",
    "section": "14.8 요약",
    "text": "14.8 요약\n이 장에서는 범주형 데이터의 빈도와 구조를 분석하는 카이제곱 검정을 학습했다. 주요 내용은 다음과 같다.\n검정 유형별 요약\n\n\n\n\n\n\n\n\n\n검정\n목적\n비교 대상\n주요 사용\n\n\n\n\n적합성 검정\n분포가 이론과 일치하는가?\n관측 분포 vs 이론 분포\n샘플링 검증, 품질 관리\n\n\n독립성 검정\n두 변수가 관련 있는가?\n변수 간 연관성\n변수 관계 탐색\n\n\nF 검정\n두 분산이 같은가?\n집단 간 분산\n이론 학습 (실무는 Levene)\n\n\n\n카이제곱 검정 핵심\n\n원리: 관측 빈도와 기대 빈도의 차이를 측정\n통계량: \\(\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\)\n조건: 기대 빈도가 모든 셀에서 5 이상 권장\n해석: p-value는 관계 존재 여부, 강도는 Cramér’s V 등으로 측정\n\n실무 체크리스트\n\n범주형 데이터인가?\n표본 크기가 충분한가?\n기대 빈도가 5 이상인가?\n검정 목적이 명확한가? (적합성 vs 독립성)\n연관성 강도도 측정했는가?\n결과를 시각화했는가?\n\n다음 단계\n\n적합성 검정 → 분포의 이론적 타당성 확인\n독립성 검정 → 변수 간 관계 파악 후 추가 분석\n평균 비교 전 → 구조적 패턴 먼저 확인\n\n카이제곱 검정은 범주형 데이터 분석의 기초이다. 빈도와 구조를 먼저 파악한 후, 필요에 따라 평균 비교나 회귀 분석 등의 고급 기법으로 진행하는 것이 바람직하다. 다음 장에서는 평균 비교를 위한 t-검정과 ANOVA를 학습할 것이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>적합성 및 독립성 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html",
    "href": "part2/05. 평균 비교 검정.html",
    "title": "15  평균 비교 검정",
    "section": "",
    "text": "15.1 평균 비교 검정의 개념\n평균 비교 검정(Mean Comparison Test)은 두 개 이상 집단 간 평균 차이가 단순한 우연인지 통계적으로 유의한지를 판단하는 방법이다. 범주형 변수로 집단을 나누고 연속형 변수의 평균을 비교한다. 이는 실무에서 가장 흔하게 사용되는 통계 기법 중 하나로, 약물 효과, 마케팅 전략, 제품 품질 등 다양한 분야에서 집단 간 차이를 과학적으로 검증하는 데 사용된다. 이 장에서는 t-검정과 ANOVA, 그리고 사후검정을 학습한다.\n예제: 데이터 로드\n평균 비교 검정은 집단 간 평균의 차이가 우연에 의한 것인지, 실제로 의미 있는 차이인지를 통계적으로 판단한다.\n평균 비교 검정의 예시",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#평균-비교-검정의-개념",
    "href": "part2/05. 평균 비교 검정.html#평균-비교-검정의-개념",
    "title": "15  평균 비교 검정",
    "section": "",
    "text": "질문\n독립 변수 (범주형)\n종속 변수 (연속형)\n검정 방법\n\n\n\n\n성별에 따라 몸무게가 다른가?\n성별 (2그룹)\n몸무게\n독립표본 t-검정\n\n\n종에 따라 부리 길이가 다른가?\n종 (3그룹)\n부리 길이\nANOVA\n\n\n약물 투여 전후 혈압이 달라졌는가?\n시점 (전/후)\n혈압\n대응표본 t-검정\n\n\n교육 방법별 시험 점수가 다른가?\n교육 방법 (4그룹)\n시험 점수\nANOVA\n\n\n\n\n15.1.1 검정 방법 선택 기준\n평균 비교 검정 종류\n\n\n\n\n\n\n\n\n\n\n상황\n집단 수\n표본 관계\n사용 검정\n조건\n\n\n\n\n두 독립 집단 비교\n2\n독립\n독립표본 t-검정\n정규성, 등분산성\n\n\n두 독립 집단 (등분산 X)\n2\n독립\nWelch t-검정\n정규성\n\n\n동일 대상 전후 비교\n2\n대응\n대응표본 t-검정\n차이값의 정규성\n\n\n세 집단 이상 비교\n3+\n독립\n일원분산분석(ANOVA)\n정규성, 등분산성\n\n\n\n\n\n15.1.2 평균 비교 전 필수 가정 확인\n평균 비교 검정은 다음 가정들을 전제로 한다.\n검정 가정\n\n\n\n\n\n\n\n\n\n가정\n내용\n확인 방법\n위배 시 대응\n\n\n\n\n1. 정규성\n각 집단의 데이터가 정규분포를 따름\nShapiro-Wilk, Q-Q plot\n비모수 검정, 변환\n\n\n2. 등분산성\n집단 간 분산이 동일함\nLevene 검정\nWelch 검정, Games-Howell\n\n\n3. 독립성\n각 관측치가 서로 독립적\n실험 설계 확인\n혼합 모델, 반복측정 ANOVA\n\n\n\n가정 위배 시 강건성\n\n표본 크기가 충분히 크면(n ≥ 30) 중심극한정리에 의해 정규성 가정이 완화됨\n등분산성은 표본 크기가 집단 간 유사하면 어느 정도 강건함\n독립성은 절대 위배되어서는 안 되는 가정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#두-집단-평균-비교-독립표본-t-검정",
    "href": "part2/05. 평균 비교 검정.html#두-집단-평균-비교-독립표본-t-검정",
    "title": "15  평균 비교 검정",
    "section": "15.2 두 집단 평균 비교: 독립표본 t-검정",
    "text": "15.2 두 집단 평균 비교: 독립표본 t-검정\n독립표본 t-검정(Independent Samples t-test)은 서로 독립된 두 집단의 평균을 비교하는 가장 기본적인 검정이다.\n가설 설정\n\nH₀ (귀무가설): μ₁ = μ₂ (두 집단의 모평균이 같다)\nH₁ (대립가설): μ₁ ≠ μ₂ (두 집단의 모평균이 다르다)\n\nt-통계량\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\]\n여기서 \\(s_p\\)는 합동 표준편차(pooled standard deviation)이다.\n\n15.2.1 예제: 성별에 따른 체중 비교\n예제: 데이터 준비 및 탐색\n\nfrom scipy.stats import ttest_ind\n\n# 필요한 열만 선택 및 결측치 제거\ndf_t = df[[\"sex\", \"body_mass_g\"]].dropna()\n\nprint(\"=== 집단별 기술 통계량 ===\")\nsummary = df_t.groupby(\"sex\")[\"body_mass_g\"].agg(['count', 'mean', 'std', 'min', 'max'])\nprint(summary)\n\n# 집단 분리\nmale = df_t[df_t[\"sex\"] == \"Male\"][\"body_mass_g\"]\nfemale = df_t[df_t[\"sex\"] == \"Female\"][\"body_mass_g\"]\n\nprint(f\"\\n남성 표본 크기: {len(male)}\")\nprint(f\"여성 표본 크기: {len(female)}\")\n\n=== 집단별 기술 통계량 ===\n        count         mean         std     min     max\nsex                                                   \nFemale    165  3862.272727  666.172050  2700.0  5200.0\nMale      168  4545.684524  787.628884  3250.0  6300.0\n\n남성 표본 크기: 168\n여성 표본 크기: 165\n\n\n예제: 가정 확인\n\n# 1. 정규성 검정\nfrom scipy.stats import shapiro, levene\n\nprint(\"\\n=== 정규성 검정 (Shapiro-Wilk) ===\")\n_, p_male = shapiro(male)\n_, p_female = shapiro(female)\nprint(f\"남성: p = {p_male:.4f} {'(정규)' if p_male &gt; 0.05 else '(비정규)'}\")\nprint(f\"여성: p = {p_female:.4f} {'(정규)' if p_female &gt; 0.05 else '(비정규)'}\")\n\n# 2. 등분산성 검정\nprint(\"\\n=== 등분산성 검정 (Levene) ===\")\n_, p_levene = levene(male, female)\nprint(f\"p = {p_levene:.4f} {'(등분산)' if p_levene &gt; 0.05 else '(이분산)'}\")\n\n\n=== 정규성 검정 (Shapiro-Wilk) ===\n남성: p = 0.0000 (비정규)\n여성: p = 0.0000 (비정규)\n\n=== 등분산성 검정 (Levene) ===\np = 0.0143 (이분산)\n\n\n예제: 독립표본 t-검정\n\n# 독립표본 t-검정 (등분산 가정)\nt_stat, p_value = ttest_ind(male, female, equal_var=True)\n\nprint(\"\\n=== 독립표본 t-검정 (Student) ===\")\nprint(f\"t-통계량: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"자유도: {len(male) + len(female) - 2}\")\n\n# 결과 해석\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(f\"✓ 귀무가설 기각: 성별에 따른 체중 차이가 유의함\")\n    print(f\"  평균 차이: {male.mean() - female.mean():.2f}g\")\nelse:\n    print(f\"✗ 귀무가설 채택: 성별에 따른 체중 차이가 유의하지 않음\")\n\n\n=== 독립표본 t-검정 (Student) ===\nt-통계량: 8.5417\np-value: 0.0000\n자유도: 331\n\n유의수준 0.05 기준:\n✓ 귀무가설 기각: 성별에 따른 체중 차이가 유의함\n  평균 차이: 683.41g\n\n\n예제: 시각화\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 박스플롯\nsns.boxplot(x=\"sex\", y=\"body_mass_g\", data=df_t, ax=axes[0])\naxes[0].set_title(\"Body Mass by Sex\")\naxes[0].set_xlabel(\"Sex\")\naxes[0].set_ylabel(\"Body Mass (g)\")\n\n# 히스토그램\naxes[1].hist(male, alpha=0.5, label=f'Male (n={len(male)})', bins=20, edgecolor='black')\naxes[1].hist(female, alpha=0.5, label=f'Female (n={len(female)})', bins=20, edgecolor='black')\naxes[1].axvline(male.mean(), color='blue', linestyle='--', linewidth=2, label=f'Male mean')\naxes[1].axvline(female.mean(), color='orange', linestyle='--', linewidth=2, label=f'Female mean')\naxes[1].set_title(\"Distribution of Body Mass\")\naxes[1].set_xlabel(\"Body Mass (g)\")\naxes[1].set_ylabel(\"Frequency\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n15.2.2 효과 크기 (Effect Size)\np-value는 차이의 존재 여부만 알려주므로, 차이의 크기를 측정하는 효과 크기를 함께 확인해야 한다.\n예제: Cohen’s d 계산\n\n# Cohen's d 계산\npooled_std = np.sqrt(((len(male)-1)*male.std()**2 + (len(female)-1)*female.std()**2) / \n                     (len(male) + len(female) - 2))\ncohens_d = (male.mean() - female.mean()) / pooled_std\n\nprint(f\"\\n=== 효과 크기 ===\")\nprint(f\"Cohen's d: {cohens_d:.4f}\")\n\n# 해석\nif abs(cohens_d) &lt; 0.2:\n    effect = \"매우 작음\"\nelif abs(cohens_d) &lt; 0.5:\n    effect = \"작음\"\nelif abs(cohens_d) &lt; 0.8:\n    effect = \"중간\"\nelse:\n    effect = \"큼\"\n\nprint(f\"효과 크기: {effect}\")\n\n\n=== 효과 크기 ===\nCohen's d: 0.9362\n효과 크기: 큼\n\n\nCohen’s d 해석 기준\n\n\n\n\nd\n\n\n\n\n0.2 미만\n매우 작음\n\n\n0.2 ~ 0.5\n작음\n\n\n0.5 ~ 0.8\n중간\n\n\n0.8 이상\n큼",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#welch-t-검정-등분산-가정-불필요",
    "href": "part2/05. 평균 비교 검정.html#welch-t-검정-등분산-가정-불필요",
    "title": "15  평균 비교 검정",
    "section": "15.3 Welch t-검정 (등분산 가정 불필요)",
    "text": "15.3 Welch t-검정 (등분산 가정 불필요)\n등분산 가정이 의심되면 Welch t-검정을 사용한다. 이는 등분산을 가정하지 않는 t-검정으로, 더 보수적인 결과를 제공한다.\n예제: Welch t-검정\n\n# Welch t-검정 (등분산 가정 불필요)\nt_stat_welch, p_value_welch = ttest_ind(male, female, equal_var=False)\n\nprint(\"\\n=== Welch t-검정 ===\")\nprint(f\"t-통계량: {t_stat_welch:.4f}\")\nprint(f\"p-value: {p_value_welch:.4f}\")\n\n# Student vs Welch 비교\nprint(\"\\n=== t-검정 비교 ===\")\nprint(f\"Student t-test: t = {t_stat:.4f}, p = {p_value:.4f}\")\nprint(f\"Welch t-test:   t = {t_stat_welch:.4f}, p = {p_value_welch:.4f}\")\n\n\n=== Welch t-검정 ===\nt-통계량: 8.5545\np-value: 0.0000\n\n=== t-검정 비교 ===\nStudent t-test: t = 8.5417, p = 0.0000\nWelch t-test:   t = 8.5545, p = 0.0000\n\n\nStudent vs Welch 선택 기준\n\n등분산성 만족 → Student t-검정 (검정력 높음)\n등분산성 의심 → Welch t-검정 (안전)\n확신 없음 → Welch t-검정 (기본 선택 권장)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#세-집단-이상-비교-일원분산분석-anova",
    "href": "part2/05. 평균 비교 검정.html#세-집단-이상-비교-일원분산분석-anova",
    "title": "15  평균 비교 검정",
    "section": "15.4 세 집단 이상 비교: 일원분산분석 (ANOVA)",
    "text": "15.4 세 집단 이상 비교: 일원분산분석 (ANOVA)\n일원분산분석(One-way ANOVA)은 세 개 이상 집단의 평균을 동시에 비교한다. 여러 번의 t-검정 대신 ANOVA를 사용하는 이유는 다중 비교로 인한 1종 오류 증가를 방지하기 위해서다.\n가설 설정\n\nH₀ (귀무가설): μ₁ = μ₂ = μ₃ = ⋯ = μₖ (모든 집단의 모평균이 같다)\nH₁ (대립가설): 적어도 하나의 모평균이 다르다\n\nF-통계량\n\\[\nF = \\frac{\\text{집단 간 분산}}{\\text{집단 내 분산}} = \\frac{MSB}{MSW}\n\\]\nF 값이 클수록 집단 간 차이가 집단 내 변동에 비해 크다는 의미이다.\n\n15.4.1 예제: 종별 부리 길이 비교\n예제: 데이터 준비 및 탐색\n\nfrom scipy.stats import f_oneway\n\n# 필요한 열 선택 및 결측치 제거\ndf_a = df[[\"species\", \"bill_length_mm\"]].dropna()\n\nprint(\"=== 종별 부리 길이 기술 통계량 ===\")\nsummary = df_a.groupby(\"species\")[\"bill_length_mm\"].agg(['count', 'mean', 'std'])\nprint(summary)\n\n# 집단별 데이터 분리\nadelie = df_a[df_a[\"species\"] == \"Adelie\"][\"bill_length_mm\"]\nchinstrap = df_a[df_a[\"species\"] == \"Chinstrap\"][\"bill_length_mm\"]\ngentoo = df_a[df_a[\"species\"] == \"Gentoo\"][\"bill_length_mm\"]\n\n=== 종별 부리 길이 기술 통계량 ===\n           count       mean       std\nspecies                              \nAdelie       151  38.791391  2.663405\nChinstrap     68  48.833824  3.339256\nGentoo       123  47.504878  3.081857\n\n\n예제: 가정 확인\n\n# 1. 각 집단의 정규성 검정\nprint(\"\\n=== 정규성 검정 ===\")\nfor species, data in [(\"Adelie\", adelie), (\"Chinstrap\", chinstrap), (\"Gentoo\", gentoo)]:\n    _, p = shapiro(data)\n    print(f\"{species}: p = {p:.4f} {'(정규)' if p &gt; 0.05 else '(비정규)'}\")\n\n# 2. 등분산성 검정\nprint(\"\\n=== 등분산성 검정 (Levene) ===\")\n_, p_levene = levene(adelie, chinstrap, gentoo)\nprint(f\"p = {p_levene:.4f} {'(등분산)' if p_levene &gt; 0.05 else '(이분산)'}\")\n\n\n=== 정규성 검정 ===\nAdelie: p = 0.7166 (정규)\nChinstrap: p = 0.1941 (정규)\nGentoo: p = 0.0135 (비정규)\n\n=== 등분산성 검정 (Levene) ===\np = 0.1078 (등분산)\n\n\n예제: 일원분산분석\n\n# ANOVA 수행\nf_stat, p_value = f_oneway(adelie, chinstrap, gentoo)\n\nprint(\"\\n=== 일원분산분석 (One-way ANOVA) ===\")\nprint(f\"F-통계량: {f_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# 결과 해석\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(f\"✓ 귀무가설 기각: 종에 따른 부리 길이 차이가 유의함\")\n    print(f\"  → 사후검정(Post-hoc test) 필요\")\nelse:\n    print(f\"✗ 귀무가설 채택: 종에 따른 부리 길이 차이가 유의하지 않음\")\n\n\n=== 일원분산분석 (One-way ANOVA) ===\nF-통계량: 410.6003\np-value: 0.0000\n\n유의수준 0.05 기준:\n✓ 귀무가설 기각: 종에 따른 부리 길이 차이가 유의함\n  → 사후검정(Post-hoc test) 필요\n\n\n예제: 시각화\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 박스플롯\nsns.boxplot(x=\"species\", y=\"bill_length_mm\", data=df_a, ax=axes[0])\naxes[0].set_title(\"Bill Length by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"Bill Length (mm)\")\n\n# 바이올린 플롯\nsns.violinplot(x=\"species\", y=\"bill_length_mm\", data=df_a, ax=axes[1])\naxes[1].set_title(\"Bill Length Distribution by Species\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_ylabel(\"Bill Length (mm)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n15.4.2 ANOVA의 한계와 사후검정 필요성\nANOVA는 “적어도 하나의 평균이 다르다”는 것만 알려주고, 구체적으로 어떤 집단 간 차이가 있는지는 알려주지 않는다. 이를 확인하기 위해 사후검정(Post-hoc Test)이 필요하다.\n다중 비교 문제\n여러 번의 t-검정을 반복하면 1종 오류(α)가 누적되어 증가한다.\n\n3개 집단: 3번 비교 → 전체 α ≈ 0.14\n4개 집단: 6번 비교 → 전체 α ≈ 0.26\n5개 집단: 10번 비교 → 전체 α ≈ 0.40\n\n사후검정은 이러한 다중 비교 문제를 보정한다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#대응표본-t-검정",
    "href": "part2/05. 평균 비교 검정.html#대응표본-t-검정",
    "title": "15  평균 비교 검정",
    "section": "15.5 대응표본 t-검정",
    "text": "15.5 대응표본 t-검정\n대응표본 t-검정(Paired Samples t-test)은 동일한 대상의 전후 변화를 비교한다. 각 대상의 차이값(전 - 후)을 계산하고, 이 차이값의 평균이 0인지 검정한다.\n가설 설정\n\nH₀ (귀무가설): μd = 0 (전후 평균 차이가 0이다)\nH₁ (대립가설): μd ≠ 0 (전후 평균 차이가 0이 아니다)\n\n예제 구조 (개념적)\nfrom scipy.stats import ttest_rel\n\n# 전후 데이터 (예시)\n# before = [120, 125, 130, 135, 128]  # 처치 전 혈압\n# after = [115, 120, 125, 130, 122]   # 처치 후 혈압\n\n# 대응표본 t-검정\n# t_stat, p_value = ttest_rel(before, after)\n\n# print(f\"t-통계량: {t_stat:.4f}\")\n# print(f\"p-value: {p_value:.4f}\")\n# \n# if p_value &lt; 0.05:\n#     print(\"처치 효과가 유의함\")\n# else:\n#     print(\"처치 효과가 유의하지 않음\")\n대응표본 vs 독립표본\n\n\n\n구분\n대응표본 t-검정\n독립표본 t-검정\n\n\n\n\n데이터 구조\n동일 대상 전후\n서로 다른 대상\n\n\n예시\n약물 투여 전후 혈압\n남성 vs 여성 체중\n\n\n장점\n개인차 통제, 검정력 높음\n구현 간단\n\n\n가정\n차이값의 정규성\n각 집단의 정규성, 등분산성",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#사후검정-post-hoc-test",
    "href": "part2/05. 평균 비교 검정.html#사후검정-post-hoc-test",
    "title": "15  평균 비교 검정",
    "section": "15.6 사후검정 (Post-hoc Test)",
    "text": "15.6 사후검정 (Post-hoc Test)\nANOVA에서 유의한 결과가 나오면, 구체적으로 어떤 집단 간 차이가 있는지 확인하기 위해 사후검정을 수행한다.\n\n15.6.1 대표적인 사후검정 방법\n사후검정 비교\n\n\n\n방법\n특징\n보수성\n가정\n사용 상황\n\n\n\n\nTukey HSD\n가장 많이 사용\n중간\n등분산\nANOVA 후 표준 선택\n\n\nBonferroni\n매우 보수적\n높음\n등분산\n비교 횟수 적을 때\n\n\nScheffé\n매우 보수적\n매우 높음\n등분산\n모든 선형 조합 비교\n\n\nGames-Howell\n등분산 불필요\n중간\n정규성만\n이분산 시 사용\n\n\nDunnett\n대조군 비교\n중간\n등분산\n하나의 대조군과 비교\n\n\n\n\n\n15.6.2 Tukey HSD (Honestly Significant Difference)\nTukey HSD는 ANOVA 이후 가장 표준적인 사후검정으로, 모든 집단 쌍의 평균 차이를 전체 유의수준을 유지하면서 비교한다.\n예제: Tukey HSD 수행\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# Tukey HSD 사후검정\ntukey = pairwise_tukeyhsd(\n    endog=df_a[\"bill_length_mm\"],   # 비교할 연속형 변수\n    groups=df_a[\"species\"],          # 집단 변수\n    alpha=0.05                        # 유의수준\n)\n\nprint(\"=== Tukey HSD 사후검정 ===\")\nprint(tukey)\n\n=== Tukey HSD 사후검정 ===\n   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n=========================================================\n  group1    group2  meandiff p-adj   lower  upper  reject\n---------------------------------------------------------\n   Adelie Chinstrap  10.0424    0.0  9.0249  11.06   True\n   Adelie    Gentoo   8.7135    0.0  7.8672 9.5598   True\nChinstrap    Gentoo  -1.3289 0.0089 -2.3819 -0.276   True\n---------------------------------------------------------\n\n\n출력 결과 해석\n\n\n\n열\n의미\n\n\n\n\ngroup1, group2\n비교되는 두 집단\n\n\nmeandiff\n평균 차이 (group1 - group2)\n\n\np-adj\n보정된 p-value (다중 비교 보정)\n\n\nlower, upper\n평균 차이의 95% 신뢰구간\n\n\nreject\n귀무가설 기각 여부 (True = 유의한 차이)\n\n\n\n예제: 사후검정 시각화\n\n# Tukey HSD 결과 시각화\ntukey.plot_simultaneous()\nplt.title(\"Tukey HSD: Simultaneous Confidence Intervals\")\nplt.show()\n\n# 박스플롯과 함께 비교\nplt.figure(figsize=(10, 6))\nsns.boxplot(x=\"species\", y=\"bill_length_mm\", data=df_a)\nplt.title(\"Bill Length by Species with Post-hoc Results\")\nplt.xlabel(\"Species\")\nplt.ylabel(\"Bill Length (mm)\")\n\n# 유의한 차이가 있는 쌍에 별표 표시 (수동)\n# (실제로는 자동화된 시각화 패키지 사용 권장)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15.6.3 Bonferroni 보정\nBonferroni 보정은 가장 보수적인 방법으로, 각 비교의 유의수준을 비교 횟수로 나눈다.\n\\[\n\\alpha_{\\text{adjusted}} = \\frac{\\alpha}{k}\n\\]\n여기서 k는 비교 횟수이다.\n예제: Bonferroni 보정 수동 적용\n\n# 3개 집단의 모든 쌍 비교 (3C2 = 3번)\ncomparisons = [\n    (\"Adelie\", \"Chinstrap\"),\n    (\"Adelie\", \"Gentoo\"),\n    (\"Chinstrap\", \"Gentoo\")\n]\n\n# Bonferroni 보정 유의수준\nalpha = 0.05\nk = len(comparisons)\nalpha_bonf = alpha / k\n\nprint(f\"\\n=== Bonferroni 보정 ===\")\nprint(f\"원래 유의수준: {alpha}\")\nprint(f\"비교 횟수: {k}\")\nprint(f\"보정된 유의수준: {alpha_bonf:.4f}\")\n\nprint(\"\\n개별 t-검정 결과:\")\nfor sp1, sp2 in comparisons:\n    group1 = df_a[df_a[\"species\"] == sp1][\"bill_length_mm\"]\n    group2 = df_a[df_a[\"species\"] == sp2][\"bill_length_mm\"]\n    \n    t_stat, p_val = ttest_ind(group1, group2)\n    \n    print(f\"\\n{sp1} vs {sp2}:\")\n    print(f\"  p-value: {p_val:.4f}\")\n    print(f\"  결과: \", end=\"\")\n    if p_val &lt; alpha_bonf:\n        print(\"유의한 차이 (Bonferroni 보정 후)\")\n    else:\n        print(\"유의하지 않음 (Bonferroni 보정 후)\")\n\n\n=== Bonferroni 보정 ===\n원래 유의수준: 0.05\n비교 횟수: 3\n보정된 유의수준: 0.0167\n\n개별 t-검정 결과:\n\nAdelie vs Chinstrap:\n  p-value: 0.0000\n  결과: 유의한 차이 (Bonferroni 보정 후)\n\nAdelie vs Gentoo:\n  p-value: 0.0000\n  결과: 유의한 차이 (Bonferroni 보정 후)\n\nChinstrap vs Gentoo:\n  p-value: 0.0062\n  결과: 유의한 차이 (Bonferroni 보정 후)\n\n\n\n\n15.6.4 Games-Howell (등분산 불필요)\n등분산 가정이 위배된 경우 Games-Howell 검정을 사용한다. 이는 Welch t-검정의 다중 비교 버전이다.\n# Games-Howell은 pingouin 라이브러리 사용 권장\n# import pingouin as pg\n# \n# games_howell = pg.pairwise_gameshowell(\n#     data=df_a,\n#     dv=\"bill_length_mm\",\n#     between=\"species\"\n# )\n# \n# print(games_howell)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/05. 평균 비교 검정.html#요약",
    "href": "part2/05. 평균 비교 검정.html#요약",
    "title": "15  평균 비교 검정",
    "section": "15.7 요약",
    "text": "15.7 요약\n이 장에서는 집단 간 평균을 비교하는 다양한 검정 방법을 학습했다. 주요 내용은 다음과 같다.\n평균 비교 검정 종합\n\n\n\n상황\n집단 수\n표본 관계\n등분산 가정\n검정 방법\n\n\n\n\n두 집단 비교\n2\n독립\n만족\nStudent t-검정\n\n\n두 집단 비교\n2\n독립\n위배\nWelch t-검정\n\n\n동일 대상 전후\n2\n대응\n-\n대응표본 t-검정\n\n\n다집단 비교\n3+\n독립\n만족\nANOVA + Tukey\n\n\n다집단 비교\n3+\n독립\n위배\nANOVA + Games-Howell\n\n\n\n분석 흐름\n\n가정 확인: 정규성(Shapiro-Wilk), 등분산성(Levene)\n적절한 검정 선택: 집단 수, 표본 관계, 가정 만족 여부\n검정 수행: t-검정 또는 ANOVA\n효과 크기 측정: Cohen’s d 또는 η² (eta-squared)\n사후검정 (ANOVA만): Tukey HSD 또는 Games-Howell\n시각화: 박스플롯, 바이올린 플롯\n\n핵심 포인트\n\np-value의 의미: 차이의 존재 여부만 판단, 크기나 중요도는 별도 측정\n효과 크기 필수: p-value와 함께 Cohen’s d 등 효과 크기 보고\n다중 비교 보정: 여러 비교 시 사후검정으로 1종 오류 통제\n가정 점검: 정규성과 등분산성 확인 후 적절한 방법 선택\n시각화 중요: 통계적 결과와 함께 분포를 시각적으로 확인\n\n주의사항\n\n통계적 유의성 ≠ 실무적 중요성\n표본 크기가 크면 작은 차이도 유의하게 나올 수 있음\n가정 위배 시 적절한 대안 검정 사용\n사후검정은 ANOVA가 유의할 때만 수행\n결과 해석 시 도메인 지식 함께 고려\n\n평균 비교 검정은 데이터 분석에서 가장 기본이 되는 기법이다. 적절한 검정을 선택하고 가정을 확인하며, 결과를 올바르게 해석하는 것이 중요하다. 다음 장에서는 변수 간 관계를 분석하는 상관분석과 회귀분석을 학습할 것이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>평균 비교 검정</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html",
    "href": "part2/06. 분산 분석.html",
    "title": "16  분산 분석",
    "section": "",
    "text": "16.1 분산분석의 기본 개념\n분산분석(ANOVA: Analysis of Variance)은 세 개 이상 집단의 평균을 동시에 비교하는 통계적 방법이다. 이름은 분산분석이지만 실제 관심 대상은 집단 간 평균 차이이며, 분산을 이용하여 평균 차이를 검정한다는 점에서 이러한 명칭이 붙었다. ANOVA는 t-검정을 여러 번 반복하는 대신 전체 유의수준을 유지하면서 다집단을 비교할 수 있어 실무에서 매우 중요하다. 이 장에서는 일원분산분석, 이원분산분석, 그리고 사후검정을 상세히 학습한다.\n예제: 데이터 로드\n분산분석은 전체 변동을 집단 간 변동과 집단 내 변동으로 분해하고, 집단 간 변동이 집단 내 변동에 비해 충분히 큰지를 검정한다.\n분산분석의 핵심 원리\n\\[\n\\text{총 변동} = \\text{집단 간 변동} + \\text{집단 내 변동}\n\\]\n\\[\nSST = SSB + SSW\n\\]\nF-통계량\n\\[\nF = \\frac{MSB}{MSW} = \\frac{SSB/(k-1)}{SSW/(N-k)}\n\\]\nF 값이 클수록 집단 간 차이가 집단 내 변동에 비해 크다는 의미이다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#일원분산분석-one-way-anova",
    "href": "part2/06. 분산 분석.html#일원분산분석-one-way-anova",
    "title": "16  분산 분석",
    "section": "16.2 일원분산분석 (One-way ANOVA)",
    "text": "16.2 일원분산분석 (One-way ANOVA)\n일원분산분석은 하나의 범주형 독립변수(요인)가 연속형 종속변수에 미치는 영향을 검정한다.\n가설 설정\n\nH₀: μ₁ = μ₂ = μ₃ = ⋯ = μₖ\nH₁: 적어도 하나의 모평균이 다르다\n\n예제: 데이터 준비\n\ndf_anova = df[[\"species\", \"bill_length_mm\"]].dropna()\n\nprint(\"=== 집단별 기술 통계량 ===\")\nprint(df_anova.groupby(\"species\")[\"bill_length_mm\"].describe())\n\n=== 집단별 기술 통계량 ===\n           count       mean       std   min    25%    50%     75%   max\nspecies                                                                \nAdelie     151.0  38.791391  2.663405  32.1  36.75  38.80  40.750  46.0\nChinstrap   68.0  48.833824  3.339256  40.9  46.35  49.55  51.075  58.0\nGentoo     123.0  47.504878  3.081857  40.9  45.30  47.30  49.550  59.6\n\n\n예제: 일원 ANOVA\n\nfrom scipy.stats import f_oneway\n\ngroups = [\n    df_anova[df_anova[\"species\"] == sp][\"bill_length_mm\"]\n    for sp in df_anova[\"species\"].unique()\n]\n\nf_stat, p_value = f_oneway(*groups)\n\nprint(\"\\n=== 일원분산분석 ===\")\nprint(f\"F-통계량: {f_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nif p_value &lt; 0.05:\n    print(\"✓ 종에 따른 부리 길이 차이가 유의함 → 사후검정 필요\")\n\n\n=== 일원분산분석 ===\nF-통계량: 410.6003\np-value: 0.0000\n✓ 종에 따른 부리 길이 차이가 유의함 → 사후검정 필요",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#이원분산분석-two-way-anova",
    "href": "part2/06. 분산 분석.html#이원분산분석-two-way-anova",
    "title": "16  분산 분석",
    "section": "16.3 이원분산분석 (Two-way ANOVA)",
    "text": "16.3 이원분산분석 (Two-way ANOVA)\n이원분산분석은 두 개의 범주형 독립변수와 그들의 상호작용 효과를 동시에 검정한다.\n예제: 이원 ANOVA\n\ndf_two = df[[\"species\", \"sex\", \"bill_length_mm\"]].dropna()\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nmodel = ols(\n    \"bill_length_mm ~ C(species) + C(sex) + C(species):C(sex)\",\n    data=df_two\n).fit()\n\nanova_table = sm.stats.anova_lm(model, typ=2)\nprint(\"\\n=== 이원분산분석 ===\")\nprint(anova_table)\n\n\n=== 이원분산분석 ===\n                        sum_sq     df           F         PR(&gt;F)\nC(species)         6975.591607    2.0  650.478579  1.059087e-114\nC(sex)             1135.683888    1.0  211.806563   2.422971e-37\nC(species):C(sex)    24.494427    2.0    2.284122   1.034865e-01\nResidual           1753.338642  327.0         NaN            NaN",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#사후검정-post-hoc-test",
    "href": "part2/06. 분산 분석.html#사후검정-post-hoc-test",
    "title": "16  분산 분석",
    "section": "16.4 사후검정 (Post-hoc Test)",
    "text": "16.4 사후검정 (Post-hoc Test)\n예제: Tukey HSD\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\ntukey = pairwise_tukeyhsd(\n    endog=df_anova[\"bill_length_mm\"],\n    groups=df_anova[\"species\"],\n    alpha=0.05\n)\n\nprint(\"\\n=== Tukey HSD 사후검정 ===\")\nprint(tukey)\n\n\n=== Tukey HSD 사후검정 ===\n   Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n=========================================================\n  group1    group2  meandiff p-adj   lower  upper  reject\n---------------------------------------------------------\n   Adelie Chinstrap  10.0424    0.0  9.0249  11.06   True\n   Adelie    Gentoo   8.7135    0.0  7.8672 9.5598   True\nChinstrap    Gentoo  -1.3289 0.0089 -2.3819 -0.276   True\n---------------------------------------------------------\n\n\n예제: 시각화\n\nsns.boxplot(\n    data=df_anova,\n    x=\"species\",\n    y=\"bill_length_mm\"\n)\nplt.title(\"Bill Length by Species\")\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#가정-위배-시-대응",
    "href": "part2/06. 분산 분석.html#가정-위배-시-대응",
    "title": "16  분산 분석",
    "section": "16.5 가정 위배 시 대응",
    "text": "16.5 가정 위배 시 대응\n\n정규성 위배 → Kruskal-Wallis 검정\n등분산성 위배 → Welch ANOVA\n독립성 위배 → 반복측정 ANOVA\n\n예제: Kruskal-Wallis (비모수)\n\nfrom scipy.stats import kruskal\n\nh_stat, p_kw = kruskal(*groups)\nprint(f\"\\n=== Kruskal-Wallis ===\")\nprint(f\"H = {h_stat:.4f}, p = {p_kw:.4f}\")\n\n\n=== Kruskal-Wallis ===\nH = 244.1367, p = 0.0000",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/06. 분산 분석.html#요약",
    "href": "part2/06. 분산 분석.html#요약",
    "title": "16  분산 분석",
    "section": "16.6 요약",
    "text": "16.6 요약\n분산분석은 다집단의 평균을 동시에 비교하는 강력한 도구이다. 가정을 확인하고 사후검정을 통해 구체적인 차이를 파악하는 것이 중요하다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>분산 분석</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html",
    "href": "part2/07. 비모수 검정.html",
    "title": "17  비모수 검정",
    "section": "",
    "text": "17.1 비모수 검정의 개념과 필요성\n비모수 검정(Non-parametric Tests)은 모집단의 분포에 대한 가정 없이 데이터를 분석하는 통계적 방법이다. 정규성이나 등분산성 같은 모수 검정의 엄격한 가정을 만족하지 못할 때, 또는 데이터가 순서형이거나 이상치가 많을 때 사용한다. 비모수 검정은 원 데이터 값 대신 순위(rank)를 사용하여 집단 간 차이를 검정하므로 분포에 덜 민감하고 강건하다. 이 장에서는 주요 비모수 검정 방법과 그 적용 상황을 학습한다.\n예제: 데이터 로드\n비모수 검정은 모집단의 분포 형태를 가정하지 않고, 데이터의 순위 정보만을 사용하여 검정한다.\n모수 검정 vs 비모수 검정",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#비모수-검정의-개념과-필요성",
    "href": "part2/07. 비모수 검정.html#비모수-검정의-개념과-필요성",
    "title": "17  비모수 검정",
    "section": "",
    "text": "구분\n모수 검정\n비모수 검정\n\n\n\n\n가정\n정규분포, 등분산성\n가정 최소 (분포 자유)\n\n\n사용 정보\n원 데이터 값\n순위(rank)\n\n\n검정 대상\n평균\n중앙값 또는 분포 위치\n\n\n검정력\n가정 만족 시 높음\n가정 미만족 시 높음\n\n\n이상치 민감도\n높음\n낮음 (강건함)\n\n\n해석\n평균 차이\n분포 위치 차이\n\n\n\n\n17.1.1 비모수 검정이 필요한 상황\n비모수 검정 적용 시점\n\n\n\n\n\n\n\n\n상황\n설명\n확인 방법\n\n\n\n\n정규성 위배\n데이터가 정규분포를 따르지 않음\nShapiro-Wilk 검정, Q-Q plot\n\n\n소표본\n표본 크기가 작아 중심극한정리 적용 불가\nn &lt; 30 (특히 n &lt; 15)\n\n\n이상치 다수\n극단값이 많아 평균이 왜곡됨\n박스플롯, IQR 분석\n\n\n순서형 데이터\n측정 단위가 순서 척도\n설문 리커트 척도 등\n\n\n불균형 표본\n집단 간 표본 크기 차이가 큼\n표본 크기 비교\n\n\n\n예제: 정규성 확인\n\n# 정규성 검정\nadelie = df[df[\"species\"] == \"Adelie\"][\"bill_length_mm\"].dropna()\ngentoo = df[df[\"species\"] == \"Gentoo\"][\"bill_length_mm\"].dropna()\n\nprint(\"=== 정규성 검정 (Shapiro-Wilk) ===\")\n_, p_adelie = stats.shapiro(adelie)\n_, p_gentoo = stats.shapiro(gentoo)\n\nprint(f\"Adelie: p = {p_adelie:.4f} {'(정규)' if p_adelie &gt; 0.05 else '(비정규)'}\")\nprint(f\"Gentoo: p = {p_gentoo:.4f} {'(정규)' if p_gentoo &gt; 0.05 else '(비정규)'}\")\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nstats.probplot(adelie, dist=\"norm\", plot=axes[0])\naxes[0].set_title(\"Q-Q Plot: Adelie\")\n\nstats.probplot(gentoo, dist=\"norm\", plot=axes[1])\naxes[1].set_title(\"Q-Q Plot: Gentoo\")\n\nplt.tight_layout()\nplt.show()\n\n=== 정규성 검정 (Shapiro-Wilk) ===\nAdelie: p = 0.7166 (정규)\nGentoo: p = 0.0135 (비정규)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#mann-whitney-u-검정-wilcoxon-rank-sum-test",
    "href": "part2/07. 비모수 검정.html#mann-whitney-u-검정-wilcoxon-rank-sum-test",
    "title": "17  비모수 검정",
    "section": "17.2 Mann-Whitney U 검정 (Wilcoxon Rank-Sum Test)",
    "text": "17.2 Mann-Whitney U 검정 (Wilcoxon Rank-Sum Test)\nMann-Whitney U 검정은 두 독립 집단의 분포 위치를 비교하는 비모수 검정으로, 독립표본 t-검정의 대안이다.\n특징\n\n대상: 두 독립 집단\n대안 검정: 독립표본 t-검정\n검정 대상: 두 집단의 분포가 같은지 (중앙값 또는 순위 합)\n가정: 두 집단의 분포 형태가 유사 (위치만 다를 수 있음)\n\n가설 설정\n\nH₀ (귀무가설): 두 집단의 분포가 동일하다 (중앙값이 같다)\nH₁ (대립가설): 두 집단의 분포가 다르다 (중앙값이 다르다)\n\n검정 원리\n\n두 집단의 데이터를 합쳐서 순위 매김\n각 집단의 순위 합 계산\nU 통계량 계산 (작은 집단의 순위 합이 작을수록 차이가 큼)\n\n\n17.2.1 예제: Adelie vs Gentoo 부리 길이 비교\n예제: 데이터 준비\n\nfrom scipy.stats import mannwhitneyu\n\n# 두 종 선택\ng1 = df[df[\"species\"] == \"Adelie\"][\"bill_length_mm\"].dropna()\ng2 = df[df[\"species\"] == \"Gentoo\"][\"bill_length_mm\"].dropna()\n\nprint(\"=== 집단 정보 ===\")\nprint(f\"Adelie: n = {len(g1)}, 중앙값 = {g1.median():.2f}mm\")\nprint(f\"Gentoo: n = {len(g2)}, 중앙값 = {g2.median():.2f}mm\")\n\n# 기술 통계량\nprint(\"\\n=== 기술 통계량 ===\")\nprint(\"Adelie:\")\nprint(g1.describe())\nprint(\"\\nGentoo:\")\nprint(g2.describe())\n\n=== 집단 정보 ===\nAdelie: n = 151, 중앙값 = 38.80mm\nGentoo: n = 123, 중앙값 = 47.30mm\n\n=== 기술 통계량 ===\nAdelie:\ncount    151.000000\nmean      38.791391\nstd        2.663405\nmin       32.100000\n25%       36.750000\n50%       38.800000\n75%       40.750000\nmax       46.000000\nName: bill_length_mm, dtype: float64\n\nGentoo:\ncount    123.000000\nmean      47.504878\nstd        3.081857\nmin       40.900000\n25%       45.300000\n50%       47.300000\n75%       49.550000\nmax       59.600000\nName: bill_length_mm, dtype: float64\n\n\n예제: Mann-Whitney U 검정\n\n# Mann-Whitney U 검정\nu_stat, p_value = mannwhitneyu(g1, g2, alternative=\"two-sided\")\n\nprint(\"\\n=== Mann-Whitney U 검정 ===\")\nprint(f\"U-통계량: {u_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(\"✓ 귀무가설 기각: 두 종의 부리 길이 분포가 유의하게 다름\")\n    print(f\"  중앙값 차이: {g2.median() - g1.median():.2f}mm\")\nelse:\n    print(\"✗ 귀무가설 채택: 두 종의 부리 길이 분포가 유의하게 다르지 않음\")\n\n\n=== Mann-Whitney U 검정 ===\nU-통계량: 224.5000\np-value: 0.0000\n\n유의수준 0.05 기준:\n✓ 귀무가설 기각: 두 종의 부리 길이 분포가 유의하게 다름\n  중앙값 차이: 8.50mm\n\n\n예제: 시각화\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 박스플롯\ndata_combined = pd.DataFrame({\n    'Bill Length': pd.concat([g1, g2]),\n    'Species': ['Adelie']*len(g1) + ['Gentoo']*len(g2)\n})\nsns.boxplot(x='Species', y='Bill Length', data=data_combined, ax=axes[0])\naxes[0].set_title(\"Bill Length: Adelie vs Gentoo\")\naxes[0].set_ylabel(\"Bill Length (mm)\")\n\n# 히스토그램\naxes[1].hist(g1, alpha=0.5, label='Adelie', bins=15, edgecolor='black')\naxes[1].hist(g2, alpha=0.5, label='Gentoo', bins=15, edgecolor='black')\naxes[1].axvline(g1.median(), color='blue', linestyle='--', linewidth=2, label=f'Adelie median')\naxes[1].axvline(g2.median(), color='orange', linestyle='--', linewidth=2, label=f'Gentoo median')\naxes[1].set_title(\"Distribution of Bill Length\")\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Frequency\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: t-검정과 비교\n\n# 독립표본 t-검정과 비교\nfrom scipy.stats import ttest_ind\n\nt_stat, p_t = ttest_ind(g1, g2)\n\nprint(\"\\n=== 모수 vs 비모수 비교 ===\")\nprint(f\"t-검정 (모수):        t = {t_stat:.4f}, p = {p_t:.4f}\")\nprint(f\"Mann-Whitney (비모수): U = {u_stat:.4f}, p = {p_value:.4f}\")\nprint(\"\\n→ 두 검정 모두 유사한 결론\")\n\n\n=== 모수 vs 비모수 비교 ===\nt-검정 (모수):        t = -25.0953, p = 0.0000\nMann-Whitney (비모수): U = 224.5000, p = 0.0000\n\n→ 두 검정 모두 유사한 결론",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#wilcoxon-signed-rank-test",
    "href": "part2/07. 비모수 검정.html#wilcoxon-signed-rank-test",
    "title": "17  비모수 검정",
    "section": "17.3 Wilcoxon Signed-Rank Test",
    "text": "17.3 Wilcoxon Signed-Rank Test\nWilcoxon 부호순위 검정은 동일 대상의 전후 변화를 비교하는 비모수 검정으로, 대응표본 t-검정의 대안이다.\n특징\n\n대상: 동일 대상의 두 측정값 (대응 표본)\n대안 검정: 대응표본 t-검정\n검정 대상: 차이의 중앙값이 0인지\n가정: 차이의 분포가 대칭\n\n가설 설정\n\nH₀ (귀무가설): 차이의 중앙값이 0이다 (전후 차이 없음)\nH₁ (대립가설): 차이의 중앙값이 0이 아니다 (전후 차이 있음)\n\n검정 원리\n\n각 쌍의 차이 계산\n차이의 절댓값에 순위 매김\n양수 차이와 음수 차이의 순위 합 비교\n\n\n17.3.1 예제: 가상 전후 데이터\npenguins 데이터셋에는 자연스러운 전후 데이터가 없으므로, 예제를 위해 가상 데이터를 생성한다.\n예제: 가상 데이터 생성\n\nfrom scipy.stats import wilcoxon\n\n# 시드 설정\nnp.random.seed(42)\n\n# 전후 데이터 생성 (예: 처치 전후 부리 길이)\nbefore = df[df[\"species\"] == \"Adelie\"][\"bill_length_mm\"].dropna().sample(30, random_state=42)\nafter = before + np.random.normal(0.5, 1.0, size=len(before))\n\nprint(\"=== 전후 데이터 ===\")\nprint(f\"처치 전 중앙값: {before.median():.2f}mm\")\nprint(f\"처치 후 중앙값: {after.median():.2f}mm\")\nprint(f\"차이 중앙값: {(after - before).median():.2f}mm\")\n\n=== 전후 데이터 ===\n처치 전 중앙값: 39.25mm\n처치 후 중앙값: 39.04mm\n차이 중앙값: 0.27mm\n\n\n예제: Wilcoxon Signed-Rank 검정\n\n# Wilcoxon 부호순위 검정\nw_stat, p_value = wilcoxon(before, after)\n\nprint(\"\\n=== Wilcoxon Signed-Rank 검정 ===\")\nprint(f\"W-통계량: {w_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(\"✓ 귀무가설 기각: 처치 전후 유의한 차이 있음\")\nelse:\n    print(\"✗ 귀무가설 채택: 처치 전후 유의한 차이 없음\")\n\n\n=== Wilcoxon Signed-Rank 검정 ===\nW-통계량: 149.0000\np-value: 0.0879\n\n유의수준 0.05 기준:\n✗ 귀무가설 채택: 처치 전후 유의한 차이 없음\n\n\n예제: 시각화\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 전후 비교 박스플롯\ndata_paired = pd.DataFrame({\n    'Value': pd.concat([before, after]),\n    'Time': ['Before']*len(before) + ['After']*len(after)\n})\nsns.boxplot(x='Time', y='Value', data=data_paired, ax=axes[0])\naxes[0].set_title(\"Before vs After Treatment\")\naxes[0].set_ylabel(\"Bill Length (mm)\")\n\n# 차이 분포\ndiff = after.values - before.values\naxes[1].hist(diff, bins=15, edgecolor='black', alpha=0.7)\naxes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='No difference')\naxes[1].axvline(np.median(diff), color='green', linestyle='--', linewidth=2, label=f'Median diff = {np.median(diff):.2f}')\naxes[1].set_title(\"Distribution of Differences (After - Before)\")\naxes[1].set_xlabel(\"Difference (mm)\")\naxes[1].set_ylabel(\"Frequency\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 대응표본 t-검정과 비교\n\n# 대응표본 t-검정과 비교\nfrom scipy.stats import ttest_rel\n\nt_stat, p_t = ttest_rel(before, after)\n\nprint(\"\\n=== 모수 vs 비모수 비교 ===\")\nprint(f\"Paired t-test (모수):  t = {t_stat:.4f}, p = {p_t:.4f}\")\nprint(f\"Wilcoxon (비모수):     W = {w_stat:.4f}, p = {p_value:.4f}\")\n\n\n=== 모수 vs 비모수 비교 ===\nPaired t-test (모수):  t = -1.8979, p = 0.0677\nWilcoxon (비모수):     W = 149.0000, p = 0.0879",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#kruskal-wallis-test",
    "href": "part2/07. 비모수 검정.html#kruskal-wallis-test",
    "title": "17  비모수 검정",
    "section": "17.4 Kruskal-Wallis Test",
    "text": "17.4 Kruskal-Wallis Test\nKruskal-Wallis 검정은 세 개 이상 독립 집단의 분포를 비교하는 비모수 검정으로, 일원분산분석(ANOVA)의 대안이다.\n특징\n\n대상: 세 개 이상 독립 집단\n대안 검정: 일원분산분석 (One-way ANOVA)\n검정 대상: 모든 집단의 분포가 같은지\n가정: 독립성 (분포 형태 가정 불필요)\n\n가설 설정\n\nH₀ (귀무가설): 모든 집단의 분포가 동일하다\nH₁ (대립가설): 적어도 하나의 집단 분포가 다르다\n\n검정 원리\n\n모든 데이터를 합쳐서 순위 매김\n각 집단의 순위 평균 계산\nH 통계량 계산 (카이제곱 분포에 근사)\n\n\n17.4.1 예제: 세 종의 부리 길이 비교\n예제: 데이터 준비\n\nfrom scipy.stats import kruskal\n\n# 세 종의 부리 길이\ngroups = [\n    df[df[\"species\"] == sp][\"bill_length_mm\"].dropna()\n    for sp in df[\"species\"].unique()\n]\n\nprint(\"=== 집단 정보 ===\")\nfor species, group in zip(df[\"species\"].unique(), groups):\n    print(f\"{species:12s}: n = {len(group):3d}, 중앙값 = {group.median():.2f}mm\")\n\n=== 집단 정보 ===\nAdelie      : n = 151, 중앙값 = 38.80mm\nChinstrap   : n =  68, 중앙값 = 49.55mm\nGentoo      : n = 123, 중앙값 = 47.30mm\n\n\n예제: Kruskal-Wallis 검정\n\n# Kruskal-Wallis 검정\nh_stat, p_value = kruskal(*groups)\n\nprint(\"\\n=== Kruskal-Wallis 검정 ===\")\nprint(f\"H-통계량: {h_stat:.4f}\")\nprint(f\"자유도: {len(groups) - 1}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(\"✓ 귀무가설 기각: 종 간 부리 길이 분포가 유의하게 다름\")\n    print(\"  → 어느 종 간 차이인지 확인하려면 사후검정 필요\")\nelse:\n    print(\"✗ 귀무가설 채택: 종 간 부리 길이 분포가 유의하게 다르지 않음\")\n\n\n=== Kruskal-Wallis 검정 ===\nH-통계량: 244.1367\n자유도: 2\np-value: 0.0000\n\n유의수준 0.05 기준:\n✓ 귀무가설 기각: 종 간 부리 길이 분포가 유의하게 다름\n  → 어느 종 간 차이인지 확인하려면 사후검정 필요\n\n\n예제: ANOVA와 비교\n\n# 일원분산분석과 비교\nfrom scipy.stats import f_oneway\n\nf_stat, p_anova = f_oneway(*groups)\n\nprint(\"\\n=== 모수 vs 비모수 비교 ===\")\nprint(f\"ANOVA (모수):          F = {f_stat:.4f}, p = {p_anova:.4f}\")\nprint(f\"Kruskal-Wallis (비모수): H = {h_stat:.4f}, p = {p_value:.4f}\")\nprint(\"\\n→ 두 검정 모두 유사한 결론\")\n\n\n=== 모수 vs 비모수 비교 ===\nANOVA (모수):          F = 410.6003, p = 0.0000\nKruskal-Wallis (비모수): H = 244.1367, p = 0.0000\n\n→ 두 검정 모두 유사한 결론\n\n\n예제: 시각화\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 박스플롯\ndf_clean = df[[\"species\", \"bill_length_mm\"]].dropna()\nsns.boxplot(x=\"species\", y=\"bill_length_mm\", data=df_clean, ax=axes[0])\naxes[0].set_title(\"Bill Length by Species\")\naxes[0].set_ylabel(\"Bill Length (mm)\")\n\n# 바이올린 플롯\nsns.violinplot(x=\"species\", y=\"bill_length_mm\", data=df_clean, ax=axes[1])\naxes[1].set_title(\"Distribution of Bill Length\")\naxes[1].set_ylabel(\"Bill Length (mm)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n17.4.2 사후검정 (Post-hoc Test)\nKruskal-Wallis 검정이 유의하면 쌍별 비교를 수행한다.\n예제: Dunn’s Test (사후검정)\n\n# Dunn's test는 scikit-posthocs 패키지 사용\n# import scikit_posthocs as sp\n# dunn_results = sp.posthoc_dunn(df_clean, val_col='bill_length_mm', group_col='species')\n# print(dunn_results)\n\n# 대안: 쌍별 Mann-Whitney U 검정 (Bonferroni 보정)\nfrom itertools import combinations\n\nspecies_list = df[\"species\"].unique()\nn_comparisons = len(list(combinations(species_list, 2)))\nalpha_bonf = 0.05 / n_comparisons\n\nprint(f\"\\n=== 쌍별 Mann-Whitney U (Bonferroni 보정) ===\")\nprint(f\"비교 횟수: {n_comparisons}\")\nprint(f\"보정된 유의수준: {alpha_bonf:.4f}\\n\")\n\nfor sp1, sp2 in combinations(species_list, 2):\n    g1 = df[df[\"species\"] == sp1][\"bill_length_mm\"].dropna()\n    g2 = df[df[\"species\"] == sp2][\"bill_length_mm\"].dropna()\n    \n    u, p = mannwhitneyu(g1, g2)\n    \n    print(f\"{sp1:12s} vs {sp2:12s}: U = {u:.4f}, p = {p:.4f} \", end=\"\")\n    print(\"→ 유의\" if p &lt; alpha_bonf else \"→ 비유의\")\n\n\n=== 쌍별 Mann-Whitney U (Bonferroni 보정) ===\n비교 횟수: 3\n보정된 유의수준: 0.0167\n\nAdelie       vs Chinstrap   : U = 101.0000, p = 0.0000 → 유의\nAdelie       vs Gentoo      : U = 224.5000, p = 0.0000 → 유의\nChinstrap    vs Gentoo      : U = 5323.5000, p = 0.0018 → 유의",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#friedman-test",
    "href": "part2/07. 비모수 검정.html#friedman-test",
    "title": "17  비모수 검정",
    "section": "17.5 Friedman Test",
    "text": "17.5 Friedman Test\nFriedman 검정은 세 개 이상의 대응 집단(동일 대상의 반복 측정)을 비교하는 비모수 검정으로, 반복측정 ANOVA의 대안이다.\n특징\n\n대상: 세 개 이상 대응 집단 (동일 대상의 반복 측정)\n대안 검정: 반복측정 ANOVA\n검정 대상: 모든 조건의 분포가 같은지\n가정: 각 블록(대상) 내에서 순위 매김 가능\n\n가설 설정\n\nH₀ (귀무가설): 모든 조건의 분포가 동일하다\nH₁ (대립가설): 적어도 하나의 조건 분포가 다르다\n\n\n17.5.1 예제: 가상 반복 측정 데이터\n예제: 가상 데이터 생성\n\nfrom scipy.stats import friedmanchisquare\n\n# 가상 반복 측정 데이터 (예: 3가지 처치 조건)\nnp.random.seed(123)\n\nn = 30\ncond1 = before.values\ncond2 = before.values + np.random.normal(0.3, 0.8, size=len(before))\ncond3 = before.values + np.random.normal(0.8, 0.8, size=len(before))\n\nprint(\"=== 조건별 중앙값 ===\")\nprint(f\"조건 1: {np.median(cond1):.2f}mm\")\nprint(f\"조건 2: {np.median(cond2):.2f}mm\")\nprint(f\"조건 3: {np.median(cond3):.2f}mm\")\n\n=== 조건별 중앙값 ===\n조건 1: 39.25mm\n조건 2: 39.97mm\n조건 3: 39.71mm\n\n\n예제: Friedman 검정\n\n# Friedman 검정\nf_stat, p_value = friedmanchisquare(cond1, cond2, cond3)\n\nprint(\"\\n=== Friedman 검정 ===\")\nprint(f\"χ²-통계량: {f_stat:.4f}\")\nprint(f\"자유도: {3 - 1}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(\"✓ 귀무가설 기각: 조건 간 분포가 유의하게 다름\")\nelse:\n    print(\"✗ 귀무가설 채택: 조건 간 분포가 유의하게 다르지 않음\")\n\n\n=== Friedman 검정 ===\nχ²-통계량: 11.6667\n자유도: 2\np-value: 0.0029\n\n유의수준 0.05 기준:\n✓ 귀무가설 기각: 조건 간 분포가 유의하게 다름\n\n\n예제: 시각화\n\n# 시각화\ndata_repeated = pd.DataFrame({\n    'Value': np.concatenate([cond1, cond2, cond3]),\n    'Condition': ['Cond1']*len(cond1) + ['Cond2']*len(cond2) + ['Cond3']*len(cond3),\n    'Subject': list(range(len(cond1))) * 3\n})\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(x='Condition', y='Value', data=data_repeated)\nplt.title(\"Repeated Measures: Three Conditions\")\nplt.ylabel(\"Bill Length (mm)\")\nplt.show()",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#모수-검정-vs-비모수-검정-종합",
    "href": "part2/07. 비모수 검정.html#모수-검정-vs-비모수-검정-종합",
    "title": "17  비모수 검정",
    "section": "17.6 모수 검정 vs 비모수 검정 종합",
    "text": "17.6 모수 검정 vs 비모수 검정 종합\n검정 방법 대응표\n\n\n\n\n\n\n\n\n\n상황\n모수 검정\n비모수 검정\n검정 대상\n\n\n\n\n두 독립 집단\n독립표본 t-검정\nMann-Whitney U\n평균 vs 분포 위치\n\n\n두 대응 집단\n대응표본 t-검정\nWilcoxon Signed-Rank\n평균 차이 vs 중앙값 차이\n\n\n세 집단 이상 (독립)\n일원분산분석 (ANOVA)\nKruskal-Wallis\n평균 vs 분포\n\n\n세 집단 이상 (대응)\n반복측정 ANOVA\nFriedman\n평균 vs 분포\n\n\n\n선택 기준\n\n\n\n조건\n권장 검정\n\n\n\n\n정규성 만족 + 등분산 + n ≥ 30\n모수 검정 (높은 검정력)\n\n\n정규성 의심 또는 이상치 많음\n비모수 검정 (강건함)\n\n\n소표본 (n &lt; 30) + 정규성 불확실\n비모수 검정 (안전)\n\n\n순서형 데이터\n비모수 검정 (유일한 선택)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/07. 비모수 검정.html#요약",
    "href": "part2/07. 비모수 검정.html#요약",
    "title": "17  비모수 검정",
    "section": "17.7 요약",
    "text": "17.7 요약\n이 장에서는 분포 가정이 필요 없는 비모수 검정을 학습했다. 주요 내용은 다음과 같다.\n비모수 검정 핵심 요약\n\n\n\n\n\n\n\n\n\n검정\n용도\n장점\n단점\n\n\n\n\nMann-Whitney U\n두 독립 집단\n정규성 불필요, 이상치 강건\n검정력 약간 낮음\n\n\nWilcoxon Signed-Rank\n두 대응 집단\n정규성 불필요, 대칭성만 가정\n검정력 약간 낮음\n\n\nKruskal-Wallis\n다집단 독립\n분포 가정 불필요\n어느 집단이 다른지 모름\n\n\nFriedman\n다집단 대응\n반복측정 가능\n사후검정 제한적\n\n\n\n비모수 검정 사용 원칙\n\n가정 확인: 정규성, 등분산성 검정으로 모수 검정 가능 여부 확인\n표본 크기: n &lt; 30이고 정규성 불확실하면 비모수 선택\n이상치: 이상치가 많으면 비모수 검정이 더 안전\n해석: 비모수는 중앙값/분포 위치 비교, 모수는 평균 비교\n검정력: 가정 만족 시 모수 검정이 검정력 높음\n\n실무 의사결정 흐름\n데이터 확보\n  ↓\n정규성 검정\n  ├─ 정규 → 등분산성 검정\n  │          ├─ 등분산 → 모수 검정 (t-test, ANOVA)\n  │          └─ 이분산 → Welch 검정 또는 비모수\n  └─ 비정규 → 비모수 검정 (Mann-Whitney, Kruskal-Wallis)\n주의사항\n\n비모수 검정은 평균이 아닌 중앙값 또는 분포 위치 비교\n검정력이 모수 검정보다 약간 낮음 (가정 만족 시)\n사후검정이 모수 검정만큼 체계적이지 않음\n효과 크기 측정이 제한적\n해석 시 “분포의 위치 차이”로 표현\n\n비모수 검정은 가정을 만족하지 못할 때 안전하고 강건한 대안이다. 데이터의 특성과 분석 목적을 고려하여 모수 검정과 비모수 검정을 적절히 선택하는 것이 중요하다. 다음으로는 변수 간 관계를 분석하는 상관분석과 회귀분석을 학습할 수 있다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>비모수 검정</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html",
    "href": "part2/08. 상관 분석.html",
    "title": "18  상관 분석",
    "section": "",
    "text": "18.1 상관계수의 기본 개념\n상관 분석(Correlation Analysis)은 두 연속형 변수 간의 관계 방향과 강도를 수치로 표현하는 통계적 방법이다. 상관계수는 두 변수가 함께 변하는 패턴을 -1에서 +1 사이의 값으로 나타내며, 변수 간 관계를 탐색하고 이해하는 데 필수적인 도구이다. 이 장에서는 Pearson, Spearman, Kendall 상관계수의 개념과 차이, 그리고 실무 적용 방법을 학습한다.\n중요: 상관관계는 두 변수 간 연관성만을 나타내며, 인과관계를 의미하지 않는다.\n예제: 데이터 로드\n상관계수는 두 변수 간 선형 또는 단조 관계의 방향과 강도를 나타낸다.\n상관계수의 특성\n상관계수 강도 해석 (경험적 기준)\n주의사항",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#상관계수의-기본-개념",
    "href": "part2/08. 상관 분석.html#상관계수의-기본-개념",
    "title": "18  상관 분석",
    "section": "",
    "text": "항목\n설명\n\n\n\n\n범위\n-1 ≤ r ≤ +1\n\n\n+1\n완벽한 양의 상관 (한 변수 증가 시 다른 변수도 완벽히 증가)\n\n\n0\n선형 관계 없음 (독립적이거나 비선형 관계)\n\n\n-1\n완벽한 음의 상관 (한 변수 증가 시 다른 변수는 완벽히 감소)\n\n\n|r|\n절댓값이 클수록 관계가 강함\n\n\n\n\n\n\n\n|r| 값\n관계 강도\n해석\n\n\n\n\n0.0 ~ 0.1\n매우 약함\n거의 관계 없음\n\n\n0.1 ~ 0.3\n약함\n약한 관계\n\n\n0.3 ~ 0.5\n중간\n중간 정도 관계\n\n\n0.5 ~ 0.7\n강함\n강한 관계\n\n\n0.7 ~ 1.0\n매우 강함\n매우 강한 관계\n\n\n\n\n\n\n\n\n\n\n\n주의점\n설명\n\n\n\n\n인과관계 아님\n“A와 B가 상관있다” ≠ “A가 B를 야기한다”\n\n\n이상치 민감\n극단값이 상관계수를 크게 왜곡할 수 있음\n\n\n비선형 관계\n비선형 관계는 상관계수가 낮게 나올 수 있음\n\n\n제3의 변수\n두 변수의 상관이 다른 변수 때문일 수 있음\n\n\nSimpson’s Paradox\n전체 데이터와 그룹별 데이터의 상관이 반대일 수 있음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#pearson-상관계수",
    "href": "part2/08. 상관 분석.html#pearson-상관계수",
    "title": "18  상관 분석",
    "section": "18.2 Pearson 상관계수",
    "text": "18.2 Pearson 상관계수\nPearson 상관계수는 두 변수 간 선형 관계의 강도와 방향을 측정하는 가장 널리 사용되는 방법이다.\n개념\n\n측정 대상: 선형 관계\n계산 기반: 공분산을 표준편차로 나눈 값\n가정: 두 변수 모두 연속형, 선형 관계, 정규분포 (근사적)\n\nPearson 상관계수 공식\n\\[\nr = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} \\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}}\n\\]\n가설 설정\n\nH₀ (귀무가설): ρ = 0 (두 변수 간 선형 상관이 없다)\nH₁ (대립가설): ρ ≠ 0 (두 변수 간 선형 상관이 있다)\n\n\n18.2.1 예제: 부리 길이와 깊이의 상관관계\n예제: 데이터 준비\n\nfrom scipy.stats import pearsonr\n\n# 결측치 제거\ndf_corr = df[[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n\nprint(\"=== 데이터 정보 ===\")\nprint(f\"샘플 수: {len(df_corr)}\")\nprint(\"\\n기술 통계량:\")\nprint(df_corr.describe())\n\n=== 데이터 정보 ===\n샘플 수: 342\n\n기술 통계량:\n       bill_length_mm  bill_depth_mm\ncount      342.000000     342.000000\nmean        43.921930      17.151170\nstd          5.459584       1.974793\nmin         32.100000      13.100000\n25%         39.225000      15.600000\n50%         44.450000      17.300000\n75%         48.500000      18.700000\nmax         59.600000      21.500000\n\n\n예제: Pearson 상관계수 계산\n\n# Pearson 상관계수\nr, p_value = pearsonr(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\nprint(\"=== Pearson 상관 분석 ===\")\nprint(f\"상관계수 (r): {r:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\n# 해석\nalpha = 0.05\nprint(f\"\\n유의수준 {alpha} 기준:\")\nif p_value &lt; alpha:\n    print(f\"✓ 상관관계가 통계적으로 유의함\")\n    \n    if r &gt; 0:\n        direction = \"양의\"\n    else:\n        direction = \"음의\"\n    \n    if abs(r) &lt; 0.3:\n        strength = \"약한\"\n    elif abs(r) &lt; 0.5:\n        strength = \"중간\"\n    else:\n        strength = \"강한\"\n    \n    print(f\"  → {direction} {strength} 선형 관계\")\nelse:\n    print(f\"✗ 상관관계가 통계적으로 유의하지 않음\")\n\n=== Pearson 상관 분석 ===\n상관계수 (r): -0.2351\np-value: 0.0000\n\n유의수준 0.05 기준:\n✓ 상관관계가 통계적으로 유의함\n  → 음의 약한 선형 관계\n\n\n예제: 산점도 시각화\n\n# 산점도와 회귀선\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 전체 데이터 산점도\naxes[0].scatter(df_corr[\"bill_length_mm\"], df_corr[\"bill_depth_mm\"], alpha=0.5)\naxes[0].set_title(f\"Bill Length vs Depth\\n(Pearson r = {r:.3f})\")\naxes[0].set_xlabel(\"Bill Length (mm)\")\naxes[0].set_ylabel(\"Bill Depth (mm)\")\naxes[0].grid(True, alpha=0.3)\n\n# 회귀선 추가\nz = np.polyfit(df_corr[\"bill_length_mm\"], df_corr[\"bill_depth_mm\"], 1)\np = np.poly1d(z)\naxes[0].plot(df_corr[\"bill_length_mm\"].sort_values(), \n             p(df_corr[\"bill_length_mm\"].sort_values()), \n             \"r--\", linewidth=2, label='Linear fit')\naxes[0].legend()\n\n# 종별 산점도 (Simpson's Paradox 확인)\nfor species in df[\"species\"].unique():\n    species_data = df[df[\"species\"] == species][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n    axes[1].scatter(species_data[\"bill_length_mm\"], \n                   species_data[\"bill_depth_mm\"], \n                   alpha=0.6, label=species)\n\naxes[1].set_title(\"Bill Length vs Depth by Species\")\naxes[1].set_xlabel(\"Bill Length (mm)\")\naxes[1].set_ylabel(\"Bill Depth (mm)\")\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nSimpson’s Paradox 확인\n\n# 종별 상관계수\nprint(\"\\n=== 종별 Pearson 상관계수 ===\")\nfor species in df[\"species\"].unique():\n    species_data = df[df[\"species\"] == species][[\"bill_length_mm\", \"bill_depth_mm\"]].dropna()\n    r_species, p_species = pearsonr(species_data[\"bill_length_mm\"], species_data[\"bill_depth_mm\"])\n    print(f\"{species:12s}: r = {r_species:6.3f}, p = {p_species:.4f}\")\n\nprint(f\"\\n전체 데이터:  r = {r:6.3f}, p = {p_value:.4f}\")\nprint(\"\\n⚠️ 전체 데이터와 그룹별 상관의 방향이 다를 수 있음 (Simpson's Paradox)\")\n\n\n=== 종별 Pearson 상관계수 ===\nAdelie      : r =  0.391, p = 0.0000\nChinstrap   : r =  0.654, p = 0.0000\nGentoo      : r =  0.643, p = 0.0000\n\n전체 데이터:  r = -0.235, p = 0.0000\n\n⚠️ 전체 데이터와 그룹별 상관의 방향이 다를 수 있음 (Simpson's Paradox)",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#spearman-순위-상관계수",
    "href": "part2/08. 상관 분석.html#spearman-순위-상관계수",
    "title": "18  상관 분석",
    "section": "18.3 Spearman 순위 상관계수",
    "text": "18.3 Spearman 순위 상관계수\nSpearman 상관계수는 데이터의 순위를 사용하여 단조 관계를 측정하는 비모수적 방법이다.\n개념\n\n측정 대상: 단조 관계 (monotonic relationship)\n계산 기반: 순위 간 Pearson 상관계수\n가정: 순서형 이상의 데이터, 정규성 불필요\n\n언제 사용하는가?\n\n\n\n상황\n설명\n\n\n\n\n정규성 위배\n데이터가 정규분포를 따르지 않음\n\n\n이상치 많음\n극단값이 결과를 왜곡할 수 있음\n\n\n비선형 단조 관계\n직선은 아니지만 한 방향으로 증가/감소\n\n\n순서형 데이터\n순위나 등급 데이터\n\n\n\n가설 설정\n\nH₀ (귀무가설): ρₛ = 0 (두 변수 간 순위 상관이 없다)\nH₁ (대립가설): ρₛ ≠ 0 (두 변수 간 순위 상관이 있다)\n\n\n18.3.1 예제: Spearman 상관계수\n예제: Spearman 상관 분석\n\nfrom scipy.stats import spearmanr\n\n# Spearman 순위 상관계수\nrho, p_value_spear = spearmanr(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\nprint(\"=== Spearman 순위 상관 분석 ===\")\nprint(f\"순위 상관계수 (ρ): {rho:.4f}\")\nprint(f\"p-value: {p_value_spear:.4f}\")\n\n# Pearson과 비교\nprint(\"\\n=== Pearson vs Spearman 비교 ===\")\nprint(f\"Pearson r:  {r:.4f} (p = {p_value:.4f})\")\nprint(f\"Spearman ρ: {rho:.4f} (p = {p_value_spear:.4f})\")\nprint(f\"차이: {abs(r - rho):.4f}\")\n\nif abs(r - rho) &gt; 0.1:\n    print(\"\\n⚠️ Pearson과 Spearman 차이가 큼 → 비선형 관계 또는 이상치 의심\")\n\n=== Spearman 순위 상관 분석 ===\n순위 상관계수 (ρ): -0.2217\np-value: 0.0000\n\n=== Pearson vs Spearman 비교 ===\nPearson r:  -0.2351 (p = 0.0000)\nSpearman ρ: -0.2217 (p = 0.0000)\n차이: 0.0133\n\n\nPearson vs Spearman 비교\n\n\n\n항목\nPearson\nSpearman\n\n\n\n\n기반\n실제 값\n순위\n\n\n측정 관계\n선형\n단조\n\n\n이상치 영향\n크다\n작다\n\n\n정규성 가정\n중요\n불필요\n\n\n계산 복잡도\n낮음\n중간\n\n\n적용 범위\n선형 관계\n단조 관계",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#kendall-순위-상관계수",
    "href": "part2/08. 상관 분석.html#kendall-순위-상관계수",
    "title": "18  상관 분석",
    "section": "18.4 Kendall 순위 상관계수",
    "text": "18.4 Kendall 순위 상관계수\nKendall τ는 순위 쌍의 일치도를 측정하는 또 다른 비모수적 상관계수이다.\n개념\n\n측정 대상: 순위 쌍의 일치 정도\n계산 기반: concordant pairs vs discordant pairs\n가정: 순서형 이상의 데이터\n\n직관적 설명\n두 관측치 쌍 (xᵢ, yᵢ)와 (xⱼ, yⱼ)에 대해: - Concordant: x와 y가 같은 방향으로 증가/감소 - Discordant: x와 y가 반대 방향\n\\[\n\\tau = \\frac{\\text{(일치 쌍 수)} - \\text{(불일치 쌍 수)}}{\\text{전체 쌍 수}}\n\\]\n언제 사용하는가?\n\n\n\n상황\n이유\n\n\n\n\n소표본\nSpearman보다 안정적\n\n\n동순위 많음\nTies 처리가 우수\n\n\n순서 중시\n순위 정보가 핵심\n\n\n보수적 추정\n절댓값이 작아 신중한 해석\n\n\n\n\n18.4.1 예제: Kendall 상관계수\n예제: Kendall τ 계산\n\nfrom scipy.stats import kendalltau\n\n# Kendall τ 계산\ntau, p_value_kendall = kendalltau(\n    df_corr[\"bill_length_mm\"],\n    df_corr[\"bill_depth_mm\"]\n)\n\nprint(\"=== Kendall τ 순위 상관 분석 ===\")\nprint(f\"Kendall τ: {tau:.4f}\")\nprint(f\"p-value: {p_value_kendall:.4f}\")\n\n# 세 가지 상관계수 비교\nprint(\"\\n=== 세 가지 상관계수 비교 ===\")\nprint(f\"Pearson r:  {r:.4f} (p = {p_value:.4f})\")\nprint(f\"Spearman ρ: {rho:.4f} (p = {p_value_spear:.4f})\")\nprint(f\"Kendall τ:  {tau:.4f} (p = {p_value_kendall:.4f})\")\n\nprint(\"\\n특징:\")\nprint(\"- Kendall τ가 가장 보수적 (절댓값 작음)\")\nprint(\"- 세 계수 모두 같은 방향 (부호 일치)\")\n\n=== Kendall τ 순위 상관 분석 ===\nKendall τ: -0.1229\np-value: 0.0008\n\n=== 세 가지 상관계수 비교 ===\nPearson r:  -0.2351 (p = 0.0000)\nSpearman ρ: -0.2217 (p = 0.0000)\nKendall τ:  -0.1229 (p = 0.0008)\n\n특징:\n- Kendall τ가 가장 보수적 (절댓값 작음)\n- 세 계수 모두 같은 방향 (부호 일치)\n\n\n세 가지 상관계수 종합 비교\n\n\n\n구분\nPearson\nSpearman\nKendall\n\n\n\n\n기반\n실제 값\n순위\n순위 쌍\n\n\n측정 관계\n선형\n단조\n단조\n\n\n이상치 영향\n큰\n중간\n작음\n\n\n정규성 필요\n높음\n낮음\n낮음\n\n\n값 크기 경향\n가장 큼\n중간\n가장 작음\n\n\n해석 안정성\n낮음\n중간\n높음\n\n\n계산 속도\n빠름\n중간\n느림\n\n\n동순위 처리\n보통\n보통\n우수",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#상관-행렬-correlation-matrix",
    "href": "part2/08. 상관 분석.html#상관-행렬-correlation-matrix",
    "title": "18  상관 분석",
    "section": "18.5 상관 행렬 (Correlation Matrix)",
    "text": "18.5 상관 행렬 (Correlation Matrix)\n여러 변수 간의 상관관계를 한눈에 파악할 수 있는 행렬이다.\n예제: 상관 행렬 계산\n\n# 수치형 변수 선택\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n\n# Pearson 상관 행렬\ncorr_matrix = df[num_cols].corr(method=\"pearson\")\n\nprint(\"=== Pearson 상관 행렬 ===\")\nprint(corr_matrix.round(3))\n\n=== Pearson 상관 행렬 ===\n                   bill_length_mm  bill_depth_mm  flipper_length_mm  \\\nbill_length_mm              1.000         -0.235              0.656   \nbill_depth_mm              -0.235          1.000             -0.584   \nflipper_length_mm           0.656         -0.584              1.000   \nbody_mass_g                 0.595         -0.472              0.871   \n\n                   body_mass_g  \nbill_length_mm           0.595  \nbill_depth_mm           -0.472  \nflipper_length_mm        0.871  \nbody_mass_g              1.000  \n\n\n예제: 상관 행렬 히트맵\n\n# 히트맵 시각화\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Pearson 상관 행렬\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", \n            center=0, vmin=-1, vmax=1, ax=axes[0],\n            cbar_kws={'label': 'Correlation'})\naxes[0].set_title(\"Pearson Correlation Matrix\")\n\n# Spearman 상관 행렬\ncorr_matrix_spear = df[num_cols].corr(method=\"spearman\")\nsns.heatmap(corr_matrix_spear, annot=True, cmap=\"coolwarm\", fmt=\".2f\",\n            center=0, vmin=-1, vmax=1, ax=axes[1],\n            cbar_kws={'label': 'Correlation'})\naxes[1].set_title(\"Spearman Correlation Matrix\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 상관계수 p-value 확인\n\n# 상관계수와 p-value를 함께 표시\ndef correlation_with_pvalue(df, method='pearson'):\n    \"\"\"상관계수와 p-value를 함께 계산\"\"\"\n    corr_matrix = df.corr(method=method)\n    n = len(df)\n    \n    # p-value 행렬 생성\n    p_matrix = pd.DataFrame(np.zeros_like(corr_matrix), \n                            columns=corr_matrix.columns, \n                            index=corr_matrix.index)\n    \n    for i, col1 in enumerate(corr_matrix.columns):\n        for j, col2 in enumerate(corr_matrix.columns):\n            if i != j:\n                if method == 'pearson':\n                    _, p = pearsonr(df[col1].dropna(), df[col2].dropna())\n                elif method == 'spearman':\n                    _, p = spearmanr(df[col1].dropna(), df[col2].dropna())\n                p_matrix.iloc[i, j] = p\n    \n    return corr_matrix, p_matrix\n\ncorr, pval = correlation_with_pvalue(df[num_cols], method='pearson')\n\nprint(\"=== 상관계수 (위) 및 p-value (아래) ===\")\nfor i, col1 in enumerate(num_cols):\n    for j, col2 in enumerate(num_cols):\n        if i &lt; j:\n            print(f\"{col1:20s} vs {col2:20s}: r = {corr.iloc[i,j]:6.3f}, p = {pval.iloc[i,j]:.4f}\")\n\n=== 상관계수 (위) 및 p-value (아래) ===\nbill_length_mm       vs bill_depth_mm       : r = -0.235, p = 0.0000\nbill_length_mm       vs flipper_length_mm   : r =  0.656, p = 0.0000\nbill_length_mm       vs body_mass_g         : r =  0.595, p = 0.0000\nbill_depth_mm        vs flipper_length_mm   : r = -0.584, p = 0.0000\nbill_depth_mm        vs body_mass_g         : r = -0.472, p = 0.0000\nflipper_length_mm    vs body_mass_g         : r =  0.871, p = 0.0000",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#상관-분석-실무-가이드",
    "href": "part2/08. 상관 분석.html#상관-분석-실무-가이드",
    "title": "18  상관 분석",
    "section": "18.6 상관 분석 실무 가이드",
    "text": "18.6 상관 분석 실무 가이드\n상관 분석 활용 분야\n\n\n\n분야\n활용\n\n\n\n\n탐색적 데이터 분석 (EDA)\n변수 간 관계 파악\n\n\n피처 선택\n다중공선성 확인, 중복 변수 제거\n\n\n회귀 모델링\n독립변수 선택, 다중공선성 진단\n\n\n이상치 탐지\n예상 밖의 낮은/높은 상관 확인\n\n\n가설 생성\n추가 연구 방향 설정\n\n\n\n상관 분석 체크리스트\n\n산점도로 관계 형태 확인 (선형/비선형)\n이상치 확인 및 처리\n적절한 상관계수 선택 (Pearson/Spearman/Kendall)\np-value 확인 (통계적 유의성)\n효과 크기 고려 (상관계수 절댓값)\n제3의 변수 가능성 검토\n인과관계로 오해하지 않기\n\n주의사항\n\n상관 ≠ 인과: “A와 B가 상관있다”는 “A가 B를 야기한다”를 의미하지 않음\n비선형 관계: 상관계수가 낮아도 강한 비선형 관계 가능\n제3의 변수: 두 변수의 상관이 다른 변수 때문일 수 있음 (교란변수)\nSimpson’s Paradox: 전체와 부분에서 상관의 방향이 반대일 수 있음\n표본 크기: n이 작으면 우연한 상관이 유의하게 나올 수 있음",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part2/08. 상관 분석.html#요약",
    "href": "part2/08. 상관 분석.html#요약",
    "title": "18  상관 분석",
    "section": "18.7 요약",
    "text": "18.7 요약\n이 장에서는 두 변수 간 관계를 측정하는 상관 분석을 학습했다. 주요 내용은 다음과 같다.\n상관계수 종합 정리\n\n\n\n상황\n권장 방법\n이유\n\n\n\n\n선형 관계 + 정규성\nPearson\n검정력 높음, 해석 명확\n\n\n단조 관계 + 비정규\nSpearman\n정규성 불필요, 이상치 강건\n\n\n소표본 + 동순위 많음\nKendall\n안정적, 동순위 처리 우수\n\n\n이상치 많음\nSpearman 또는 Kendall\nPearson보다 강건\n\n\n순서형 데이터\nSpearman 또는 Kendall\n순위 기반\n\n\n\n상관 분석 의사결정 흐름\n데이터 확인\n  ↓\n산점도 작성\n  ├─ 선형 관계?\n  │    ├─ Yes → 정규성 확인\n  │    │         ├─ Yes → Pearson\n  │    │         └─ No → Spearman\n  │    └─ No → 단조 관계?\n  │              ├─ Yes → Spearman 또는 Kendall\n  │              └─ No → 비선형 모델 고려\n  └─ 이상치 많음? → Spearman 또는 Kendall\n핵심 포인트\n\nPearson: 선형 관계의 표준, 가장 널리 사용\nSpearman: 단조 관계, 이상치에 강건\nKendall: 보수적, 소표본과 동순위에 강함\n상관 ≠ 인과: 항상 주의\n시각화 필수: 산점도로 관계 형태 확인\n\n상관 분석은 변수 간 관계를 탐색하고 이해하는 기본 도구이다. 적절한 상관계수를 선택하고, 결과를 올바르게 해석하며, 인과관계로 오해하지 않는 것이 중요하다. 다음으로는 상관 분석을 확장한 회귀 분석을 학습할 수 있다.",
    "crumbs": [
      "Ⅱ. 통계분석 및 가설검정",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>상관 분석</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html",
    "href": "part3/01. 데이터 분할 및 검정.html",
    "title": "19  데이터 분할 및 검증",
    "section": "",
    "text": "19.1 데이터 분할의 필요성\n데이터 분할(Data Splitting)과 검증(Validation)은 머신러닝 모델의 일반화 성능을 평가하는 핵심 과정이다. 모델의 성능은 알고리즘 자체보다 데이터를 어떻게 나누고 검증했는지에 더 크게 좌우된다. 잘못된 데이터 분할은 과적합을 발견하지 못하고 모델 성능을 과대평가하게 만든다. 이 장에서는 올바른 데이터 분할 방법과 교차 검증 기법을 학습한다.\n예제: 데이터 로드\n머신러닝 모델은 학습 데이터에 최적화되므로, 동일한 데이터로 평가하면 성능이 과대평가된다.\n과적합과 일반화\n데이터 분할의 목적\n잘못된 평가의 예",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#데이터-분할의-필요성",
    "href": "part3/01. 데이터 분할 및 검정.html#데이터-분할의-필요성",
    "title": "19  데이터 분할 및 검증",
    "section": "",
    "text": "개념\n설명\n문제점\n\n\n\n\n과적합 (Overfitting)\n학습 데이터에 지나치게 최적화\n새로운 데이터에서 성능 저하\n\n\n과소적합 (Underfitting)\n학습 데이터조차 제대로 학습 못함\n전반적으로 낮은 성능\n\n\n일반화 (Generalization)\n보지 못한 데이터에도 잘 작동\n목표\n\n\n\n\n\n일반화 성능 평가: 모델이 새로운 데이터에서 얼마나 잘 작동하는가?\n과적합 탐지: 학습 성능과 검증 성능의 차이 확인\n모델 선택: 여러 모델 중 가장 좋은 모델 선택\n하이퍼파라미터 튜닝: 최적의 설정값 찾기\n공정한 비교: 동일한 기준으로 모델 성능 비교\n\n\n# ❌ 잘못된 예: 학습 데이터로 평가\nmodel.fit(X, y)\nscore = model.score(X, y)  # 과대평가!\n\n# ✓ 올바른 예: 별도의 테스트 데이터로 평가\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)  # 올바른 평가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#데이터-분할의-종류",
    "href": "part3/01. 데이터 분할 및 검정.html#데이터-분할의-종류",
    "title": "19  데이터 분할 및 검증",
    "section": "19.2 데이터 분할의 종류",
    "text": "19.2 데이터 분할의 종류\n데이터셋의 역할\n\n\n\n데이터셋\n영어\n역할\n사용 시점\n\n\n\n\n학습\nTrain\n모델 파라미터 학습\n매 에포크/반복\n\n\n검증\nValidation\n하이퍼파라미터 튜닝, 조기 종료\n학습 중\n\n\n테스트\nTest\n최종 성능 평가\n단 한 번 (마지막)\n\n\n\n일반적인 분할 비율\n\n\n\n전체 데이터 크기\n분할 비율 (Train/Val/Test)\n설명\n\n\n\n\n매우 큼 (100만+)\n98/1/1\n충분한 데이터, 작은 비율도 충분\n\n\n큼 (10만+)\n90/5/5 또는 80/10/10\n일반적 상황\n\n\n중간 (1만+)\n70/15/15 또는 60/20/20\n표준 비율\n\n\n작음 (1천+)\n교차 검증 + 80/20\n검증셋 없이 CV 사용\n\n\n매우 작음 (100+)\nLeave-One-Out CV\n모든 데이터 활용",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#기본-데이터-분할",
    "href": "part3/01. 데이터 분할 및 검정.html#기본-데이터-분할",
    "title": "19  데이터 분할 및 검증",
    "section": "19.3 기본 데이터 분할",
    "text": "19.3 기본 데이터 분할\n\n19.3.1 학습/테스트 분할\n예제: 데이터 준비\n\n# 특성과 타겟 분리\nX = df_ml[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]]\ny = df_ml[\"species\"]\n\nprint(\"특성 행렬(X) 크기:\", X.shape)\nprint(\"타겟 벡터(y) 크기:\", y.shape)\nprint(\"\\n타겟 클래스 분포:\")\nprint(y.value_counts())\nprint(f\"\\n클래스 비율:\")\nprint((y.value_counts() / len(y)).round(3))\n\n특성 행렬(X) 크기: (333, 4)\n타겟 벡터(y) 크기: (333,)\n\n타겟 클래스 분포:\nspecies\nAdelie       146\nGentoo       119\nChinstrap     68\nName: count, dtype: int64\n\n클래스 비율:\nspecies\nAdelie       0.438\nGentoo       0.357\nChinstrap    0.204\nName: count, dtype: float64\n\n\n예제: 학습/테스트 분할\n\n# 학습/테스트 분할 (80/20)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,        # 테스트 20%\n    random_state=42,       # 재현성\n    stratify=y            # 클래스 비율 유지\n)\n\nprint(\"=== 데이터 분할 결과 ===\")\nprint(f\"전체 데이터: {len(X)}\")\nprint(f\"학습 데이터: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\nprint(f\"테스트 데이터: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n\n# 클래스 비율 확인\nprint(\"\\n=== 클래스 비율 비교 ===\")\nprint(\"전체 데이터:\")\nprint((y.value_counts() / len(y)).round(3))\nprint(\"\\n학습 데이터:\")\nprint((y_train.value_counts() / len(y_train)).round(3))\nprint(\"\\n테스트 데이터:\")\nprint((y_test.value_counts() / len(y_test)).round(3))\n\n=== 데이터 분할 결과 ===\n전체 데이터: 333\n학습 데이터: 266 (79.9%)\n테스트 데이터: 67 (20.1%)\n\n=== 클래스 비율 비교 ===\n전체 데이터:\nspecies\nAdelie       0.438\nGentoo       0.357\nChinstrap    0.204\nName: count, dtype: float64\n\n학습 데이터:\nspecies\nAdelie       0.440\nGentoo       0.357\nChinstrap    0.203\nName: count, dtype: float64\n\n테스트 데이터:\nspecies\nAdelie       0.433\nGentoo       0.358\nChinstrap    0.209\nName: count, dtype: float64\n\n\ntrain_test_split 주요 파라미터\n\n\n\n파라미터\n설명\n권장값\n\n\n\n\ntest_size\n테스트셋 비율\n0.2 ~ 0.3\n\n\nrandom_state\n난수 시드 (재현성)\n고정값 (예: 42)\n\n\nstratify\n클래스 비율 유지\n분류 문제에서 y\n\n\nshuffle\n섞기 여부\nTrue (기본값)\n\n\n\n\n\n19.3.2 학습/검증/테스트 분할\n하이퍼파라미터 튜닝이 필요한 경우 3-way 분할을 수행한다.\n예제: 3-way 분할\n\n# 1단계: 학습+검증 vs 테스트 (80/20)\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n# 2단계: 학습 vs 검증 (60/20)\n# temp의 25%는 전체의 20% (0.8 * 0.25 = 0.2)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp,\n    test_size=0.25,  # temp의 25% = 전체의 20%\n    random_state=42,\n    stratify=y_temp\n)\n\nprint(\"=== 3-way 데이터 분할 결과 ===\")\nprint(f\"전체 데이터: {len(X)}\")\nprint(f\"학습 데이터: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\nprint(f\"검증 데이터: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\nprint(f\"테스트 데이터: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n\n# 시각화\nsizes = [len(X_train), len(X_val), len(X_test)]\nlabels = ['Train (60%)', 'Validation (20%)', 'Test (20%)']\ncolors = ['#3498db', '#2ecc71', '#e74c3c']\n\nplt.figure(figsize=(10, 6))\nplt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\nplt.title(\"Data Split: Train/Validation/Test\")\nplt.axis('equal')\nplt.show()\n\n=== 3-way 데이터 분할 결과 ===\n전체 데이터: 333\n학습 데이터: 199 (59.8%)\n검증 데이터: 67 (20.1%)\n테스트 데이터: 67 (20.1%)\n\n\n\n\n\n\n\n\n\n3-way 분할 사용 흐름\n학습 데이터 (Train)\n  ↓\n모델 학습 (fit)\n  ↓\n검증 데이터 (Validation)\n  ↓\n하이퍼파라미터 튜닝\n  ↓\n최종 모델 선택\n  ↓\n테스트 데이터 (Test)\n  ↓\n최종 성능 평가 (단 한 번!)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#교차-검증-cross-validation",
    "href": "part3/01. 데이터 분할 및 검정.html#교차-검증-cross-validation",
    "title": "19  데이터 분할 및 검증",
    "section": "19.4 교차 검증 (Cross-Validation)",
    "text": "19.4 교차 검증 (Cross-Validation)\n교차 검증은 데이터를 여러 번 나누어 학습과 검증을 반복함으로써 더 안정적이고 신뢰할 수 있는 성능 추정을 제공한다.\n교차 검증의 장점\n\n\n\n장점\n설명\n\n\n\n\n안정적 평가\n단일 분할의 운(luck)에 의존하지 않음\n\n\n데이터 효율성\n모든 데이터를 학습과 검증에 사용\n\n\n분산 추정\n성능의 평균과 표준편차 확인 가능\n\n\n과적합 탐지\n폴드 간 성능 차이로 과적합 감지\n\n\n\n교차 검증 vs 홀드아웃\n\n\n\n항목\n홀드아웃 (단일 분할)\n교차 검증\n\n\n\n\n계산 비용\n낮음\n높음 (k배)\n\n\n안정성\n낮음 (운에 좌우)\n높음\n\n\n데이터 효율\n낮음\n높음 (모든 데이터 활용)\n\n\n성능 분산\n알 수 없음\n측정 가능\n\n\n소표본 적합성\n부적합\n적합\n\n\n사용 시점\n빠른 실험\n최종 평가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#k-fold-교차-검증",
    "href": "part3/01. 데이터 분할 및 검정.html#k-fold-교차-검증",
    "title": "19  데이터 분할 및 검증",
    "section": "19.5 K-Fold 교차 검증",
    "text": "19.5 K-Fold 교차 검증\nK-Fold는 데이터를 K개의 폴드로 나누어 각 폴드를 한 번씩 검증셋으로 사용하는 방법이다.\nK-Fold 원리\n전체 데이터를 K개로 분할\n\nFold 1: [Validation] [Train] [Train] [Train] [Train]\nFold 2: [Train] [Validation] [Train] [Train] [Train]\nFold 3: [Train] [Train] [Validation] [Train] [Train]\nFold 4: [Train] [Train] [Train] [Validation] [Train]\nFold 5: [Train] [Train] [Train] [Train] [Validation]\n\n→ K번 학습 후 평균 성능 계산\n예제: K-Fold 교차 검증\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# 모델 정의\nmodel = LogisticRegression(max_iter=1000, random_state=42)\n\n# K-Fold 설정\nkf = KFold(\n    n_splits=5,         # 5개 폴드\n    shuffle=True,        # 섞기\n    random_state=42      # 재현성\n)\n\n# 교차 검증 수행\nscores = cross_val_score(\n    model, X, y,\n    cv=kf,\n    scoring='accuracy'\n)\n\nprint(\"=== K-Fold 교차 검증 결과 ===\")\nprint(f\"각 폴드 정확도: {scores}\")\nprint(f\"평균 정확도: {scores.mean():.4f}\")\nprint(f\"표준편차: {scores.std():.4f}\")\nprint(f\"95% 신뢰구간: {scores.mean():.4f} ± {1.96 * scores.std():.4f}\")\n\n=== K-Fold 교차 검증 결과 ===\n각 폴드 정확도: [0.98507463 0.97014925 0.98507463 1.         0.98484848]\n평균 정확도: 0.9850\n표준편차: 0.0094\n95% 신뢰구간: 0.9850 ± 0.0185\n\n\n예제: 폴드별 성능 시각화\n\n# 폴드별 성능 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(scores)+1), scores, 'o-', linewidth=2, markersize=8)\nplt.axhline(scores.mean(), color='r', linestyle='--', linewidth=2, \n            label=f'Mean = {scores.mean():.4f}')\nplt.fill_between(range(1, len(scores)+1), \n                 scores.mean() - scores.std(), \n                 scores.mean() + scores.std(), \n                 alpha=0.2, color='r', label=f'Std = {scores.std():.4f}')\nplt.xlabel('Fold')\nplt.ylabel('Accuracy')\nplt.title('K-Fold Cross-Validation Results')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.xticks(range(1, len(scores)+1))\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#stratified-k-fold",
    "href": "part3/01. 데이터 분할 및 검정.html#stratified-k-fold",
    "title": "19  데이터 분할 및 검증",
    "section": "19.6 Stratified K-Fold",
    "text": "19.6 Stratified K-Fold\nStratified K-Fold는 각 폴드에서 클래스 비율을 유지하는 K-Fold의 변형으로, 불균형 데이터에서 필수적이다.\n왜 필요한가?\n일반 K-Fold는 클래스를 고려하지 않으므로: - 한 폴드에 특정 클래스가 몰릴 수 있음 - 폴드 간 성능 차이가 클래스 불균형 때문일 수 있음 - 소수 클래스가 일부 폴드에 없을 수 있음\n예제: Stratified K-Fold\n\nfrom sklearn.model_selection import StratifiedKFold\n\n# Stratified K-Fold 설정\nskf = StratifiedKFold(\n    n_splits=5,\n    shuffle=True,\n    random_state=42\n)\n\n# 교차 검증 수행\nscores_stratified = cross_val_score(\n    model, X, y,\n    cv=skf,\n    scoring='accuracy'\n)\n\nprint(\"=== Stratified K-Fold 교차 검증 결과 ===\")\nprint(f\"각 폴드 정확도: {scores_stratified}\")\nprint(f\"평균 정확도: {scores_stratified.mean():.4f}\")\nprint(f\"표준편차: {scores_stratified.std():.4f}\")\n\n# K-Fold vs Stratified K-Fold 비교\nprint(\"\\n=== K-Fold vs Stratified K-Fold 비교 ===\")\nprint(f\"K-Fold:           평균 = {scores.mean():.4f}, 표준편차 = {scores.std():.4f}\")\nprint(f\"Stratified K-Fold: 평균 = {scores_stratified.mean():.4f}, 표준편차 = {scores_stratified.std():.4f}\")\n\n=== Stratified K-Fold 교차 검증 결과 ===\n각 폴드 정확도: [0.97014925 0.98507463 0.98507463 1.         0.98484848]\n평균 정확도: 0.9850\n표준편차: 0.0094\n\n=== K-Fold vs Stratified K-Fold 비교 ===\nK-Fold:           평균 = 0.9850, 표준편차 = 0.0094\nStratified K-Fold: 평균 = 0.9850, 표준편차 = 0.0094\n\n\n예제: 폴드별 클래스 분포 확인\n\n# 폴드별 클래스 분포 확인\nprint(\"\\n=== 각 폴드의 클래스 분포 ===\")\nfor fold_idx, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n    y_fold = y.iloc[val_idx]\n    print(f\"\\nFold {fold_idx}:\")\n    print(f\"  크기: {len(y_fold)}\")\n    print(f\"  클래스 비율:\")\n    print((y_fold.value_counts() / len(y_fold)).round(3).to_dict())\n\n\n=== 각 폴드의 클래스 분포 ===\n\nFold 1:\n  크기: 67\n  클래스 비율:\n{'Adelie': 0.448, 'Gentoo': 0.358, 'Chinstrap': 0.194}\n\nFold 2:\n  크기: 67\n  클래스 비율:\n{'Adelie': 0.433, 'Gentoo': 0.358, 'Chinstrap': 0.209}\n\nFold 3:\n  크기: 67\n  클래스 비율:\n{'Adelie': 0.433, 'Gentoo': 0.358, 'Chinstrap': 0.209}\n\nFold 4:\n  크기: 66\n  클래스 비율:\n{'Adelie': 0.439, 'Gentoo': 0.348, 'Chinstrap': 0.212}\n\nFold 5:\n  크기: 66\n  클래스 비율:\n{'Adelie': 0.439, 'Gentoo': 0.364, 'Chinstrap': 0.197}",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#기타-교차-검증-방법",
    "href": "part3/01. 데이터 분할 및 검정.html#기타-교차-검증-방법",
    "title": "19  데이터 분할 및 검증",
    "section": "19.7 기타 교차 검증 방법",
    "text": "19.7 기타 교차 검증 방법\n교차 검증 방법 비교\n\n\n\n\n\n\n\n\n방법\n설명\n사용 상황\n\n\n\n\nK-Fold\n데이터를 K개로 분할\n회귀, 균형 분류\n\n\nStratified K-Fold\n클래스 비율 유지\n불균형 분류 (권장)\n\n\nLeave-One-Out (LOO)\nK = n (각 샘플을 검증셋으로)\n매우 소량 데이터\n\n\nLeave-P-Out (LPO)\nP개씩 조합\n극소량 데이터\n\n\nShuffle Split\n무작위 분할 반복\n빠른 근사 평가\n\n\nGroup K-Fold\n그룹 단위 분할\n그룹 종속 데이터\n\n\nTime Series Split\n시간 순서 유지\n시계열 데이터\n\n\n\n\n19.7.1 Time Series Split (시계열 교차 검증)\n시계열 데이터는 미래를 예측하므로, 학습 데이터가 검증 데이터보다 과거여야 한다.\n예제: Time Series Split\n\nfrom sklearn.model_selection import TimeSeriesSplit\n\n# Time Series Split 설정\ntscv = TimeSeriesSplit(n_splits=5)\n\n# 분할 시각화\nprint(\"=== Time Series Split 구조 ===\")\nfor fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n    print(f\"Fold {fold_idx}:\")\n    print(f\"  Train: {min(train_idx):3d} ~ {max(train_idx):3d} (n={len(train_idx)})\")\n    print(f\"  Val:   {min(val_idx):3d} ~ {max(val_idx):3d} (n={len(val_idx)})\")\n\n=== Time Series Split 구조 ===\nFold 1:\n  Train:   0 ~  57 (n=58)\n  Val:    58 ~ 112 (n=55)\nFold 2:\n  Train:   0 ~ 112 (n=113)\n  Val:   113 ~ 167 (n=55)\nFold 3:\n  Train:   0 ~ 167 (n=168)\n  Val:   168 ~ 222 (n=55)\nFold 4:\n  Train:   0 ~ 222 (n=223)\n  Val:   223 ~ 277 (n=55)\nFold 5:\n  Train:   0 ~ 277 (n=278)\n  Val:   278 ~ 332 (n=55)\n\n\n시계열 데이터 주의사항\n\n❌ 일반 K-Fold 사용 금지 (미래 데이터로 학습)\n✓ TimeSeriesSplit 사용\n✓ 학습 데이터 &lt; 검증 데이터 (시간 순서)\n✓ 데이터 섞기(shuffle) 금지",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#교차-검증-실무-가이드",
    "href": "part3/01. 데이터 분할 및 검정.html#교차-검증-실무-가이드",
    "title": "19  데이터 분할 및 검증",
    "section": "19.8 교차 검증 실무 가이드",
    "text": "19.8 교차 검증 실무 가이드\n교차 검증 선택 가이드\n분류 문제?\n├─ Yes → 클래스 불균형?\n│         ├─ Yes → Stratified K-Fold (필수)\n│         └─ No → K-Fold 또는 Stratified K-Fold\n└─ No (회귀) → 시계열?\n                ├─ Yes → TimeSeriesSplit\n                └─ No → K-Fold\nK 값 선택\n\n\n\n데이터 크기\n권장 K\n이유\n\n\n\n\nn &lt; 100\nk = n (LOO)\n데이터 최대 활용\n\n\n100 ≤ n &lt; 1000\nk = 10\n표준, 계산 가능\n\n\nn ≥ 1000\nk = 5\n계산 효율성\n\n\nn ≥ 10000\nk = 3 또는 홀드아웃\n빠른 평가",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#데이터-누수-data-leakage-방지",
    "href": "part3/01. 데이터 분할 및 검정.html#데이터-누수-data-leakage-방지",
    "title": "19  데이터 분할 및 검증",
    "section": "19.9 데이터 누수 (Data Leakage) 방지",
    "text": "19.9 데이터 누수 (Data Leakage) 방지\n데이터 누수는 학습 과정에서 테스트 정보가 유입되어 성능이 과대평가되는 현상이다.\n데이터 누수의 원인\n\n\n\n\n\n\n\n\n원인\n예시\n올바른 방법\n\n\n\n\n전처리 순서 오류\n전체 데이터 스케일링 후 분할\n분할 후 학습 데이터로만 스케일링\n\n\n피처 선택 오류\n전체 데이터로 피처 선택\n분할 후 학습 데이터로만 선택\n\n\n타겟 인코딩 오류\n전체 데이터로 타겟 인코딩\n분할 후 학습 데이터로만 인코딩\n\n\n검증셋 재사용\n검증셋으로 여러 번 튜닝\n최종은 테스트셋으로 단 한 번\n\n\n\n올바른 파이프라인\n# ✓ 올바른 예: 분할 후 전처리\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# 1. 데이터 분할\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# 2. 파이프라인 (분할 후 전처리)\npipeline = Pipeline([\n    ('scaler', StandardScaler()),  # 학습 데이터로만 fit\n    ('model', LogisticRegression())\n])\n\n# 3. 학습 (scaler는 X_train으로만 fit됨)\npipeline.fit(X_train, y_train)\n\n# 4. 평가 (scaler 파라미터는 학습 때 학습됨)\nscore = pipeline.score(X_test, y_test)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/01. 데이터 분할 및 검정.html#요약",
    "href": "part3/01. 데이터 분할 및 검정.html#요약",
    "title": "19  데이터 분할 및 검증",
    "section": "19.10 요약",
    "text": "19.10 요약\n이 장에서는 데이터 분할과 교차 검증을 학습했다. 주요 내용은 다음과 같다.\n데이터 분할 핵심\n\n목적: 일반화 성능 평가, 과적합 탐지\n기본 분할: Train/Validation/Test (60/20/20)\nStratify: 분류 문제에서 클래스 비율 유지 필수\n테스트셋: 단 한 번만 사용 (최종 평가)\n\n교차 검증 핵심\n\nK-Fold: 데이터를 K개로 나누어 반복 평가\nStratified K-Fold: 분류 문제에서 권장 (클래스 비율 유지)\nTime Series Split: 시계열 데이터 전용\n장점: 안정적 평가, 모든 데이터 활용\n\n실무 체크리스트\n\n분류 문제인가? → Stratify 사용\n데이터가 적은가? → 교차 검증 사용\n하이퍼파라미터 튜닝? → 검증셋 분리\n시계열 데이터? → TimeSeriesSplit 사용\n데이터 누수 방지? → 분할 후 전처리\n최종 평가? → 테스트셋은 마지막에 단 한 번\n\n주의사항\n\n테스트셋은 절대 학습에 사용하지 않음\n전처리(스케일링 등)는 분할 후 수행\n검증셋을 여러 번 보면 간접적인 과적합\n시계열은 반드시 TimeSeriesSplit 사용\n\n올바른 데이터 분할과 검증은 신뢰할 수 있는 모델 평가의 기초이다. 다음 장에서는 모델 성능을 측정하는 다양한 평가 지표를 학습할 것이다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>데이터 분할 및 검증</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html",
    "href": "part3/02. 특성 선택.html",
    "title": "20  특성 선택",
    "section": "",
    "text": "20.1 특성 선택의 개념과 필요성\n특성 선택(Feature Selection)은 모델 학습에 사용되는 입력 변수 중에서 중요한 특성만 선별하는 과정이다. 모든 변수를 사용하는 것이 항상 최선은 아니며, 불필요하거나 중복된 특성은 오히려 모델 성능을 저하시키고 과적합을 유발할 수 있다. 이 장에서는 필터(Filter), 래퍼(Wrapper), 임베디드(Embedded) 방법의 원리와 실무 적용 전략을 학습한다.\n예제: 데이터 로드\n특성 선택은 데이터의 차원을 줄이고 모델 성능을 향상시키는 핵심 기법이다.\n특성 선택의 목적\n특성이 많을 때의 문제",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#특성-선택의-개념과-필요성",
    "href": "part3/02. 특성 선택.html#특성-선택의-개념과-필요성",
    "title": "20  특성 선택",
    "section": "",
    "text": "목적\n설명\n효과\n\n\n\n\n모델 성능 향상\n과적합 감소, 일반화 성능 향상\n테스트 정확도 증가\n\n\n학습 시간 단축\n변수 수 감소로 계산량 감소\n실험 속도 향상\n\n\n해석력 향상\n중요 변수만 사용하여 이해 쉬움\n비즈니스 인사이트 도출\n\n\n차원의 저주 방지\n고차원 데이터 문제 완화\n샘플 수 대비 변수 수 적절화\n\n\n다중공선성 제거\n상관성 높은 변수 제거\n모델 안정성 향상\n\n\n비용 절감\n데이터 수집/저장/처리 비용 감소\n실무 효율성 증가\n\n\n\n\n\n\n\n문제\n설명\n\n\n\n\n차원의 저주\n변수가 많아질수록 필요한 샘플 수가 기하급수적 증가\n\n\n과적합\n노이즈 변수가 학습에 포함되어 일반화 실패\n\n\n계산 비용\n학습 시간과 메모리 사용량 증가\n\n\n다중공선성\n상관성 높은 변수들이 모델 불안정하게 만듦\n\n\n해석 어려움\n변수가 많아 중요 요인 파악 곤란\n\n\n\n\n20.1.1 특성 선택 vs 특성 추출\n혼동하기 쉬운 개념을 명확히 구분한다.\n특성 선택 vs 특성 추출 비교\n\n\n\n\n\n\n\n\n구분\n특성 선택 (Feature Selection)\n특성 추출 (Feature Extraction)\n\n\n\n\n정의\n기존 변수 중 일부를 선택\n기존 변수를 변환하여 새로운 변수 생성\n\n\n변수 의미\n유지됨\n변경됨 (조합/변환)\n\n\n해석력\n높음\n낮음\n\n\n예시\n100개 중 10개 선택\nPCA로 10개 주성분 생성\n\n\n대표 방법\n상관계수, RFE, Lasso\nPCA, t-SNE, Autoencoder\n\n\n원본 변수\n그대로 사용\n변환하여 사용\n\n\n\n예제: 특성 선택 vs 특성 추출\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# 특성 선택: 원본 변수 중 k개 선택\nselector = SelectKBest(score_func=f_classif, k=2)\nX_selected = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support()]\n\nprint(\"=== 특성 선택 ===\")\nprint(f\"선택된 변수: {selected_features.tolist()}\")\nprint(f\"결과 차원: {X_selected.shape}\")\n\n# 특성 추출: 새로운 변수 생성\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\nprint(\"\\n=== 특성 추출 (PCA) ===\")\nprint(f\"주성분 변수: PC1, PC2\")\nprint(f\"결과 차원: {X_pca.shape}\")\nprint(f\"설명 분산 비율: {pca.explained_variance_ratio_}\")\n\n=== 특성 선택 ===\n선택된 변수: ['bill_length_mm', 'flipper_length_mm']\n결과 차원: (333, 2)\n\n=== 특성 추출 (PCA) ===\n주성분 변수: PC1, PC2\n결과 차원: (333, 2)\n설명 분산 비율: [9.99893229e-01 7.82232504e-05]",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#특성-선택-방법의-분류",
    "href": "part3/02. 특성 선택.html#특성-선택-방법의-분류",
    "title": "20  특성 선택",
    "section": "20.2 특성 선택 방법의 분류",
    "text": "20.2 특성 선택 방법의 분류\n특성 선택 방법은 크게 세 가지로 분류된다.\n특성 선택 방법 분류\n\n\n\n방법\n모델 사용\n계산 비용\n성능\n특징\n\n\n\n\n필터 (Filter)\n사용 안 함\n낮음\n중간\n통계량 기반, 빠름\n\n\n래퍼 (Wrapper)\n사용함\n매우 높음\n높음\n모델 성능 기반, 느림\n\n\n임베디드 (Embedded)\n사용함\n중간\n높음\n학습 과정에 내장",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#필터filter-방법",
    "href": "part3/02. 특성 선택.html#필터filter-방법",
    "title": "20  특성 선택",
    "section": "20.3 필터(Filter) 방법",
    "text": "20.3 필터(Filter) 방법\n필터 방법은 모델을 사용하지 않고 데이터의 통계적 특성만으로 각 특성을 독립적으로 평가한다.\n필터 방법의 특징\n\n\n\n장점\n단점\n\n\n\n\n계산 속도가 매우 빠름\n변수 간 상호작용 고려 못함\n\n\n모델과 무관하게 사용 가능\n모델별 최적화 불가\n\n\n대용량 데이터에 적합\n성능이 래퍼보다 낮을 수 있음\n\n\n과적합 위험 낮음\n단순한 선형 관계만 파악\n\n\n\n\n20.3.1 분산 기반 선택\n분산이 거의 없는 변수는 정보량이 적으므로 제거한다.\n예제: 분산 기반 선택\n\nfrom sklearn.feature_selection import VarianceThreshold\n\n# 데이터에 분산이 낮은 변수 추가 (예시)\nX_with_low_var = X.copy()\nX_with_low_var['constant_feature'] = 1  # 분산 = 0\nX_with_low_var['low_variance_feature'] = np.random.uniform(3.9, 4.1, len(X))\n\nprint(\"=== 분산 확인 ===\")\nprint(X_with_low_var.var().sort_values())\n\n# 분산 임계값 설정 (분산 &lt; 0.1인 변수 제거)\nselector = VarianceThreshold(threshold=0.1)\nX_high_var = selector.fit_transform(X_with_low_var)\n\nprint(f\"\\n원본 변수 수: {X_with_low_var.shape[1]}\")\nprint(f\"선택된 변수 수: {X_high_var.shape[1]}\")\nprint(f\"제거된 변수: {X_with_low_var.columns[~selector.get_support()].tolist()}\")\n\n=== 분산 확인 ===\nconstant_feature             0.000000\nlow_variance_feature         0.003033\nbill_depth_mm                3.877888\nbill_length_mm              29.906333\nflipper_length_mm          196.441677\nbody_mass_g             648372.487699\ndtype: float64\n\n원본 변수 수: 6\n선택된 변수 수: 4\n제거된 변수: ['constant_feature', 'low_variance_feature']\n\n\n\n\n20.3.2 상관계수 기반 선택\n타겟 변수와의 상관관계를 측정하여 중요한 변수를 선택한다.\n예제: 상관계수 기반 선택\n\n# 분류 문제를 위해 타겟을 수치형으로 변환\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# 각 특성과 타겟 간 상관계수 계산\ncorrelations = {}\nfor col in X.columns:\n    corr = np.corrcoef(X[col], y_encoded)[0, 1]\n    correlations[col] = abs(corr)\n\n# 상관계수로 정렬\ncorr_df = pd.DataFrame({\n    'Feature': list(correlations.keys()),\n    'Correlation': list(correlations.values())\n}).sort_values('Correlation', ascending=False)\n\nprint(\"=== 타겟과의 상관계수 ===\")\nprint(corr_df)\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(corr_df['Feature'], corr_df['Correlation'])\nplt.xlabel('Absolute Correlation with Target')\nplt.title('Feature Correlation with Target')\nplt.tight_layout()\nplt.show()\n\n=== 타겟과의 상관계수 ===\n             Feature  Correlation\n2  flipper_length_mm     0.850737\n3        body_mass_g     0.750434\n1      bill_depth_mm     0.740346\n0     bill_length_mm     0.730548\n\n\n\n\n\n\n\n\n\n\n\n20.3.3 통계적 검정 (SelectKBest)\n통계적 유의성을 기반으로 상위 k개 변수를 선택한다.\n예제: SelectKBest (ANOVA F-test)\n\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\n\n# ANOVA F-test 기반 선택\nselector = SelectKBest(score_func=f_classif, k=2)\nX_selected = selector.fit_transform(X, y)\n\n# 점수 확인\nscores = pd.DataFrame({\n    'Feature': X.columns,\n    'Score': selector.scores_\n}).sort_values('Score', ascending=False)\n\nprint(\"=== SelectKBest (ANOVA F-test) ===\")\nprint(scores)\nprint(f\"\\n선택된 변수: {X.columns[selector.get_support()].tolist()}\")\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.bar(scores['Feature'], scores['Score'])\nplt.xlabel('Feature')\nplt.ylabel('F-score')\nplt.title('Feature Importance (ANOVA F-test)')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()\n\n=== SelectKBest (ANOVA F-test) ===\n             Feature       Score\n2  flipper_length_mm  567.406992\n0     bill_length_mm  397.299437\n1      bill_depth_mm  344.825082\n3        body_mass_g  341.894895\n\n선택된 변수: ['bill_length_mm', 'flipper_length_mm']\n\n\n\n\n\n\n\n\n\n통계적 검정 함수\n\n\n\n함수\n용도\n설명\n\n\n\n\nf_classif\n분류\nANOVA F-통계량\n\n\nchi2\n분류 (양수만)\n카이제곱 검정\n\n\nf_regression\n회귀\nF-통계량\n\n\nmutual_info_classif\n분류\n상호정보량\n\n\nmutual_info_regression\n회귀\n상호정보량",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#래퍼wrapper-방법",
    "href": "part3/02. 특성 선택.html#래퍼wrapper-방법",
    "title": "20  특성 선택",
    "section": "20.4 래퍼(Wrapper) 방법",
    "text": "20.4 래퍼(Wrapper) 방법\n래퍼 방법은 모델의 성능을 기준으로 특성 조합을 평가하여 특정 모델에 최적화된 선택이 가능하다.\n래퍼 방법의 특징\n\n\n\n장점\n단점\n\n\n\n\n모델 성능 최적화\n계산 비용 매우 높음\n\n\n변수 간 상호작용 고려\n특정 모델에 종속적\n\n\n최적 조합 탐색\n대용량 데이터 부적합\n\n\n높은 정확도\n과적합 위험\n\n\n\n\n20.4.1 재귀적 특성 제거 (RFE)\nRFE는 모델을 학습하고 중요도가 낮은 특성을 재귀적으로 제거하는 방법이다.\nRFE 작동 원리\n\n모든 특성으로 모델 학습\n특성 중요도 계산\n가장 중요도 낮은 특성 제거\n원하는 특성 개수까지 반복\n\n예제: RFE\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# 모델 정의\nmodel = LogisticRegression(max_iter=1000)\n\n# RFE 수행\nrfe = RFE(estimator=model, n_features_to_select=2)\nX_rfe = rfe.fit_transform(X, y)\n\n# 결과 확인\nrfe_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Selected': rfe.support_,\n    'Ranking': rfe.ranking_\n}).sort_values('Ranking')\n\nprint(\"=== RFE 결과 ===\")\nprint(rfe_df)\nprint(f\"\\n선택된 변수: {X.columns[rfe.support_].tolist()}\")\n\n# 시각화\nplt.figure(figsize=(10, 5))\ncolors = ['green' if s else 'red' for s in rfe.support_]\nplt.bar(X.columns, 1/rfe.ranking_, color=colors)\nplt.xlabel('Feature')\nplt.ylabel('Importance (1/Ranking)')\nplt.title('RFE Feature Selection')\nplt.legend(['Selected', 'Not Selected'])\nplt.tight_layout()\nplt.show()\n\n=== RFE 결과 ===\n             Feature  Selected  Ranking\n0     bill_length_mm      True        1\n1      bill_depth_mm      True        1\n2  flipper_length_mm     False        2\n3        body_mass_g     False        3\n\n선택된 변수: ['bill_length_mm', 'bill_depth_mm']\n\n\n\n\n\n\n\n\n\n\n\n20.4.2 RFECV (교차 검증 포함)\nRFECV는 교차 검증을 통해 최적의 특성 개수를 자동으로 찾는다.\n예제: RFECV\n\nfrom sklearn.feature_selection import RFECV\n\n# RFECV 수행\nrfecv = RFECV(estimator=model, cv=5, scoring='accuracy')\nX_rfecv = rfecv.fit_transform(X, y)\n\nprint(\"=== RFECV 결과 ===\")\nprint(f\"최적 변수 수: {rfecv.n_features_}\")\nprint(f\"선택된 변수: {X.columns[rfecv.support_].tolist()}\")\n\n# 변수 개수별 성능 시각화\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), \n         rfecv.cv_results_['mean_test_score'], marker='o')\nplt.axvline(rfecv.n_features_, color='r', linestyle='--', \n            label=f'Optimal: {rfecv.n_features_} features')\nplt.xlabel('Number of Features')\nplt.ylabel('Cross-Validation Score')\nplt.title('RFECV: Optimal Number of Features')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n=== RFECV 결과 ===\n최적 변수 수: 4\n선택된 변수: ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n\n\n\n\n\n\n\n\n\n\n\n20.4.3 순차 특성 선택\n예제: 전진 선택 (Sequential Feature Selector)\n\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\n# 전진 선택\nsfs_forward = SequentialFeatureSelector(\n    model, \n    n_features_to_select=2, \n    direction='forward',\n    cv=5\n)\nX_forward = sfs_forward.fit_transform(X, y)\n\nprint(\"=== 전진 선택 (Forward Selection) ===\")\nprint(f\"선택된 변수: {X.columns[sfs_forward.get_support()].tolist()}\")\n\n# 후진 제거\nsfs_backward = SequentialFeatureSelector(\n    model, \n    n_features_to_select=2, \n    direction='backward',\n    cv=5\n)\nX_backward = sfs_backward.fit_transform(X, y)\n\nprint(\"\\n=== 후진 제거 (Backward Elimination) ===\")\nprint(f\"선택된 변수: {X.columns[sfs_backward.get_support()].tolist()}\")\n\n=== 전진 선택 (Forward Selection) ===\n선택된 변수: ['bill_length_mm', 'flipper_length_mm']\n\n\n\n=== 후진 제거 (Backward Elimination) ===\n선택된 변수: ['bill_length_mm', 'bill_depth_mm']",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#임베디드embedded-방법",
    "href": "part3/02. 특성 선택.html#임베디드embedded-방법",
    "title": "20  특성 선택",
    "section": "20.5 임베디드(Embedded) 방법",
    "text": "20.5 임베디드(Embedded) 방법\n임베디드 방법은 모델 학습 과정 자체에 특성 선택이 포함되어 있어 필터와 래퍼의 중간 성격을 가진다.\n임베디드 방법의 특징\n\n\n\n장점\n단점\n\n\n\n\n계산 효율과 성능의 균형\n특정 알고리즘에 종속\n\n\n변수 간 상호작용 고려\n모든 모델에 적용 불가\n\n\n래퍼보다 빠름\n설명력이 필터보다 낮을 수 있음\n\n\n모델 학습과 동시 수행\n하이퍼파라미터에 민감\n\n\n\n\n20.5.1 L1 정규화 (Lasso)\nL1 정규화는 중요하지 않은 특성의 계수를 정확히 0으로 만들어 자동 선택한다.\n예제: Lasso 기반 선택\n\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.preprocessing import StandardScaler\n\n# 데이터 표준화 (Lasso는 스케일에 민감)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Lasso (회귀이므로 타겟을 연속형으로 변환)\n# 분류 문제이므로 LogisticRegression with L1 사용\nfrom sklearn.linear_model import LogisticRegression\n\nlasso = LogisticRegression(penalty='l1', solver='saga', C=1.0)\nlasso.fit(X_scaled, y)\n\n# 계수 확인\ncoef_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': np.abs(lasso.coef_[0])\n}).sort_values('Coefficient', ascending=False)\n\nprint(\"=== Lasso (L1 정규화) ===\")\nprint(coef_df)\nprint(f\"\\n0이 아닌 계수: {(coef_df['Coefficient'] &gt; 0.01).sum()}개\")\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(coef_df['Feature'], coef_df['Coefficient'])\nplt.xlabel('Absolute Coefficient')\nplt.title('Lasso Feature Importance')\nplt.tight_layout()\nplt.show()\n\n=== Lasso (L1 정규화) ===\n             Feature  Coefficient\n0     bill_length_mm     4.978380\n1      bill_depth_mm     1.980757\n2  flipper_length_mm     0.000000\n3        body_mass_g     0.000000\n\n0이 아닌 계수: 2개\n\n\n\n\n\n\n\n\n\n\n\n20.5.2 트리 기반 특성 중요도\n결정트리, 랜덤 포레스트, Gradient Boosting 등은 특성 중요도를 자동으로 제공한다.\n예제: 랜덤 포레스트 특성 중요도\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 랜덤 포레스트 학습\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf.fit(X, y)\n\n# 특성 중요도\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"=== 랜덤 포레스트 특성 중요도 ===\")\nprint(importance_df)\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Feature Importance')\nplt.title('Random Forest Feature Importance')\nplt.tight_layout()\nplt.show()\n\n=== 랜덤 포레스트 특성 중요도 ===\n             Feature  Importance\n0     bill_length_mm    0.407645\n2  flipper_length_mm    0.323960\n1      bill_depth_mm    0.192569\n3        body_mass_g    0.075826\n\n\n\n\n\n\n\n\n\n예제: SelectFromModel\n\nfrom sklearn.feature_selection import SelectFromModel\n\n# 중요도 기반 자동 선택\nselector = SelectFromModel(rf, threshold='median')\nX_important = selector.fit_transform(X, y)\n\nprint(f\"\\n선택된 변수 수: {X_important.shape[1]}\")\nprint(f\"선택된 변수: {X.columns[selector.get_support()].tolist()}\")\n\n\n선택된 변수 수: 2\n선택된 변수: ['bill_length_mm', 'flipper_length_mm']",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#방법별-종합-비교",
    "href": "part3/02. 특성 선택.html#방법별-종합-비교",
    "title": "20  특성 선택",
    "section": "20.6 방법별 종합 비교",
    "text": "20.6 방법별 종합 비교\n특성 선택 방법 종합 비교\n\n\n\n방법\n계산 비용\n성능\n해석력\n모델 의존성\n적용 상황\n\n\n\n\n필터\n낮음\n중간\n높음\n낮음\n대용량, 빠른 탐색\n\n\n래퍼\n매우 높음\n높음\n중간\n높음\n소규모, 성능 중시\n\n\n임베디드\n중간\n높음\n중간\n중간\n일반적 상황 (권장)\n\n\n\n방법 선택 가이드\n데이터 크기\n├─ 대용량 (변수 100+) → 필터 방법\n│                      ↓\n│                     임베디드 방법으로 정제\n└─ 소규모 (변수 &lt; 50) → 래퍼 또는 임베디드\n\n목적\n├─ 빠른 탐색 → 필터 (상관계수, SelectKBest)\n├─ 최고 성능 → 래퍼 (RFECV)\n└─ 균형 잡힌 접근 → 임베디드 (Lasso, 트리 중요도)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#실무-적용-전략",
    "href": "part3/02. 특성 선택.html#실무-적용-전략",
    "title": "20  특성 선택",
    "section": "20.7 실무 적용 전략",
    "text": "20.7 실무 적용 전략\n단계별 특성 선택 전략\n\n\n\n단계\n방법\n목적\n\n\n\n\n1단계 (거름망)\n분산 기반\n분산 없는 변수 제거\n\n\n2단계 (예비 선택)\n필터 (상관계수)\n명백히 무관한 변수 제거\n\n\n3단계 (정제)\n임베디드 (트리 중요도)\n중요 변수 추출\n\n\n4단계 (최적화)\n래퍼 (RFECV)\n최종 조합 선택\n\n\n5단계 (검증)\n교차 검증\n성능 확인\n\n\n\n예제: 종합 파이프라인\n\nfrom sklearn.pipeline import Pipeline\n\n# 1. 분산 제거\nvar_threshold = VarianceThreshold(threshold=0.01)\n\n# 2. SelectKBest (상위 3개)\nselect_k = SelectKBest(f_classif, k=3)\n\n# 3. 모델\nmodel_final = LogisticRegression(max_iter=1000)\n\n# 파이프라인 구성\npipeline = Pipeline([\n    ('variance', var_threshold),\n    ('select', select_k),\n    ('model', model_final)\n])\n\n# 학습 및 평가\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\npipeline.fit(X_train, y_train)\n\nprint(\"=== 파이프라인 결과 ===\")\nprint(f\"학습 정확도: {pipeline.score(X_train, y_train):.4f}\")\nprint(f\"테스트 정확도: {pipeline.score(X_test, y_test):.4f}\")\n\n=== 파이프라인 결과 ===\n학습 정확도: 0.9887\n테스트 정확도: 1.0000",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/02. 특성 선택.html#요약",
    "href": "part3/02. 특성 선택.html#요약",
    "title": "20  특성 선택",
    "section": "20.8 요약",
    "text": "20.8 요약\n이 장에서는 특성 선택의 개념과 세 가지 주요 방법을 학습했다. 주요 내용은 다음과 같다.\n특성 선택 핵심 요약\n\n목적: 과적합 방지, 성능 향상, 해석력 증대, 비용 절감\n필터: 통계량 기반, 빠르지만 상호작용 고려 못함\n래퍼: 모델 성능 기반, 정확하지만 느림\n임베디드: 학습 과정에 통합, 균형 잡힌 접근\n\n실무 권장사항\n\n대용량 데이터: 필터 → 임베디드 순서로 적용\n소규모 데이터: 임베디드 또는 래퍼 사용\n해석 중요: 필터 또는 Lasso 선호\n성능 우선: RFECV 또는 트리 중요도 사용\n파이프라인 구성: 여러 방법을 순차적으로 조합\n\n주의사항\n\n특성 선택은 데이터 분할 후 학습셋으로만 수행\n교차 검증으로 안정성 확인\n도메인 지식과 통계적 방법 결합\n제거된 변수도 문서화하여 추후 검토\n\n특성 선택은 모델 성능과 해석력을 동시에 향상시키는 강력한 도구이다. 데이터 특성과 목적에 맞는 방법을 선택하고, 여러 방법을 조합하여 최적의 변수 집합을 찾는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>특성 선택</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html",
    "href": "part3/03. 차원 축소.html",
    "title": "21  차원 축소",
    "section": "",
    "text": "21.1 차원 축소의 개념과 필요성\n차원 축소(Dimensionality Reduction)는 여러 개의 입력 변수(차원)를 더 적은 수의 변수로 변환하는 과정이다. 원본 특성을 그대로 사용하지 않고 새로운 특성(축)을 만들어 데이터를 표현하는 방법으로, 특성 추출(Feature Extraction)을 포함하는 개념이다. 차원 축소는 고차원 데이터의 시각화, 학습 속도 향상, 일반화 성능 개선에 핵심적인 역할을 한다. 이 장에서는 PCA, t-SNE, UMAP 등 주요 차원 축소 기법의 원리와 실무 활용법을 학습한다.\n예제: 데이터 로드\n차원 축소는 고차원 데이터를 저차원으로 변환하여 데이터를 효율적으로 표현하고 분석한다.\n차원 축소가 필요한 이유\n차원 축소의 기대 효과",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#차원-축소의-개념과-필요성",
    "href": "part3/03. 차원 축소.html#차원-축소의-개념과-필요성",
    "title": "21  차원 축소",
    "section": "",
    "text": "문제\n설명\n차원 축소로 해결\n\n\n\n\n차원의 저주\n변수가 많아질수록 필요한 샘플 수 기하급수적 증가\n핵심 변수만 유지\n\n\n시각화 어려움\n3차원 이상은 직접 시각화 불가\n2D/3D로 변환\n\n\n학습 시간 증가\n변수가 많을수록 계산량 증가\n연산량 감소\n\n\n다중공선성\n상관성 높은 변수들이 모델 불안정화\n독립적인 축 생성\n\n\n노이즈\n불필요한 변수가 노이즈 포함\n주요 신호만 추출\n\n\n메모리 사용\n고차원 데이터는 저장 공간 많이 차지\n데이터 압축\n\n\n\n\n\n\n\n효과\n설명\n\n\n\n\n데이터 구조 단순화\n복잡한 데이터를 간결하게 표현\n\n\n학습 속도 향상\n변수 수 감소로 연산량 감소\n\n\n일반화 성능 개선\n과적합 위험 감소\n\n\n시각화 가능\n2D/3D로 변환하여 패턴 파악\n\n\n저장 공간 절약\n데이터 압축 효과\n\n\n\n\n21.1.1 특성 선택 vs 차원 축소 (특성 추출)\n특성 선택 vs 차원 축소 비교\n\n\n\n\n\n\n\n\n구분\n특성 선택 (Feature Selection)\n차원 축소 (Feature Extraction)\n\n\n\n\n정의\n기존 변수 중 일부를 선택\n기존 변수를 변환하여 새로운 변수 생성\n\n\n원본 변수\n그대로 유지\n변환/조합\n\n\n변수 의미\n유지됨\n해석 어려워짐\n\n\n정보 손실\n선택 안 된 변수의 정보 손실\n압축 과정에서 일부 손실\n\n\n예시\n상관계수로 3개 변수 선택\nPCA로 2개 주성분 생성\n\n\n해석력\n높음\n낮음\n\n\n적용\n도메인 지식 중요\n데이터 구조 의존\n\n\n\n예제: 특성 선택 vs 차원 축소\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.decomposition import PCA\n\n# 데이터 표준화\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# 특성 선택: 원본 변수 중 2개 선택\nselector = SelectKBest(score_func=f_classif, k=2)\nX_selected = selector.fit_transform(X, y)\nselected_features = X.columns[selector.get_support()]\n\nprint(\"=== 특성 선택 ===\")\nprint(f\"선택된 변수: {selected_features.tolist()}\")\nprint(f\"변수 의미: 원본 그대로 유지\")\n\n# 차원 축소: 새로운 2개 주성분 생성\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"\\n=== 차원 축소 (PCA) ===\")\nprint(f\"생성된 변수: PC1, PC2\")\nprint(f\"변수 의미: 원본 변수들의 선형 조합\")\nprint(f\"설명 분산: {pca.explained_variance_ratio_}\")\n\n=== 특성 선택 ===\n선택된 변수: ['bill_length_mm', 'flipper_length_mm']\n변수 의미: 원본 그대로 유지\n\n=== 차원 축소 (PCA) ===\n생성된 변수: PC1, PC2\n변수 의미: 원본 변수들의 선형 조합\n설명 분산: [0.68633893 0.19452929]",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#선형-차원-축소-pca",
    "href": "part3/03. 차원 축소.html#선형-차원-축소-pca",
    "title": "21  차원 축소",
    "section": "21.2 선형 차원 축소: PCA",
    "text": "21.2 선형 차원 축소: PCA\nPCA(Principal Component Analysis, 주성분 분석)는 가장 널리 사용되는 선형 차원 축소 방법이다.\nPCA의 핵심 아이디어\n\n데이터의 분산을 가장 잘 설명하는 방향을 찾음\n기존 변수의 선형 결합으로 새로운 축 생성\n새로운 축을 주성분(Principal Component)이라 부름\n각 주성분은 서로 직교(독립)\n\nPCA의 작동 원리\n\n데이터 표준화 (평균 0, 분산 1)\n공분산 행렬 계산\n고유값과 고유벡터 계산\n고유값이 큰 순서로 주성분 선택\n원본 데이터를 새로운 주성분 공간으로 투영\n\nPCA의 특징\n\n\n\n특징\n설명\n\n\n\n\n선형 변환\n원본 변수의 선형 조합\n\n\n분산 최대화\n데이터가 가장 퍼진 방향 찾기\n\n\n직교성\n주성분들은 서로 독립\n\n\n스케일 민감\n표준화 필수\n\n\n비지도 학습\n타겟 정보 사용 안 함\n\n\n해석 가능\n각 주성분의 기여도 확인 가능\n\n\n\n\n21.2.1 PCA 실습\n예제: PCA 적용\n\nfrom sklearn.decomposition import PCA\n\n# 데이터 표준화\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# PCA 적용 (2차원으로 축소)\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\nprint(\"=== PCA 결과 ===\")\nprint(f\"원본 차원: {X.shape[1]}\")\nprint(f\"축소 차원: {X_pca.shape[1]}\")\nprint(f\"\\n각 주성분의 설명 분산 비율:\")\nfor i, var in enumerate(pca.explained_variance_ratio_, 1):\n    print(f\"  PC{i}: {var:.4f} ({var*100:.2f}%)\")\nprint(f\"\\n누적 설명 분산: {pca.explained_variance_ratio_.sum():.4f} ({pca.explained_variance_ratio_.sum()*100:.2f}%)\")\n\n=== PCA 결과 ===\n원본 차원: 4\n축소 차원: 2\n\n각 주성분의 설명 분산 비율:\n  PC1: 0.6863 (68.63%)\n  PC2: 0.1945 (19.45%)\n\n누적 설명 분산: 0.8809 (88.09%)\n\n\n예제: 주성분 구성 확인\n\n# 각 주성분을 구성하는 원본 변수의 기여도\ncomponents_df = pd.DataFrame(\n    pca.components_,\n    columns=X.columns,\n    index=[f'PC{i+1}' for i in range(pca.n_components_)]\n)\n\nprint(\"\\n=== 주성분 구성 (변수별 기여도) ===\")\nprint(components_df.round(3))\n\n# 시각화\nplt.figure(figsize=(10, 5))\nsns.heatmap(components_df, annot=True, cmap='coolwarm', center=0, \n            cbar_kws={'label': 'Contribution'})\nplt.title(\"PCA Component Loadings\")\nplt.tight_layout()\nplt.show()\n\n\n=== 주성분 구성 (변수별 기여도) ===\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nPC1           0.454         -0.399              0.577        0.550\nPC2           0.600          0.796              0.006        0.076\n\n\n\n\n\n\n\n\n\n예제: 설명 분산 시각화\n\n# 모든 주성분의 설명 분산\npca_full = PCA()\npca_full.fit(X_scaled)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 개별 설명 분산\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n            pca_full.explained_variance_ratio_)\naxes[0].set_xlabel('Principal Component')\naxes[0].set_ylabel('Explained Variance Ratio')\naxes[0].set_title('Variance Explained by Each Component')\naxes[0].set_xticks(range(1, len(pca_full.explained_variance_ratio_)+1))\n\n# 누적 설명 분산\ncumsum = np.cumsum(pca_full.explained_variance_ratio_)\naxes[1].plot(range(1, len(cumsum)+1), cumsum, 'o-', linewidth=2)\naxes[1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\naxes[1].set_xlabel('Number of Components')\naxes[1].set_ylabel('Cumulative Explained Variance')\naxes[1].set_title('Cumulative Variance Explained')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\naxes[1].set_xticks(range(1, len(cumsum)+1))\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n95% 분산 설명에 필요한 주성분 수: {np.argmax(cumsum &gt;= 0.95) + 1}\")\n\n\n\n\n\n\n\n\n\n95% 분산 설명에 필요한 주성분 수: 3\n\n\n예제: PCA 시각화\n\n# 2D 시각화\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y.astype('category').cat.codes, \n                     cmap='viridis', alpha=0.6, edgecolors='k')\nplt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\nplt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.title('PCA: 2D Projection')\nplt.colorbar(scatter, label='Species')\nplt.legend(handles=scatter.legend_elements()[0], labels=y.unique())\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n21.2.2 PCA 적정 차원 수 선택\n차원 수 선택 기준\n\n\n\n\n\n\n\n\n방법\n설명\n기준\n\n\n\n\n누적 설명 분산\n전체 분산의 일정 비율 유지\n일반적으로 85~95%\n\n\nElbow Method\n설명 분산 증가율이 급감하는 지점\n그래프에서 꺾이는 점\n\n\nKaiser Rule\n고유값 &gt; 1인 주성분 선택\n표준화 데이터 기준\n\n\n도메인 지식\n해석 가능한 차원 수\n실무 목적에 맞게\n\n\n\n예제: 적정 차원 수 자동 선택\n\n# 95% 분산 설명하는 차원 수 자동 선택\npca_auto = PCA(n_components=0.95)\nX_pca_auto = pca_auto.fit_transform(X_scaled)\n\nprint(f\"=== 자동 차원 선택 (95% 분산) ===\")\nprint(f\"선택된 주성분 수: {pca_auto.n_components_}\")\nprint(f\"실제 설명 분산: {pca_auto.explained_variance_ratio_.sum():.4f}\")\n\n=== 자동 차원 선택 (95% 분산) ===\n선택된 주성분 수: 3\n실제 설명 분산: 0.9730",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#비선형-차원-축소",
    "href": "part3/03. 차원 축소.html#비선형-차원-축소",
    "title": "21  차원 축소",
    "section": "21.3 비선형 차원 축소",
    "text": "21.3 비선형 차원 축소\n선형 변환으로는 복잡한 데이터 구조를 표현하기 어려운 경우 비선형 차원 축소를 사용한다.\n선형 vs 비선형 차원 축소\n\n\n\n구분\n선형 (PCA)\n비선형 (t-SNE, UMAP)\n\n\n\n\n가정\n데이터가 선형 부분공간에 존재\n데이터가 비선형 다양체에 존재\n\n\n변환\n선형 조합\n비선형 변환\n\n\n전역 구조\n잘 보존\n보존 정도 다름\n\n\n국소 구조\n보존 못할 수 있음\n잘 보존\n\n\n주 용도\n전처리, 압축\n시각화\n\n\n역변환\n가능\n불가능\n\n\n\n\n21.3.1 t-SNE\nt-SNE(t-Distributed Stochastic Neighbor Embedding)는 데이터 간 이웃 관계를 확률적으로 모델링하는 비선형 차원 축소 방법이다.\nt-SNE의 특징\n\n\n\n특징\n설명\n\n\n\n\n비선형\n복잡한 구조 표현 가능\n\n\n국소 구조 보존\n가까운 점들을 가깝게 유지\n\n\n시각화 특화\n주로 2D/3D 시각화 목적\n\n\n계산 비용 높음\n대용량 데이터에 느림\n\n\n비결정적\n초기화에 따라 결과 다름\n\n\n역변환 불가\n모델 입력용 부적합\n\n\n\nt-SNE 하이퍼파라미터\n\n\n\n파라미터\n설명\n권장값\n\n\n\n\nn_components\n출력 차원\n2 또는 3\n\n\nperplexity\n이웃 수 (5~50)\n30 (기본값)\n\n\nlearning_rate\n학습률\n200 (기본값)\n\n\nn_iter\n최적화 반복 횟수\n1000+\n\n\nrandom_state\n재현성\n고정값\n\n\n\n예제: t-SNE 적용\n\nfrom sklearn.manifold import TSNE\n\n# t-SNE 적용\ntsne = TSNE(n_components=2, random_state=42, perplexity=30)\nX_tsne = tsne.fit_transform(X_scaled)\n\nprint(\"=== t-SNE 결과 ===\")\nprint(f\"원본 차원: {X.shape[1]}\")\nprint(f\"축소 차원: {X_tsne.shape[1]}\")\n\n# 시각화\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y.astype('category').cat.codes,\n                     cmap='viridis', alpha=0.6, edgecolors='k')\nplt.xlabel('t-SNE Component 1')\nplt.ylabel('t-SNE Component 2')\nplt.title('t-SNE: 2D Projection')\nplt.legend(handles=scatter.legend_elements()[0], labels=y.unique())\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n=== t-SNE 결과 ===\n원본 차원: 4\n축소 차원: 2\n\n\n\n\n\n\n\n\n\n예제: Perplexity 영향 확인\n\n# 다양한 perplexity 값으로 실험\nperplexities = [5, 30, 50]\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\nfor idx, perp in enumerate(perplexities):\n    tsne_temp = TSNE(n_components=2, random_state=42, perplexity=perp)\n    X_tsne_temp = tsne_temp.fit_transform(X_scaled)\n    \n    scatter = axes[idx].scatter(X_tsne_temp[:, 0], X_tsne_temp[:, 1], \n                               c=y.astype('category').cat.codes,\n                               cmap='viridis', alpha=0.6, edgecolors='k')\n    axes[idx].set_title(f't-SNE (perplexity={perp})')\n    axes[idx].set_xlabel('Component 1')\n    axes[idx].set_ylabel('Component 2')\n    axes[idx].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n21.3.2 UMAP\nUMAP(Uniform Manifold Approximation and Projection)은 데이터가 저차원 다양체에 놓여 있다고 가정하고, 전역 구조와 국소 구조를 동시에 보존하려는 비선형 차원 축소 방법이다.\nUMAP의 특징\n\n\n\n특징\n설명\n\n\n\n\nt-SNE보다 빠름\n대규모 데이터 처리 가능\n\n\n전역+국소 구조 보존\n균형 잡힌 표현\n\n\n재현성 좋음\n같은 설정에서 일관된 결과\n\n\n다목적\n시각화 + 전처리 모두 가능\n\n\n하이퍼파라미터 민감\n튜닝 필요\n\n\n\nUMAP 하이퍼파라미터\n\n\n\n파라미터\n설명\n권장값\n\n\n\n\nn_components\n출력 차원\n2 또는 3\n\n\nn_neighbors\n이웃 수\n15 (기본값)\n\n\nmin_dist\n점 간 최소 거리\n0.1 (기본값)\n\n\nmetric\n거리 측정 방법\n‘euclidean’\n\n\nrandom_state\n재현성\n고정값\n\n\n\n예제: UMAP 적용\n\n# UMAP 설치 필요: pip install umap-learn\ntry:\n    import umap\n    \n    # UMAP 적용\n    umap_model = umap.UMAP(n_components=2, random_state=42)\n    X_umap = umap_model.fit_transform(X_scaled)\n    \n    print(\"=== UMAP 결과 ===\")\n    print(f\"원본 차원: {X.shape[1]}\")\n    print(f\"축소 차원: {X_umap.shape[1]}\")\n    \n    # 시각화\n    plt.figure(figsize=(10, 6))\n    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=y.astype('category').cat.codes,\n                         cmap='viridis', alpha=0.6, edgecolors='k')\n    plt.xlabel('UMAP Component 1')\n    plt.ylabel('UMAP Component 2')\n    plt.title('UMAP: 2D Projection')\n    plt.legend(handles=scatter.legend_elements()[0], labels=y.unique())\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \nexcept ImportError:\n    print(\"UMAP이 설치되지 않았습니다. 설치하려면: pip install umap-learn\")\n\nUMAP이 설치되지 않았습니다. 설치하려면: pip install umap-learn",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#방법-간-종합-비교",
    "href": "part3/03. 차원 축소.html#방법-간-종합-비교",
    "title": "21  차원 축소",
    "section": "21.4 방법 간 종합 비교",
    "text": "21.4 방법 간 종합 비교\nPCA vs t-SNE vs UMAP 비교\n\n\n\n구분\nPCA\nt-SNE\nUMAP\n\n\n\n\n유형\n선형\n비선형\n비선형\n\n\n속도\n매우 빠름\n느림\n빠름\n\n\n전역 구조 보존\n우수\n보통\n우수\n\n\n국소 구조 보존\n보통\n우수\n우수\n\n\n해석 가능성\n높음\n낮음\n낮음\n\n\n역변환\n가능\n불가능\n불가능\n\n\n재현성\n완벽\n낮음\n높음\n\n\n대용량 데이터\n적합\n부적합\n적합\n\n\n주 용도\n전처리, 압축, 시각화\n시각화\n시각화, 전처리\n\n\n모델 입력\n가능\n불가능\n가능\n\n\n\n예제: 세 방법 비교 시각화\n\n# 세 가지 방법 비교\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# PCA\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y.astype('category').cat.codes,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[0].set_title('PCA (Linear)')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[0].grid(True, alpha=0.3)\n\n# t-SNE\nscatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y.astype('category').cat.codes,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[1].set_title('t-SNE (Non-linear)')\naxes[1].set_xlabel('Component 1')\naxes[1].set_ylabel('Component 2')\naxes[1].grid(True, alpha=0.3)\n\n# UMAP (설치된 경우)\ntry:\n    scatter3 = axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y.astype('category').cat.codes,\n                               cmap='viridis', alpha=0.6, edgecolors='k')\n    axes[2].set_title('UMAP (Non-linear)')\n    axes[2].set_xlabel('Component 1')\n    axes[2].set_ylabel('Component 2')\n    axes[2].grid(True, alpha=0.3)\nexcept:\n    axes[2].text(0.5, 0.5, 'UMAP not installed', \n                ha='center', va='center', fontsize=14)\n    axes[2].set_title('UMAP (Not Available)')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#실무-활용-가이드",
    "href": "part3/03. 차원 축소.html#실무-활용-가이드",
    "title": "21  차원 축소",
    "section": "21.5 실무 활용 가이드",
    "text": "21.5 실무 활용 가이드\n사용 목적별 방법 선택\n\n\n\n목적\n권장 방법\n이유\n\n\n\n\n모델 학습 전 전처리\nPCA\n빠르고 안정적, 역변환 가능\n\n\n데이터 탐색 및 시각화\nt-SNE 또는 UMAP\n비선형 구조 잘 표현\n\n\n대용량 데이터 시각화\nUMAP\n속도와 품질의 균형\n\n\n노이즈 제거\nPCA\n주요 신호만 추출\n\n\n다중공선성 제거\nPCA\n독립적인 축 생성\n\n\n클러스터링 전처리\nPCA 또는 UMAP\n차원 줄여 효율성 향상\n\n\n\n데이터 크기별 권장\n\n\n\n샘플 수\n변수 수\n권장 방법\n\n\n\n\n&lt; 1,000\n&lt; 50\nPCA, t-SNE, UMAP 모두 가능\n\n\n&lt; 10,000\n50~100\nPCA, UMAP (t-SNE 주의)\n\n\n&gt; 10,000\n&gt; 100\nPCA, UMAP (t-SNE 피함)\n\n\n\n파이프라인 예제\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# PCA + 분류 모델 파이프라인\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('pca', PCA(n_components=0.95)),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\n# 교차 검증\nscores = cross_val_score(pipeline, X, y, cv=5)\n\nprint(\"=== PCA + RandomForest 파이프라인 ===\")\nprint(f\"교차 검증 정확도: {scores.mean():.4f} ± {scores.std():.4f}\")\n\n=== PCA + RandomForest 파이프라인 ===\n교차 검증 정확도: 0.9791 ± 0.0152",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#주의사항",
    "href": "part3/03. 차원 축소.html#주의사항",
    "title": "21  차원 축소",
    "section": "21.6 주의사항",
    "text": "21.6 주의사항\n차원 축소 사용 시 주의점\n\n\n\n\n\n\n\n\n주의사항\n설명\n대응 방법\n\n\n\n\n정보 손실\n차원 축소 시 일부 정보 손실 불가피\n설명 분산 비율 확인\n\n\n해석 어려움\n새로운 변수의 의미 파악 곤란\nPCA 로딩 분석\n\n\n타겟 미반영\n비지도 학습이라 타겟 고려 안 함\nLDA 등 지도 학습 차원 축소 고려\n\n\n스케일 민감\n변수 스케일에 따라 결과 달라짐\n반드시 표준화\n\n\n과적합 위험\nt-SNE 등은 학습 데이터에만 적용\n테스트셋 변환 시 주의\n\n\n하이퍼파라미터\n설정에 따라 결과 크게 달라짐\n여러 값 실험\n\n\n\n체크리스트\n\n데이터 표준화 수행\n적절한 방법 선택 (목적에 맞게)\n설명 분산 또는 재구성 오차 확인\n시각화로 결과 검증\n원본 데이터와 비교\n파이프라인에 통합",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/03. 차원 축소.html#요약",
    "href": "part3/03. 차원 축소.html#요약",
    "title": "21  차원 축소",
    "section": "21.7 요약",
    "text": "21.7 요약\n이 장에서는 고차원 데이터를 저차원으로 변환하는 차원 축소 기법을 학습했다. 주요 내용은 다음과 같다.\n핵심 요약\n\n목적: 차원의 저주 해결, 시각화, 학습 효율화\nPCA: 선형, 빠름, 해석 가능, 전처리 및 시각화\nt-SNE: 비선형, 시각화 특화, 국소 구조 보존\nUMAP: 비선형, 빠름, 전역+국소 보존, 다목적\n\n선택 가이드\n\n전처리 목적 → PCA\n시각화 목적 (소규모) → t-SNE\n시각화 목적 (대규모) → UMAP\n균형 잡힌 접근 → PCA로 시작, 필요시 UMAP\n\n차원 축소는 데이터의 본질적인 구조를 파악하고 효율적으로 분석하는 강력한 도구이다. 데이터 특성과 분석 목적에 맞는 방법을 선택하고, 결과를 시각화하여 검증하는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>차원 축소</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html",
    "href": "part3/04. 회귀 모델.html",
    "title": "22  회귀 모델",
    "section": "",
    "text": "22.1 회귀의 개념\n회귀 모델(Regression Models)은 입력 변수(X)와 연속형 타겟 변수(y) 사이의 관계를 학습하여 새로운 입력에 대한 수치값을 예측하는 지도 학습 모델이다. 예측 대상이 범주가 아닌 수치이며, 관계를 수학적 함수 형태로 모델링한다. 이 장에서는 선형 회귀부터 정규화 회귀(Ridge, Lasso, ElasticNet)까지 주요 회귀 모델의 원리와 실무 활용법을 학습한다.\n예제: 데이터 로드\n회귀는 입력 변수와 출력 변수 간의 관계를 모델링하여 예측하는 기법이다.\n회귀 vs 분류\n회귀 문제의 예",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#회귀의-개념",
    "href": "part3/04. 회귀 모델.html#회귀의-개념",
    "title": "22  회귀 모델",
    "section": "",
    "text": "구분\n회귀 (Regression)\n분류 (Classification)\n\n\n\n\n출력 타입\n연속형 (수치)\n범주형 (클래스)\n\n\n예측값\n실수 값\n클래스 라벨 또는 확률\n\n\n예시\n체중, 가격, 온도\n종, 합격/불합격, 질병 유무\n\n\n평가 지표\nMSE, MAE, R²\n정확도, 정밀도, 재현율\n\n\n대표 모델\n선형 회귀, Ridge, Lasso\n로지스틱 회귀, 결정 트리\n\n\n\n\n\n\n\n분야\n입력 변수\n출력 변수\n\n\n\n\n부동산\n면적, 방 개수, 위치\n주택 가격\n\n\n의료\n나이, BMI, 혈압\n혈당 수치\n\n\n마케팅\n광고비, 계절, 프로모션\n매출액\n\n\n생물학\n부리 길이, 날개 길이\n체중",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#데이터-준비-및-분할",
    "href": "part3/04. 회귀 모델.html#데이터-준비-및-분할",
    "title": "22  회귀 모델",
    "section": "22.2 데이터 준비 및 분할",
    "text": "22.2 데이터 준비 및 분할\n예제: 학습/테스트 분할\n\n# 데이터 분할\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"=== 데이터 분할 ===\")\nprint(f\"학습 데이터: {X_train.shape}\")\nprint(f\"테스트 데이터: {X_test.shape}\")\n\n# 타겟 분포 확인\nprint(\"\\n=== 타겟 분포 ===\")\nprint(f\"학습셋 평균: {y_train.mean():.2f}g, 표준편차: {y_train.std():.2f}g\")\nprint(f\"테스트셋 평균: {y_test.mean():.2f}g, 표준편차: {y_test.std():.2f}g\")\n\n=== 데이터 분할 ===\n학습 데이터: (266, 3)\n테스트 데이터: (67, 3)\n\n=== 타겟 분포 ===\n학습셋 평균: 4214.76g, 표준편차: 807.92g\n테스트셋 평균: 4176.49g, 표준편차: 799.68g",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#선형-회귀-linear-regression",
    "href": "part3/04. 회귀 모델.html#선형-회귀-linear-regression",
    "title": "22  회귀 모델",
    "section": "22.3 선형 회귀 (Linear Regression)",
    "text": "22.3 선형 회귀 (Linear Regression)\n선형 회귀는 입력 변수와 출력 변수 사이의 관계를 선형 결합으로 표현하는 가장 기본적인 회귀 모델이다.\n선형 회귀 수식\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\varepsilon\n\\]\n여기서: - \\(y\\): 타겟 변수 - \\(x_i\\): 입력 변수 - \\(\\beta_0\\): 절편 (intercept) - \\(\\beta_i\\): 회귀 계수 (coefficient) - \\(\\varepsilon\\): 오차항\n선형 회귀의 가정\n\n\n\n가정\n설명\n확인 방법\n\n\n\n\n선형성\nX와 y가 선형 관계\n잔차 플롯\n\n\n독립성\n관측치들이 서로 독립\n실험 설계 확인\n\n\n등분산성\n오차의 분산이 일정\n잔차 플롯\n\n\n정규성\n오차가 정규분포를 따름\nQ-Q plot, 히스토그램\n\n\n다중공선성 없음\n독립변수들이 서로 독립\nVIF, 상관행렬\n\n\n\n현실에서는 완벽히 만족하지 않는 경우가 많지만, 어느 정도 근사하면 사용 가능하다.\n\n22.3.1 선형 회귀 실습\n예제: 선형 회귀 학습\n\nfrom sklearn.linear_model import LinearRegression\n\n# 모델 생성 및 학습\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# 모델 파라미터 확인\nprint(\"=== 선형 회귀 계수 ===\")\ncoef_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': lr.coef_\n}).sort_values('Coefficient', key=abs, ascending=False)\nprint(coef_df)\nprint(f\"\\n절편 (intercept): {lr.intercept_:.2f}\")\n\n# 예측\ny_pred_train = lr.predict(X_train)\ny_pred_test = lr.predict(X_test)\n\n# 평가\nprint(\"\\n=== 모델 성능 ===\")\nprint(f\"학습 R²: {r2_score(y_train, y_pred_train):.4f}\")\nprint(f\"테스트 R²: {r2_score(y_test, y_pred_test):.4f}\")\nprint(f\"테스트 RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.2f}g\")\nprint(f\"테스트 MAE: {mean_absolute_error(y_test, y_pred_test):.2f}g\")\n\n=== 선형 회귀 계수 ===\n             Feature  Coefficient\n2  flipper_length_mm    50.247255\n1      bill_depth_mm    10.058133\n0     bill_length_mm     3.857683\n\n절편 (intercept): -6227.69\n\n=== 모델 성능 ===\n학습 R²: 0.7551\n테스트 R²: 0.7981\n테스트 RMSE: 356.65g\n테스트 MAE: 289.69g\n\n\n예제: 회귀 계수 시각화\n\n# 계수 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(coef_df['Feature'], coef_df['Coefficient'])\nplt.xlabel('Coefficient Value')\nplt.title('Linear Regression Coefficients')\nplt.axvline(x=0, color='red', linestyle='--', linewidth=1)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 예측 vs 실제값\n\n# 예측 vs 실제값 플롯\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_test, alpha=0.6, edgecolors='k')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n         'r--', linewidth=2, label='Perfect Prediction')\nplt.xlabel('Actual Body Mass (g)')\nplt.ylabel('Predicted Body Mass (g)')\nplt.title('Linear Regression: Actual vs Predicted')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# 상관계수\ncorr = np.corrcoef(y_test, y_pred_test)[0, 1]\nprint(f\"예측-실제 상관계수: {corr:.4f}\")\n\n\n\n\n\n\n\n\n예측-실제 상관계수: 0.8943\n\n\n예제: 잔차 분석\n\n# 잔차 계산\nresiduals = y_test - y_pred_test\n\n# 잔차 플롯\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# 잔차 vs 예측값\naxes[0].scatter(y_pred_test, residuals, alpha=0.6, edgecolors='k')\naxes[0].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[0].set_xlabel('Predicted Values')\naxes[0].set_ylabel('Residuals')\naxes[0].set_title('Residual Plot')\naxes[0].grid(True, alpha=0.3)\n\n# 잔차 히스토그램\naxes[1].hist(residuals, bins=20, edgecolor='black', alpha=0.7)\naxes[1].axvline(x=0, color='r', linestyle='--', linewidth=2)\naxes[1].set_xlabel('Residuals')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('Residual Distribution')\n\n# Q-Q plot\nfrom scipy import stats\nstats.probplot(residuals, dist=\"norm\", plot=axes[2])\naxes[2].set_title('Q-Q Plot of Residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n22.3.2 선형 회귀의 한계\n선형 회귀의 문제점\n\n\n\n문제\n설명\n증상\n\n\n\n\n다중공선성\n독립변수 간 높은 상관\n계수가 불안정, 부호 반전\n\n\n과적합\n학습 데이터에 과도 최적화\n학습 성능 &gt;&gt; 테스트 성능\n\n\n고차원 문제\n변수가 샘플보다 많음\n해가 무수히 많음\n\n\n이상치 민감\n극단값에 크게 영향받음\n계수가 왜곡됨\n\n\n\n이러한 문제를 해결하기 위해 정규화 회귀(Regularization)를 사용한다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#정규화regularization의-개념",
    "href": "part3/04. 회귀 모델.html#정규화regularization의-개념",
    "title": "22  회귀 모델",
    "section": "22.4 정규화(Regularization)의 개념",
    "text": "22.4 정규화(Regularization)의 개념\n정규화는 모델의 복잡도를 제한하여 과적합을 방지하는 기법이다.\n정규화가 필요한 이유\n\n계수가 너무 커지는 것을 방지\n불필요한 변수의 영향 축소\n과적합 감소\n모델의 일반화 성능 향상\n\n정규화 손실 함수\n\\[\n\\text{Loss} = \\text{MSE} + \\lambda \\times \\text{Penalty}\n\\]\n여기서: - MSE: 평균제곱오차 (예측 오차) - λ (lambda 또는 alpha): 규제 강도 - Penalty: L1, L2, 또는 둘의 조합\nλ (규제 강도) 효과\n\n\n\nλ 값\n효과\n\n\n\n\nλ = 0\n일반 선형 회귀 (규제 없음)\n\n\nλ 작음\n약한 규제, 계수 조금 축소\n\n\nλ 적절\n최적의 편향-분산 균형\n\n\nλ 큼\n강한 규제, 과소적합 위험",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#ridge-회귀-l2-정규화",
    "href": "part3/04. 회귀 모델.html#ridge-회귀-l2-정규화",
    "title": "22  회귀 모델",
    "section": "22.5 Ridge 회귀 (L2 정규화)",
    "text": "22.5 Ridge 회귀 (L2 정규화)\nRidge 회귀는 계수의 제곱합(L2 norm)에 패널티를 부여하는 방법이다.\nRidge 손실 함수\n\\[\n\\text{Loss}_{\\text{Ridge}} = \\text{MSE} + \\alpha \\sum_{i=1}^{p} \\beta_i^2\n\\]\nRidge의 특징\n\n\n\n특징\n설명\n\n\n\n\n계수 축소\n모든 계수를 0에 가깝게 만듦\n\n\n변수 유지\n계수를 0으로 만들지는 않음\n\n\n다중공선성 완화\n상관 높은 변수들 안정화\n\n\n해석\n모든 변수가 기여\n\n\n적용 상황\n변수 간 상관 높을 때\n\n\n\n예제: Ridge 회귀\n\nfrom sklearn.linear_model import Ridge\n\n# 데이터 표준화 (Ridge는 스케일에 민감)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Ridge 회귀\nridge = Ridge(alpha=1.0)\nridge.fit(X_train_scaled, y_train)\n\n# 계수 비교\nprint(\"=== Ridge vs 선형 회귀 계수 비교 ===\")\ncomparison_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Linear Regression': lr.coef_,\n    'Ridge (α=1.0)': ridge.coef_\n})\nprint(comparison_df.round(2))\n\n# 예측 및 평가\ny_pred_ridge = ridge.predict(X_test_scaled)\nprint(f\"\\n=== Ridge 성능 ===\")\nprint(f\"테스트 R²: {r2_score(y_test, y_pred_ridge):.4f}\")\nprint(f\"테스트 RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_ridge)):.2f}g\")\n\n=== Ridge vs 선형 회귀 계수 비교 ===\n             Feature  Linear Regression  Ridge (α=1.0)\n0     bill_length_mm               3.86          24.89\n1      bill_depth_mm              10.06          16.44\n2  flipper_length_mm              50.25         690.57\n\n=== Ridge 성능 ===\n테스트 R²: 0.7977\n테스트 RMSE: 356.98g\n\n\n예제: Alpha 값에 따른 계수 변화\n\n# 다양한 alpha 값으로 실험\nalphas = [0.01, 0.1, 1.0, 10.0, 100.0]\ncoefs = []\n\nfor alpha in alphas:\n    ridge_temp = Ridge(alpha=alpha)\n    ridge_temp.fit(X_train_scaled, y_train)\n    coefs.append(ridge_temp.coef_)\n\n# 계수 변화 시각화\ncoefs_df = pd.DataFrame(coefs, columns=X.columns, index=[f'α={a}' for a in alphas])\n\nplt.figure(figsize=(12, 6))\nfor col in coefs_df.columns:\n    plt.plot(range(len(alphas)), coefs_df[col], marker='o', label=col, linewidth=2)\n\nplt.xlabel('Regularization Strength')\nplt.ylabel('Coefficient Value')\nplt.title('Ridge: Coefficient Path')\nplt.xticks(range(len(alphas)), [f'α={a}' for a in alphas])\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axhline(y=0, color='black', linestyle='--', linewidth=1)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#lasso-회귀-l1-정규화",
    "href": "part3/04. 회귀 모델.html#lasso-회귀-l1-정규화",
    "title": "22  회귀 모델",
    "section": "22.6 Lasso 회귀 (L1 정규화)",
    "text": "22.6 Lasso 회귀 (L1 정규화)\nLasso 회귀는 계수의 절댓값 합(L1 norm)에 패널티를 부여하며, 일부 계수를 정확히 0으로 만든다.\nLasso 손실 함수\n\\[\n\\text{Loss}_{\\text{Lasso}} = \\text{MSE} + \\alpha \\sum_{i=1}^{p} |\\beta_i|\n\\]\nLasso의 특징\n\n\n\n특징\n설명\n\n\n\n\n계수 0화\n일부 계수를 정확히 0으로 만듦\n\n\n자동 변수 선택\n중요 변수만 남김\n\n\n희소 모델\n적은 변수로 설명\n\n\n해석 용이\n선택된 변수만 해석\n\n\n적용 상황\n변수 선택이 필요할 때\n\n\n\n예제: Lasso 회귀\n\nfrom sklearn.linear_model import Lasso\n\n# Lasso 회귀\nlasso = Lasso(alpha=0.1)\nlasso.fit(X_train_scaled, y_train)\n\n# 계수 비교\nprint(\"=== Lasso vs Ridge vs 선형 회귀 계수 비교 ===\")\ncomparison_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Linear Regression': lr.coef_,\n    'Ridge (α=1.0)': ridge.coef_,\n    'Lasso (α=0.1)': lasso.coef_\n})\nprint(comparison_df.round(2))\n\n# 0이 아닌 계수 개수\nn_nonzero = np.sum(lasso.coef_ != 0)\nprint(f\"\\nLasso가 선택한 변수 수: {n_nonzero}/{len(X.columns)}\")\n\n# 예측 및 평가\ny_pred_lasso = lasso.predict(X_test_scaled)\nprint(f\"\\n=== Lasso 성능 ===\")\nprint(f\"테스트 R²: {r2_score(y_test, y_pred_lasso):.4f}\")\nprint(f\"테스트 RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_lasso)):.2f}g\")\n\n=== Lasso vs Ridge vs 선형 회귀 계수 비교 ===\n             Feature  Linear Regression  Ridge (α=1.0)  Lasso (α=0.1)\n0     bill_length_mm               3.86          24.89          20.83\n1      bill_depth_mm              10.06          16.44          19.63\n2  flipper_length_mm              50.25         690.57         697.66\n\nLasso가 선택한 변수 수: 3/3\n\n=== Lasso 성능 ===\n테스트 R²: 0.7980\n테스트 RMSE: 356.68g\n\n\n예제: Lasso 변수 선택 시각화\n\n# 다양한 alpha 값에서 변수 선택\nalphas_lasso = [0.01, 0.1, 1.0, 10.0, 50.0]\ncoefs_lasso = []\nn_features = []\n\nfor alpha in alphas_lasso:\n    lasso_temp = Lasso(alpha=alpha, max_iter=10000)\n    lasso_temp.fit(X_train_scaled, y_train)\n    coefs_lasso.append(lasso_temp.coef_)\n    n_features.append(np.sum(lasso_temp.coef_ != 0))\n\n# 계수 경로 시각화\nplt.figure(figsize=(12, 6))\ncoefs_lasso_df = pd.DataFrame(coefs_lasso, columns=X.columns)\n\nfor col in coefs_lasso_df.columns:\n    plt.plot(range(len(alphas_lasso)), coefs_lasso_df[col], marker='o', label=col, linewidth=2)\n\nplt.xlabel('Regularization Strength')\nplt.ylabel('Coefficient Value')\nplt.title('Lasso: Coefficient Path (Automatic Feature Selection)')\nplt.xticks(range(len(alphas_lasso)), [f'α={a}\\n({n}개 변수)' for a, n in zip(alphas_lasso, n_features)])\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.axhline(y=0, color='black', linestyle='--', linewidth=1)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#elasticnet-l1-l2",
    "href": "part3/04. 회귀 모델.html#elasticnet-l1-l2",
    "title": "22  회귀 모델",
    "section": "22.7 ElasticNet (L1 + L2)",
    "text": "22.7 ElasticNet (L1 + L2)\nElasticNet은 Ridge와 Lasso의 패널티를 결합한 방법이다.\nElasticNet 손실 함수\n\\[\n\\text{Loss}_{\\text{ElasticNet}} = \\text{MSE} + \\alpha \\rho \\sum_{i=1}^{p} |\\beta_i| + \\alpha \\frac{1-\\rho}{2} \\sum_{i=1}^{p} \\beta_i^2\n\\]\n여기서: - α (alpha): 전체 규제 강도 - ρ (l1_ratio): L1 비율 (0~1)\nElasticNet 하이퍼파라미터\n\n\n\n파라미터\n범위\n효과\n\n\n\n\nalpha\n0 ~ ∞\n전체 규제 강도\n\n\nl1_ratio = 0\n-\nRidge와 동일\n\n\nl1_ratio = 1\n-\nLasso와 동일\n\n\nl1_ratio = 0.5\n-\n균형 잡힌 규제\n\n\n\nElasticNet의 특징\n\nRidge와 Lasso의 장점 결합\n상관 높은 변수들을 함께 선택/제거\n고차원 데이터에 적합\n변수 수 &gt; 샘플 수인 경우 유용\n\n예제: ElasticNet\n\nfrom sklearn.linear_model import ElasticNet\n\n# ElasticNet\nelastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\nelastic.fit(X_train_scaled, y_train)\n\n# 계수 비교\nprint(\"=== 모든 모델 계수 비교 ===\")\ncomparison_final = pd.DataFrame({\n    'Feature': X.columns,\n    'Linear': lr.coef_,\n    'Ridge': ridge.coef_,\n    'Lasso': lasso.coef_,\n    'ElasticNet': elastic.coef_\n})\nprint(comparison_final.round(2))\n\n# 예측 및 평가\ny_pred_elastic = elastic.predict(X_test_scaled)\nprint(f\"\\n=== ElasticNet 성능 ===\")\nprint(f\"테스트 R²: {r2_score(y_test, y_pred_elastic):.4f}\")\nprint(f\"테스트 RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_elastic)):.2f}g\")\n\n=== 모든 모델 계수 비교 ===\n             Feature  Linear   Ridge   Lasso  ElasticNet\n0     bill_length_mm    3.86   24.89   20.83       64.22\n1      bill_depth_mm   10.06   16.44   19.63      -17.30\n2  flipper_length_mm   50.25  690.57  697.66      616.16\n\n=== ElasticNet 성능 ===\n테스트 R²: 0.7891\n테스트 RMSE: 364.47g",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#회귀-모델-종합-비교",
    "href": "part3/04. 회귀 모델.html#회귀-모델-종합-비교",
    "title": "22  회귀 모델",
    "section": "22.8 회귀 모델 종합 비교",
    "text": "22.8 회귀 모델 종합 비교\n모델 특성 비교\n\n\n\n모델\n패널티\n변수 선택\n계수 크기\n다중공선성\n적용 상황\n\n\n\n\nLinear Regression\n없음\n✗\n제한 없음\n취약\n기준 모델\n\n\nRidge (L2)\n제곱합\n✗\n축소\n완화\n상관 높은 변수\n\n\nLasso (L1)\n절댓값\n✓\n0 포함\n보통\n변수 선택 필요\n\n\nElasticNet (L1+L2)\n혼합\n✓\n0 포함\n완화\n고차원, 복합 상황\n\n\n\n예제: 모든 모델 성능 비교\n\n# 모든 모델 평가\nmodels = {\n    'Linear Regression': lr,\n    'Ridge': ridge,\n    'Lasso': lasso,\n    'ElasticNet': elastic\n}\n\nresults = []\nfor name, model in models.items():\n    if name == 'Linear Regression':\n        pred = model.predict(X_test)\n    else:\n        pred = model.predict(X_test_scaled)\n    \n    results.append({\n        'Model': name,\n        'R²': r2_score(y_test, pred),\n        'RMSE': np.sqrt(mean_squared_error(y_test, pred)),\n        'MAE': mean_absolute_error(y_test, pred),\n        'Non-zero Coef': np.sum(model.coef_ != 0)\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"=== 모델 성능 비교 ===\")\nprint(results_df.round(4))\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# R² 비교\naxes[0].bar(results_df['Model'], results_df['R²'])\naxes[0].set_ylabel('R² Score')\naxes[0].set_title('Model Comparison: R²')\naxes[0].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n\n# RMSE 비교\naxes[1].bar(results_df['Model'], results_df['RMSE'])\naxes[1].set_ylabel('RMSE')\naxes[1].set_title('Model Comparison: RMSE')\naxes[1].set_xticklabels(results_df['Model'], rotation=45, ha='right')\n\nplt.tight_layout()\nplt.show()\n\n=== 모델 성능 비교 ===\n               Model      R²      RMSE       MAE  Non-zero Coef\n0  Linear Regression  0.7981  356.6518  289.6890              3\n1              Ridge  0.7977  356.9827  290.2179              3\n2              Lasso  0.7980  356.6800  289.7059              3\n3         ElasticNet  0.7891  364.4700  297.0190              3",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#모델-선택-가이드",
    "href": "part3/04. 회귀 모델.html#모델-선택-가이드",
    "title": "22  회귀 모델",
    "section": "22.9 모델 선택 가이드",
    "text": "22.9 모델 선택 가이드\n상황별 모델 선택\n\n\n\n상황\n권장 모델\n이유\n\n\n\n\n변수 수 적고 해석 중요\nLinear Regression\n단순하고 명확\n\n\n변수 간 상관 높음\nRidge\n다중공선성 완화\n\n\n변수 선택 필요\nLasso\n자동 변수 선택\n\n\n고차원 데이터 (p &gt; n)\nElasticNet\n안정적 변수 선택\n\n\n예측만 중요\n교차 검증으로 선택\n성능 기준\n\n\n\n의사결정 흐름\n변수 선택이 필요한가?\n├─ Yes → 변수가 매우 많은가?\n│         ├─ Yes → ElasticNet\n│         └─ No → Lasso\n└─ No → 다중공선성이 문제인가?\n          ├─ Yes → Ridge\n          └─ No → Linear Regression",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#하이퍼파라미터-튜닝",
    "href": "part3/04. 회귀 모델.html#하이퍼파라미터-튜닝",
    "title": "22  회귀 모델",
    "section": "22.10 하이퍼파라미터 튜닝",
    "text": "22.10 하이퍼파라미터 튜닝\n예제: GridSearchCV로 최적 Alpha 찾기\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# 파이프라인 구성 (스케일링 + 모델)\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', Ridge())\n])\n\n# 하이퍼파라미터 그리드\nparam_grid = {\n    'model__alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n}\n\n# GridSearchCV\ngrid_search = GridSearchCV(\n    pipeline, param_grid, cv=5, scoring='r2', n_jobs=-1\n)\ngrid_search.fit(X_train, y_train)\n\nprint(\"=== GridSearchCV 결과 ===\")\nprint(f\"최적 alpha: {grid_search.best_params_['model__alpha']}\")\nprint(f\"최적 R² (CV): {grid_search.best_score_:.4f}\")\n\n# 테스트 성능\nbest_model = grid_search.best_estimator_\ny_pred_best = best_model.predict(X_test)\nprint(f\"테스트 R²: {r2_score(y_test, y_pred_best):.4f}\")\n\n# Alpha에 따른 성능 변화\nresults_cv = pd.DataFrame(grid_search.cv_results_)\nplt.figure(figsize=(10, 6))\nplt.semilogx(results_cv['param_model__alpha'], results_cv['mean_test_score'], \n             marker='o', linewidth=2)\nplt.xlabel('Alpha (log scale)')\nplt.ylabel('Mean R² (5-fold CV)')\nplt.title('Ridge: Alpha Tuning')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n=== GridSearchCV 결과 ===\n최적 alpha: 1.0\n최적 R² (CV): 0.7450\n테스트 R²: 0.7977",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#전처리-주의사항",
    "href": "part3/04. 회귀 모델.html#전처리-주의사항",
    "title": "22  회귀 모델",
    "section": "22.11 전처리 주의사항",
    "text": "22.11 전처리 주의사항\n정규화 회귀 사용 시 필수 사항\n\n✓ 반드시 표준화: Ridge, Lasso, ElasticNet은 스케일에 민감\n✓ 파이프라인 사용: 전처리와 모델을 하나로 묶기\n✓ 테스트셋 분리: 전처리는 학습셋 기준으로\n✓ 교차 검증: 하이퍼파라미터 튜닝 시 필수\n\n예제: 올바른 파이프라인\n\n# 최종 파이프라인\nfinal_pipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', Ridge(alpha=1.0))\n])\n\n# 학습\nfinal_pipeline.fit(X_train, y_train)\n\n# 평가\nprint(\"=== 파이프라인 최종 성능 ===\")\nprint(f\"학습 R²: {final_pipeline.score(X_train, y_train):.4f}\")\nprint(f\"테스트 R²: {final_pipeline.score(X_test, y_test):.4f}\")\n\n=== 파이프라인 최종 성능 ===\n학습 R²: 0.7551\n테스트 R²: 0.7977",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/04. 회귀 모델.html#요약",
    "href": "part3/04. 회귀 모델.html#요약",
    "title": "22  회귀 모델",
    "section": "22.12 요약",
    "text": "22.12 요약\n이 장에서는 연속형 예측을 위한 회귀 모델을 학습했다. 주요 내용은 다음과 같다.\n회귀 모델 핵심\n\n선형 회귀: 기준 모델, 규제 없음\nRidge: L2 정규화, 다중공선성 완화, 모든 변수 유지\nLasso: L1 정규화, 자동 변수 선택, 희소 모델\nElasticNet: L1+L2, 균형 잡힌 접근, 고차원 데이터\n\n실무 체크리스트\n\n데이터 표준화 수행 (정규화 회귀 필수)\n적절한 모델 선택 (목적에 맞게)\n하이퍼파라미터 튜닝 (GridSearchCV)\n잔차 분석으로 가정 확인\n교차 검증으로 성능 평가\n파이프라인으로 구성\n\n주의사항\n\n정규화 회귀는 반드시 표준화 후 사용\n계수 해석 시 원본 스케일 고려\n과적합 방지가 최우선 목표\n모델 선택보다 전처리가 더 중요할 수 있음\n\n회귀 모델은 연속형 예측의 기본이며, 정규화는 과적합 방지의 핵심이다. 데이터 특성과 목적에 맞는 모델을 선택하고, 적절한 전처리와 하이퍼파라미터 튜닝을 통해 최적의 성능을 달성하는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>회귀 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html",
    "href": "part3/05. 분류 모델.html",
    "title": "23  분류 모델",
    "section": "",
    "text": "23.1 분류의 개념\n분류 모델(Classification Models)은 입력 변수(X)를 기반으로 범주형 타겟 변수(y)를 예측하는 지도 학습 모델이다. 회귀와 달리 출력이 이산적인 클래스이며, 새로운 데이터가 어느 범주에 속하는지 판단한다. 이 장에서는 로지스틱 회귀, k-NN, 결정 트리, 랜덤 포레스트 등 주요 분류 모델의 원리와 실무 활용법을 학습한다.\n예제: 데이터 로드\n분류는 입력 데이터를 사전에 정의된 범주 중 하나로 할당하는 문제이다.\n회귀 vs 분류 비교\n분류 문제의 유형\n분류 문제 예시",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#분류의-개념",
    "href": "part3/05. 분류 모델.html#분류의-개념",
    "title": "23  분류 모델",
    "section": "",
    "text": "구분\n회귀 (Regression)\n분류 (Classification)\n\n\n\n\n출력 타입\n연속형 (수치)\n범주형 (클래스)\n\n\n예측값\n실수 값\n클래스 라벨 또는 확률\n\n\n예시\n체중, 가격, 온도\n종, 합격/불합격, 질병 유무\n\n\n평가 지표\nMSE, MAE, R²\n정확도, 정밀도, 재현율, F1\n\n\n대표 모델\n선형 회귀, Ridge, Lasso\n로지스틱 회귀, SVM, 결정 트리\n\n\n\n\n\n\n\n유형\n설명\n예시\n\n\n\n\n이진 분류\n2개 클래스\n합격/불합격, 스팸/정상\n\n\n다중 클래스 분류\n3개 이상 클래스\n펭귄 종(3개), 손글씨 숫자(10개)\n\n\n다중 레이블 분류\n여러 클래스 동시 예측\n영화 장르(액션+코미디)\n\n\n\n\n\n\n\n분야\n입력 변수\n출력 변수 (클래스)\n\n\n\n\n의료\n증상, 나이, 검사 결과\n질병 유무\n\n\n금융\n신용 점수, 소득, 채무\n대출 승인/거부\n\n\n마케팅\n구매 이력, 방문 빈도\n이탈 여부\n\n\n생물학\n부리 크기, 날개 길이\n종(species)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#데이터-준비",
    "href": "part3/05. 분류 모델.html#데이터-준비",
    "title": "23  분류 모델",
    "section": "23.2 데이터 준비",
    "text": "23.2 데이터 준비\n예제: 학습/테스트 분할\n\nfrom sklearn.pipeline import Pipeline\n\n# 데이터 분할 (stratify로 클래스 비율 유지)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y  # 클래스 비율 유지 (분류에서 매우 중요)\n)\n\nprint(\"=== 데이터 분할 ===\")\nprint(f\"학습 데이터: {X_train.shape}\")\nprint(f\"테스트 데이터: {X_test.shape}\")\n\n# 클래스 비율 확인\nprint(\"\\n=== 클래스 비율 비교 ===\")\nprint(\"전체 데이터:\")\nprint((y.value_counts() / len(y) * 100).round(1))\nprint(\"\\n학습 데이터:\")\nprint((y_train.value_counts() / len(y_train) * 100).round(1))\nprint(\"\\n테스트 데이터:\")\nprint((y_test.value_counts() / len(y_test) * 100).round(1))\n\n=== 데이터 분할 ===\n학습 데이터: (266, 4)\n테스트 데이터: (67, 4)\n\n=== 클래스 비율 비교 ===\n전체 데이터:\nspecies\nAdelie       43.8\nGentoo       35.7\nChinstrap    20.4\nName: count, dtype: float64\n\n학습 데이터:\nspecies\nAdelie       44.0\nGentoo       35.7\nChinstrap    20.3\nName: count, dtype: float64\n\n테스트 데이터:\nspecies\nAdelie       43.3\nGentoo       35.8\nChinstrap    20.9\nName: count, dtype: float64\n\n\nStratify의 중요성\n\n클래스 불균형 데이터에서 필수\n학습/테스트셋의 클래스 비율 유지\n편향된 평가 방지",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#선형-분류-모델-로지스틱-회귀",
    "href": "part3/05. 분류 모델.html#선형-분류-모델-로지스틱-회귀",
    "title": "23  분류 모델",
    "section": "23.3 선형 분류 모델: 로지스틱 회귀",
    "text": "23.3 선형 분류 모델: 로지스틱 회귀\n로지스틱 회귀(Logistic Regression)는 이름은 회귀지만 분류 모델로, 선형 결정 경계를 사용한다.\n로지스틱 회귀의 핵심\n\n선형 결합 → 시그모이드 함수 → 확률\n출력을 0~1 사이 확률로 변환\n확률이 0.5 이상이면 양성 클래스\n\n로지스틱 회귀 특징\n\n\n\n특징\n설명\n\n\n\n\n선형 결정 경계\n직선/평면으로 클래스 구분\n\n\n확률 출력\n각 클래스에 속할 확률 제공\n\n\n해석 가능\n계수로 변수 영향도 파악\n\n\n고차원 적합\n변수 많아도 안정적\n\n\n다중 클래스 지원\nOne-vs-Rest 또는 Multinomial\n\n\n스케일 민감\n표준화 필요\n\n\n\n\n23.3.1 로지스틱 회귀 실습\n예제: 로지스틱 회귀 학습\n\nfrom sklearn.linear_model import LogisticRegression\n\n# 파이프라인 구성 (스케일링 + 모델)\npipe_lr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# 학습\npipe_lr.fit(X_train, y_train)\n\n# 예측\ny_pred_lr = pipe_lr.predict(X_test)\ny_proba_lr = pipe_lr.predict_proba(X_test)\n\n# 평가\naccuracy = accuracy_score(y_test, y_pred_lr)\nprint(\"=== 로지스틱 회귀 성능 ===\")\nprint(f\"정확도: {accuracy:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_lr))\n\n=== 로지스틱 회귀 성능 ===\n정확도: 1.0000\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        29\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\n예제: 혼동 행렬 시각화\n\nimport seaborn as sns\n\n# 혼동 행렬\ncm = confusion_matrix(y_test, y_pred_lr)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=pipe_lr.classes_,\n            yticklabels=pipe_lr.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Logistic Regression: Confusion Matrix')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 확률 출력 확인\n\n# 확률 출력 (처음 5개 샘플)\nprint(\"\\n=== 예측 확률 (처음 5개) ===\")\nproba_df = pd.DataFrame(\n    y_proba_lr[:5],\n    columns=pipe_lr.classes_\n)\nproba_df['Predicted'] = y_pred_lr[:5]\nproba_df['Actual'] = y_test.iloc[:5].values\nprint(proba_df.round(3))\n\n\n=== 예측 확률 (처음 5개) ===\n   Adelie  Chinstrap  Gentoo  Predicted     Actual\n0   0.000      0.021   0.979     Gentoo     Gentoo\n1   0.092      0.875   0.033  Chinstrap  Chinstrap\n2   0.991      0.006   0.003     Adelie     Adelie\n3   0.000      0.006   0.994     Gentoo     Gentoo\n4   0.001      0.014   0.984     Gentoo     Gentoo",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#거리-기반-모델-k-최근접-이웃-k-nn",
    "href": "part3/05. 분류 모델.html#거리-기반-모델-k-최근접-이웃-k-nn",
    "title": "23  분류 모델",
    "section": "23.4 거리 기반 모델: k-최근접 이웃 (k-NN)",
    "text": "23.4 거리 기반 모델: k-최근접 이웃 (k-NN)\nk-NN은 새로운 데이터 주변의 k개 이웃을 기준으로 다수결 투표로 분류하는 가장 직관적인 모델이다.\nk-NN의 특징\n\n\n\n특징\n설명\n\n\n\n\n학습 없음\n인스턴스 기반 학습 (lazy learning)\n\n\n거리 기반\n유클리드, 맨하탄 등 거리 사용\n\n\n비선형 경계\n복잡한 경계 표현 가능\n\n\n스케일 민감\n반드시 표준화 필요\n\n\n예측 느림\n모든 학습 데이터와 거리 계산\n\n\n해석 어려움\n이웃 기반이라 직관적이지만 설명 곤란\n\n\n\nk 값의 영향\n\n\n\nk 값\n효과\n문제점\n\n\n\n\nk = 1\n가장 가까운 1개만\n과적합, 노이즈 민감\n\n\nk 작음 (3~5)\n세밀한 경계\n과적합 경향\n\n\nk 적절 (5~15)\n균형 잡힌 성능\n최적값 찾기\n\n\nk 큼\n부드러운 경계\n과소적합\n\n\n\n\n23.4.1 k-NN 실습\n예제: k-NN 학습\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# 파이프라인 구성\npipe_knn = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', KNeighborsClassifier(n_neighbors=5))\n])\n\n# 학습 및 예측\npipe_knn.fit(X_train, y_train)\ny_pred_knn = pipe_knn.predict(X_test)\n\n# 평가\naccuracy_knn = accuracy_score(y_test, y_pred_knn)\nprint(\"=== k-NN 성능 (k=5) ===\")\nprint(f\"정확도: {accuracy_knn:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_knn))\n\n=== k-NN 성능 (k=5) ===\n정확도: 1.0000\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        29\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\n예제: k 값 튜닝\n\n# 다양한 k 값으로 실험\nk_values = range(1, 21)\naccuracies = []\n\nfor k in k_values:\n    knn_temp = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', KNeighborsClassifier(n_neighbors=k))\n    ])\n    knn_temp.fit(X_train, y_train)\n    acc = knn_temp.score(X_test, y_test)\n    accuracies.append(acc)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, accuracies, marker='o', linewidth=2)\nplt.xlabel('k (Number of Neighbors)')\nplt.ylabel('Accuracy')\nplt.title('k-NN: Optimal k Value')\nplt.grid(True, alpha=0.3)\nplt.axvline(x=k_values[np.argmax(accuracies)], color='r', linestyle='--',\n            label=f'Best k = {k_values[np.argmax(accuracies)]}')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\nprint(f\"최적 k: {k_values[np.argmax(accuracies)]}\")\nprint(f\"최고 정확도: {max(accuracies):.4f}\")\n\n\n\n\n\n\n\n\n최적 k: 1\n최고 정확도: 1.0000",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#트리-기반-모델-결정-트리",
    "href": "part3/05. 분류 모델.html#트리-기반-모델-결정-트리",
    "title": "23  분류 모델",
    "section": "23.5 트리 기반 모델: 결정 트리",
    "text": "23.5 트리 기반 모델: 결정 트리\n결정 트리(Decision Tree)는 질문을 반복하며 데이터를 분기하여 분류하는 사람이 이해하기 쉬운 모델이다.\n결정 트리의 특징\n\n\n\n특징\n설명\n\n\n\n\n직관적 구조\nif-else 규칙으로 해석 가능\n\n\n비선형 경계\n복잡한 패턴 표현 가능\n\n\n스케일 불필요\n표준화 필요 없음\n\n\n범주형 처리\n범주형 변수 직접 사용 가능\n\n\n변수 중요도\n각 변수의 기여도 제공\n\n\n과적합 취약\n깊이 제한 필요\n\n\n\n주요 하이퍼파라미터\n\n\n\n파라미터\n설명\n권장값\n\n\n\n\nmax_depth\n트리 최대 깊이\n3~10\n\n\nmin_samples_split\n분할 위한 최소 샘플 수\n2~20\n\n\nmin_samples_leaf\n리프 노드 최소 샘플 수\n1~10\n\n\nmax_features\n분할 시 고려할 최대 변수 수\n‘sqrt’, ‘log2’\n\n\n\n\n23.5.1 결정 트리 실습\n예제: 결정 트리 학습\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# 결정 트리 (스케일링 불필요)\ntree = DecisionTreeClassifier(\n    max_depth=4,\n    random_state=42\n)\n\ntree.fit(X_train, y_train)\ny_pred_tree = tree.predict(X_test)\n\n# 평가\naccuracy_tree = accuracy_score(y_test, y_pred_tree)\nprint(\"=== 결정 트리 성능 ===\")\nprint(f\"정확도: {accuracy_tree:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_tree))\n\n=== 결정 트리 성능 ===\n정확도: 0.9403\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       0.96      0.93      0.95        29\n   Chinstrap       0.82      1.00      0.90        14\n      Gentoo       1.00      0.92      0.96        24\n\n    accuracy                           0.94        67\n   macro avg       0.93      0.95      0.94        67\nweighted avg       0.95      0.94      0.94        67\n\n\n\n예제: 트리 시각화\n\nfrom sklearn.tree import plot_tree\n\n# 트리 구조 시각화\nplt.figure(figsize=(20, 10))\nplot_tree(tree, \n          feature_names=X.columns,\n          class_names=tree.classes_,\n          filled=True,\n          rounded=True,\n          fontsize=10)\nplt.title(\"Decision Tree Structure\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 변수 중요도\n\n# 변수 중요도\nimportance_df = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': tree.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\n=== 변수 중요도 ===\")\nprint(importance_df)\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance')\nplt.title('Decision Tree: Feature Importance')\nplt.tight_layout()\nplt.show()\n\n\n=== 변수 중요도 ===\n             Feature  Importance\n2  flipper_length_mm    0.527022\n0     bill_length_mm    0.369690\n1      bill_depth_mm    0.092077\n3        body_mass_g    0.011211\n\n\n\n\n\n\n\n\n\n결정 트리의 한계\n\n과적합에 매우 취약\n데이터 작은 변화에 민감 (불안정)\n선형 관계 표현에 비효율적\n\n이를 보완하기 위해 앙상블 모델을 사용한다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#앙상블-모델-랜덤-포레스트",
    "href": "part3/05. 분류 모델.html#앙상블-모델-랜덤-포레스트",
    "title": "23  분류 모델",
    "section": "23.6 앙상블 모델: 랜덤 포레스트",
    "text": "23.6 앙상블 모델: 랜덤 포레스트\n랜덤 포레스트(Random Forest)는 여러 결정 트리를 결합한 배깅(Bagging) 기반 앙상블 모델이다.\n랜덤 포레스트의 원리\n\n부트스트랩 샘플링으로 여러 데이터셋 생성\n각 데이터셋마다 결정 트리 학습\n무작위로 변수 부분집합 선택\n모든 트리의 투표로 최종 예측\n\n랜덤 포레스트의 특징\n\n\n\n특징\n설명\n\n\n\n\n과적합 감소\n여러 트리 평균으로 안정화\n\n\n높은 성능\n실무에서 우수한 성능\n\n\n변수 중요도\n평균 중요도 제공\n\n\n병렬 처리\n트리 독립적으로 학습 가능\n\n\n해석 어려움\n개별 트리보다 복잡\n\n\n하이퍼파라미터\n튜닝 필요\n\n\n\n주요 하이퍼파라미터\n\n\n\n파라미터\n설명\n권장값\n\n\n\n\nn_estimators\n트리 개수\n100~500\n\n\nmax_depth\n각 트리 최대 깊이\nNone 또는 10~30\n\n\nmax_features\n분할 시 고려 변수 수\n‘sqrt’, ‘log2’\n\n\nmin_samples_split\n분할 최소 샘플\n2~10\n\n\n\n\n23.6.1 랜덤 포레스트 실습\n예제: 랜덤 포레스트 학습\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 랜덤 포레스트\nrf = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=10,\n    random_state=42,\n    n_jobs=-1  # 병렬 처리\n)\n\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# 평가\naccuracy_rf = accuracy_score(y_test, y_pred_rf)\nprint(\"=== 랜덤 포레스트 성능 ===\")\nprint(f\"정확도: {accuracy_rf:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_rf))\n\n=== 랜덤 포레스트 성능 ===\n정확도: 0.9701\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      0.97      0.98        29\n   Chinstrap       0.88      1.00      0.93        14\n      Gentoo       1.00      0.96      0.98        24\n\n    accuracy                           0.97        67\n   macro avg       0.96      0.97      0.96        67\nweighted avg       0.97      0.97      0.97        67\n\n\n\n예제: 변수 중요도\n\n# 변수 중요도\nrf_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\n=== 랜덤 포레스트 변수 중요도 ===\")\nprint(rf_importance)\n\n# 시각화\nplt.figure(figsize=(10, 5))\nplt.barh(rf_importance['Feature'], rf_importance['Importance'])\nplt.xlabel('Importance')\nplt.title('Random Forest: Feature Importance')\nplt.tight_layout()\nplt.show()\n\n\n=== 랜덤 포레스트 변수 중요도 ===\n             Feature  Importance\n0     bill_length_mm    0.415594\n2  flipper_length_mm    0.330553\n1      bill_depth_mm    0.168720\n3        body_mass_g    0.085133\n\n\n\n\n\n\n\n\n\n예제: 트리 개수에 따른 성능\n\n# n_estimators 영향 확인\nn_trees = [10, 50, 100, 200, 500]\naccuracies_rf = []\n\nfor n in n_trees:\n    rf_temp = RandomForestClassifier(n_estimators=n, random_state=42, n_jobs=-1)\n    rf_temp.fit(X_train, y_train)\n    acc = rf_temp.score(X_test, y_test)\n    accuracies_rf.append(acc)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(n_trees, accuracies_rf, marker='o', linewidth=2)\nplt.xlabel('Number of Trees')\nplt.ylabel('Accuracy')\nplt.title('Random Forest: Effect of n_estimators')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#모델-종합-비교",
    "href": "part3/05. 분류 모델.html#모델-종합-비교",
    "title": "23  분류 모델",
    "section": "23.7 모델 종합 비교",
    "text": "23.7 모델 종합 비교\n분류 모델 특성 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\n모델\n선형성\n스케일 민감\n해석력\n학습 속도\n예측 속도\n과적합\n적용 상황\n\n\n\n\n로지스틱 회귀\n선형\n높음\n높음\n빠름\n빠름\n낮음\n기준 모델, 해석 중요\n\n\nk-NN\n비선형\n매우 높음\n중간\n빠름\n느림\n중간\n소규모 데이터\n\n\n결정 트리\n비선형\n낮음\n높음\n빠름\n빠름\n높음\n탐색, 설명\n\n\n랜덤 포레스트\n비선형\n낮음\n중간\n느림\n중간\n낮음\n실무, 성능 우선\n\n\n\n예제: 모든 모델 성능 비교\n\n# 모든 모델 성능 비교\nmodels = {\n    'Logistic Regression': pipe_lr,\n    'k-NN (k=5)': pipe_knn,\n    'Decision Tree': tree,\n    'Random Forest': rf\n}\n\nresults = []\nfor name, model in models.items():\n    if name == 'Logistic Regression' or name == 'k-NN (k=5)':\n        pred = model.predict(X_test)\n    else:\n        pred = model.predict(X_test)\n    \n    acc = accuracy_score(y_test, pred)\n    results.append({'Model': name, 'Accuracy': acc})\n\nresults_df = pd.DataFrame(results).sort_values('Accuracy', ascending=False)\nprint(\"=== 모델 성능 비교 ===\")\nprint(results_df)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.barh(results_df['Model'], results_df['Accuracy'])\nplt.xlabel('Accuracy')\nplt.title('Classification Models Comparison')\nplt.xlim([0.8, 1.0])\nplt.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()\n\n=== 모델 성능 비교 ===\n                 Model  Accuracy\n0  Logistic Regression  1.000000\n1           k-NN (k=5)  1.000000\n3        Random Forest  0.970149\n2        Decision Tree  0.940299",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#모델-선택-가이드",
    "href": "part3/05. 분류 모델.html#모델-선택-가이드",
    "title": "23  분류 모델",
    "section": "23.8 모델 선택 가이드",
    "text": "23.8 모델 선택 가이드\n상황별 모델 선택\n\n\n\n\n\n\n\n\n상황\n권장 모델\n이유\n\n\n\n\n빠른 기준선 필요\n로지스틱 회귀\n학습 빠르고 안정적\n\n\n해석 중요\n로지스틱 회귀, 결정 트리\n계수/규칙으로 설명 가능\n\n\n비선형 관계\n결정 트리, 랜덤 포레스트\n복잡한 경계 표현\n\n\n성능 최우선\n랜덤 포레스트, Gradient Boosting\n앙상블로 높은 정확도\n\n\n데이터 적음\nk-NN, 로지스틱 회귀\n과적합 위험 낮음\n\n\n고차원 데이터\n로지스틱 회귀 (L1/L2)\n차원 저주 완화\n\n\n\n의사결정 흐름\n해석이 필수인가?\n├─ Yes → 선형 관계인가?\n│         ├─ Yes → 로지스틱 회귀\n│         └─ No → 결정 트리\n└─ No → 성능이 최우선인가?\n          ├─ Yes → 랜덤 포레스트\n          └─ No → 데이터 크기는?\n                   ├─ 작음 → k-NN\n                   └─ 큼 → 로지스틱 회귀",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#실무-체크리스트",
    "href": "part3/05. 분류 모델.html#실무-체크리스트",
    "title": "23  분류 모델",
    "section": "23.9 실무 체크리스트",
    "text": "23.9 실무 체크리스트\n분류 모델 적용 시 확인사항\n\n클래스 불균형 확인 (stratify 사용)\n적절한 평가 지표 선택 (정확도만으로 불충분)\n스케일링 필요 여부 확인 (모델별 다름)\n교차 검증으로 성능 평가\n하이퍼파라미터 튜닝\n혼동 행렬로 오류 패턴 분석\n변수 중요도 확인 (가능한 경우)\n과적합 여부 확인 (학습 vs 테스트 성능)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/05. 분류 모델.html#요약",
    "href": "part3/05. 분류 모델.html#요약",
    "title": "23  분류 모델",
    "section": "23.10 요약",
    "text": "23.10 요약\n이 장에서는 범주형 예측을 위한 주요 분류 모델을 학습했다. 주요 내용은 다음과 같다.\n분류 모델 핵심\n\n로지스틱 회귀: 선형, 해석 가능, 기준 모델\nk-NN: 거리 기반, 직관적, 스케일 민감\n결정 트리: 비선형, 해석 용이, 과적합 주의\n랜덤 포레스트: 앙상블, 안정적, 실무 우수\n\n실무 권장사항\n\n로지스틱 회귀로 시작: 빠른 기준선 확보\n랜덤 포레스트로 개선: 성능 향상\n하이퍼파라미터 튜닝: GridSearchCV 활용\n앙상블 결합: 여러 모델 결합 고려\n\n분류 모델은 실무에서 가장 흔한 머신러닝 문제이다. 데이터 특성과 목적에 맞는 모델을 선택하고, 적절한 전처리와 평가를 통해 최적의 성능을 달성하는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>분류 모델</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html",
    "href": "part3/06. 군집 분석.html",
    "title": "24  군집 분석",
    "section": "",
    "text": "24.1 군집 분석의 개념\n군집 분석(Clustering)은 라벨이 없는 데이터에서 유사한 데이터끼리 자동으로 묶는 비지도 학습 기법이다. 정답이 주어지지 않은 상태에서 데이터의 내재된 구조를 발견하고 패턴을 파악하는 것이 목적이다. 군집 분석은 데이터 탐색, 세분화, 이상치 탐지, 전처리 등 다양한 분야에서 활용된다. 이 장에서는 K-Means, DBSCAN, GMM 등 주요 군집 알고리즘의 원리와 실무 활용법을 학습한다.\n예제: 데이터 로드\n군집 분석은 비지도 학습으로, 타겟 변수 없이 데이터의 구조를 발견한다.\n군집 분석 vs 분류\n군집 분석의 목적\n군집 분석의 가정\n알고리즘마다 데이터에 대한 가정이 다르다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#군집-분석의-개념",
    "href": "part3/06. 군집 분석.html#군집-분석의-개념",
    "title": "24  군집 분석",
    "section": "",
    "text": "구분\n군집 분석 (Clustering)\n분류 (Classification)\n\n\n\n\n학습 유형\n비지도 학습\n지도 학습\n\n\n타겟\n없음\n있음 (라벨)\n\n\n목적\n구조 발견\n예측\n\n\n평가\n내재적 지표, 시각화\n정확도, F1 등\n\n\n예시\n고객 세분화, 패턴 발견\n종 분류, 질병 진단\n\n\n\n\n\n\n\n목적\n설명\n예시\n\n\n\n\n데이터 탐색\n구조와 패턴 파악\n신규 데이터 이해\n\n\n세분화\n유사 그룹 생성\n고객 세그먼트\n\n\n이상치 탐지\n군집에 속하지 않는 점 발견\n이상 거래 탐지\n\n\n전처리\n군집을 새로운 특성으로 사용\n군집 ID를 범주형 변수로\n\n\n차원 축소 보조\n시각화 전 그룹 파악\nPCA + 군집\n\n\n\n\n\n\n\n\n알고리즘\n가정\n\n\n\n\nK-Means\n구형(spherical) 군집, 유사한 크기\n\n\nDBSCAN\n밀도 기반 군집, 밀도 차이 적음\n\n\nGMM\n가우시안 분포 혼합\n\n\n계층적 군집\n트리 구조 가능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#데이터-준비",
    "href": "part3/06. 군집 분석.html#데이터-준비",
    "title": "24  군집 분석",
    "section": "24.2 데이터 준비",
    "text": "24.2 데이터 준비\n예제: 데이터 표준화\n\n# 표준화 (군집은 거리 기반이므로 필수)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_cluster)\n\nprint(\"=== 표준화 전후 비교 ===\")\nprint(\"원본 데이터 범위:\")\nprint(X_cluster.describe().loc[['min', 'max']])\nprint(\"\\n표준화 후 데이터 범위:\")\nprint(pd.DataFrame(X_scaled, columns=X_cluster.columns).describe().loc[['min', 'max']])\n\n=== 표준화 전후 비교 ===\n원본 데이터 범위:\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nmin            32.1           13.1              172.0       2700.0\nmax            59.6           21.5              231.0       6300.0\n\n표준화 후 데이터 범위:\n     bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nmin       -2.177987      -2.067291          -2.069852    -1.874435\nmax        2.858227       2.204743           2.146028     2.603144\n\n\n표준화의 중요성\n\n거리 기반 알고리즘에서 필수\n스케일이 큰 변수가 군집을 지배하는 것 방지\nK-Means, DBSCAN에서 특히 중요",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#분할-기반-군집-k-means",
    "href": "part3/06. 군집 분석.html#분할-기반-군집-k-means",
    "title": "24  군집 분석",
    "section": "24.3 분할 기반 군집: K-Means",
    "text": "24.3 분할 기반 군집: K-Means\nK-Means는 가장 널리 사용되는 군집 알고리즘으로, 데이터를 K개의 군집으로 분할한다.\nK-Means 알고리즘 작동 원리\n\nK개의 초기 중심점(centroid) 무작위 선택\n각 데이터를 가장 가까운 중심점에 할당\n각 군집의 중심점을 새로 계산 (평균)\n중심점 변화가 없을 때까지 2-3 반복\n\nK-Means 특징\n\n\n\n특징\n설명\n\n\n\n\n군집 수\n사전에 K 지정 필요\n\n\n군집 형태\n구형(spherical) 가정\n\n\n계산 속도\n빠름 (대용량 적합)\n\n\n이상치 민감\n평균 사용으로 민감\n\n\n결정론적\n초기값에 따라 결과 다름\n\n\n하드 할당\n각 데이터는 하나의 군집만\n\n\n\n\n24.3.1 K-Means 실습\n예제: K-Means 군집 분석\n\nfrom sklearn.cluster import KMeans\n\n# K-Means 군집 (K=3, 실제 종 개수와 동일)\nkmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\n\nprint(\"=== K-Means 결과 ===\")\nprint(f\"군집 개수: {kmeans.n_clusters}\")\nprint(f\"반복 횟수: {kmeans.n_iter_}\")\nprint(\"\\n군집별 샘플 수:\")\nprint(pd.Series(labels_kmeans).value_counts().sort_index())\n\n# 실제 종과 비교\ncomparison_df = pd.DataFrame({\n    'True Species': y_true,\n    'K-Means Cluster': labels_kmeans\n})\nprint(\"\\n=== 실제 종 vs 군집 ===\")\nprint(pd.crosstab(comparison_df['True Species'], comparison_df['K-Means Cluster']))\n\n=== K-Means 결과 ===\n군집 개수: 3\n반복 횟수: 8\n\n군집별 샘플 수:\n0    129\n1    119\n2     85\nName: count, dtype: int64\n\n=== 실제 종 vs 군집 ===\nK-Means Cluster    0    1   2\nTrue Species                 \nAdelie           124    0  22\nChinstrap          5    0  63\nGentoo             0  119   0\n\n\n예제: 중심점 확인\n\n# 군집 중심점 (표준화된 공간)\ncentroids = kmeans.cluster_centers_\n\nprint(\"\\n=== 군집 중심점 ===\")\ncentroids_df = pd.DataFrame(\n    centroids,\n    columns=X_cluster.columns,\n    index=[f'Cluster {i}' for i in range(3)]\n)\nprint(centroids_df.round(2))\n\n# 원래 스케일로 역변환\ncentroids_original = scaler.inverse_transform(centroids)\ncentroids_original_df = pd.DataFrame(\n    centroids_original,\n    columns=X_cluster.columns,\n    index=[f'Cluster {i}' for i in range(3)]\n)\nprint(\"\\n=== 군집 중심점 (원본 스케일) ===\")\nprint(centroids_original_df.round(2))\n\n\n=== 군집 중심점 ===\n           bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nCluster 0           -1.05           0.49              -0.88        -0.76\nCluster 1            0.65          -1.10               1.16         1.10\nCluster 2            0.67           0.81              -0.29        -0.38\n\n=== 군집 중심점 (원본 스케일) ===\n           bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\nCluster 0           38.28          18.12             188.63      3593.80\nCluster 1           47.57          15.00             217.24      5092.44\nCluster 2           47.66          18.75             196.92      3898.24\n\n\n\n\n24.3.2 최적 K 값 선택\n방법 1: Elbow Method\n\n# 다양한 K 값으로 실험\ninertias = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans_temp.fit(X_scaled)\n    inertias.append(kmeans_temp.inertia_)\n\n# Elbow Plot\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, inertias, marker='o', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Inertia (Within-cluster Sum of Squares)')\nplt.title('Elbow Method for Optimal K')\nplt.grid(True, alpha=0.3)\nplt.xticks(K_range)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n방법 2: Silhouette Score\n\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n\n# Silhouette Score 계산\nsilhouette_scores = []\n\nfor k in K_range:\n    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans_temp.fit_predict(X_scaled)\n    score = silhouette_score(X_scaled, labels)\n    silhouette_scores.append(score)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.plot(K_range, silhouette_scores, marker='o', linewidth=2, markersize=8)\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score for Optimal K')\nplt.grid(True, alpha=0.3)\nplt.axhline(y=np.max(silhouette_scores), color='r', linestyle='--',\n            label=f'Max at K={K_range[np.argmax(silhouette_scores)]}')\nplt.legend()\nplt.xticks(K_range)\nplt.tight_layout()\nplt.show()\n\nprint(f\"최적 K (Silhouette): {K_range[np.argmax(silhouette_scores)]}\")\nprint(f\"최대 Silhouette Score: {max(silhouette_scores):.4f}\")\n\n\n\n\n\n\n\n\n최적 K (Silhouette): 2\n최대 Silhouette Score: 0.5308\n\n\nSilhouette Score 해석\n\n\n\n점수 범위\n해석\n\n\n\n\n0.71 ~ 1.0\n강한 구조\n\n\n0.51 ~ 0.70\n합리적 구조\n\n\n0.26 ~ 0.50\n약한 구조\n\n\n&lt; 0.25\n구조 없음\n\n\n\n\n\n24.3.3 군집 시각화\n예제: PCA로 2D 시각화\n\n# PCA로 2차원으로 축소\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\n\n# K-Means 군집 결과 시각화\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# K-Means 군집\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans, \n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[0].scatter(pca.transform(centroids)[:, 0], pca.transform(centroids)[:, 1],\n                c='red', marker='X', s=200, edgecolors='black', linewidths=2,\n                label='Centroids')\naxes[0].set_title('K-Means Clustering (K=3)')\naxes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\naxes[0].legend()\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# 실제 종\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], \n                           c=y_true.astype('category').cat.codes,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[1].set_title('True Species')\naxes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\naxes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\nplt.colorbar(scatter2, ax=axes[1], label='Species')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#밀도-기반-군집-dbscan",
    "href": "part3/06. 군집 분석.html#밀도-기반-군집-dbscan",
    "title": "24  군집 분석",
    "section": "24.4 밀도 기반 군집: DBSCAN",
    "text": "24.4 밀도 기반 군집: DBSCAN\nDBSCAN(Density-Based Spatial Clustering of Applications with Noise)은 밀도가 높은 영역을 군집으로 간주하는 알고리즘이다.\nDBSCAN 핵심 개념\n\nCore Point: 반경 eps 내에 min_samples 이상의 이웃이 있는 점\nBorder Point: Core Point의 이웃이지만 자신은 Core가 아닌 점\nNoise Point: Core도 Border도 아닌 점 (라벨 -1)\n\nDBSCAN 특징\n\n\n\n특징\n설명\n\n\n\n\n군집 수\n자동 결정 (K 지정 불필요)\n\n\n군집 형태\n비구형, 복잡한 형태 가능\n\n\n이상치 처리\n노이즈 자동 탐지 (-1)\n\n\n밀도 가정\n밀도가 비슷해야 함\n\n\n파라미터 민감\neps, min_samples 튜닝 중요\n\n\n\n주요 하이퍼파라미터\n\n\n\n파라미터\n설명\n설정 가이드\n\n\n\n\neps\n이웃 거리 임계값\n데이터 분포에 따라 실험\n\n\nmin_samples\n최소 이웃 수\n일반적으로 차원 수 + 1 또는 5~10\n\n\n\n\n24.4.1 DBSCAN 실습\n예제: DBSCAN 군집 분석\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN 군집\ndbscan = DBSCAN(eps=0.8, min_samples=5)\nlabels_dbscan = dbscan.fit_predict(X_scaled)\n\nprint(\"=== DBSCAN 결과 ===\")\nprint(f\"군집 개수 (노이즈 제외): {len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)}\")\nprint(f\"노이즈 포인트 수: {(labels_dbscan == -1).sum()}\")\nprint(\"\\n군집별 샘플 수:\")\nprint(pd.Series(labels_dbscan).value_counts().sort_index())\n\n=== DBSCAN 결과 ===\n군집 개수 (노이즈 제외): 2\n노이즈 포인트 수: 5\n\n군집별 샘플 수:\n-1      5\n 0    211\n 1    117\nName: count, dtype: int64\n\n\n예제: eps 값에 따른 군집 변화\n\n# 다양한 eps 값으로 실험\neps_values = [0.4, 0.6, 0.8, 1.0, 1.2]\n\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.ravel()\n\nfor idx, eps in enumerate(eps_values):\n    dbscan_temp = DBSCAN(eps=eps, min_samples=5)\n    labels_temp = dbscan_temp.fit_predict(X_scaled)\n    \n    n_clusters = len(set(labels_temp)) - (1 if -1 in labels_temp else 0)\n    n_noise = (labels_temp == -1).sum()\n    \n    scatter = axes[idx].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_temp,\n                               cmap='viridis', alpha=0.6, edgecolors='k')\n    axes[idx].set_title(f'DBSCAN (eps={eps})\\nClusters: {n_clusters}, Noise: {n_noise}')\n    axes[idx].set_xlabel('PC1')\n    axes[idx].set_ylabel('PC2')\n\n# 마지막 subplot은 실제 종\nscatter = axes[5].scatter(X_pca[:, 0], X_pca[:, 1], \n                         c=y_true.astype('category').cat.codes,\n                         cmap='viridis', alpha=0.6, edgecolors='k')\naxes[5].set_title('True Species')\naxes[5].set_xlabel('PC1')\naxes[5].set_ylabel('PC2')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#모델-기반-군집-gaussian-mixture-model-gmm",
    "href": "part3/06. 군집 분석.html#모델-기반-군집-gaussian-mixture-model-gmm",
    "title": "24  군집 분석",
    "section": "24.5 모델 기반 군집: Gaussian Mixture Model (GMM)",
    "text": "24.5 모델 기반 군집: Gaussian Mixture Model (GMM)\nGMM은 데이터가 여러 개의 가우시안 분포 혼합으로 생성되었다고 가정하는 확률적 군집 방법이다.\nGMM 특징\n\n\n\n특징\n설명\n\n\n\n\n소프트 할당\n각 데이터가 군집에 속할 확률 제공\n\n\n타원형 군집\n공분산 고려로 다양한 형태\n\n\n분포 가정\n가우시안 분포 혼합\n\n\n군집 수\n사전 지정 필요\n\n\n확률 기반\n불확실성 정량화 가능\n\n\n\nK-Means vs GMM\n\n\n\n구분\nK-Means\nGMM\n\n\n\n\n할당 방식\n하드 (0 또는 1)\n소프트 (확률)\n\n\n군집 형태\n구형\n타원형\n\n\n기반\n거리\n확률\n\n\n불확실성\n없음\n확률로 표현\n\n\n\n\n24.5.1 GMM 실습\n예제: GMM 군집 분석\n\nfrom sklearn.mixture import GaussianMixture\n\n# GMM 군집\ngmm = GaussianMixture(n_components=3, random_state=42)\nlabels_gmm = gmm.fit_predict(X_scaled)\nprobs_gmm = gmm.predict_proba(X_scaled)\n\nprint(\"=== GMM 결과 ===\")\nprint(f\"군집 개수: {gmm.n_components}\")\nprint(\"\\n군집별 샘플 수:\")\nprint(pd.Series(labels_gmm).value_counts().sort_index())\n\n# 확률 출력 (처음 5개)\nprint(\"\\n=== 군집 소속 확률 (처음 5개) ===\")\nprobs_df = pd.DataFrame(\n    probs_gmm[:5],\n    columns=[f'Cluster {i}' for i in range(3)]\n)\nprobs_df['Assigned'] = labels_gmm[:5]\nprint(probs_df.round(3))\n\n=== GMM 결과 ===\n군집 개수: 3\n\n군집별 샘플 수:\n0    147\n1    119\n2     67\nName: count, dtype: int64\n\n=== 군집 소속 확률 (처음 5개) ===\n   Cluster 0  Cluster 1  Cluster 2  Assigned\n0      1.000        0.0      0.000         0\n1      0.999        0.0      0.001         0\n2      0.967        0.0      0.033         0\n3      1.000        0.0      0.000         0\n4      1.000        0.0      0.000         0\n\n\n예제: 불확실성 분석\n\n# 최대 확률 (확신도)\nmax_probs = probs_gmm.max(axis=1)\n\n# 불확실성이 높은 샘플 (확신도 낮음)\nuncertain_mask = max_probs &lt; 0.7\n\nprint(f\"\\n=== 불확실성 분석 ===\")\nprint(f\"확신도 &gt; 0.9: {(max_probs &gt; 0.9).sum()}개 ({(max_probs &gt; 0.9).sum()/len(max_probs)*100:.1f}%)\")\nprint(f\"확신도 &lt; 0.7: {uncertain_mask.sum()}개 ({uncertain_mask.sum()/len(max_probs)*100:.1f}%)\")\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# GMM 군집\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_gmm,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[0].set_title('GMM Clustering')\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\nplt.colorbar(scatter1, ax=axes[0], label='Cluster')\n\n# 확신도 시각화\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=max_probs,\n                           cmap='RdYlGn', alpha=0.6, edgecolors='k', vmin=0, vmax=1)\naxes[1].set_title('GMM Confidence (Max Probability)')\naxes[1].set_xlabel('PC1')\naxes[1].set_ylabel('PC2')\nplt.colorbar(scatter2, ax=axes[1], label='Confidence')\n\nplt.tight_layout()\nplt.show()\n\n\n=== 불확실성 분석 ===\n확신도 &gt; 0.9: 327개 (98.2%)\n확신도 &lt; 0.7: 2개 (0.6%)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#알고리즘-종합-비교",
    "href": "part3/06. 군집 분석.html#알고리즘-종합-비교",
    "title": "24  군집 분석",
    "section": "24.6 알고리즘 종합 비교",
    "text": "24.6 알고리즘 종합 비교\n군집 알고리즘 비교\n\n\n\n\n\n\n\n\n\n구분\nK-Means\nDBSCAN\nGMM\n\n\n\n\n군집 수\n사전 지정\n자동\n사전 지정\n\n\n군집 형태\n구형\n자유 (비구형)\n타원형\n\n\n할당 방식\n하드\n하드\n소프트 (확률)\n\n\n이상치 처리\n약함\n강함 (노이즈 탐지)\n약함\n\n\n계산 속도\n빠름\n중간\n느림\n\n\n스케일 민감도\n높음\n높음\n높음\n\n\n파라미터 튜닝\nK\neps, min_samples\nn_components\n\n\n적용 상황\n일반적, 빠른 탐색\n이상치 탐지, 복잡한 형태\n확률 필요, 타원형 군집\n\n\n\n예제: 세 가지 알고리즘 비교\n\n# 세 가지 알고리즘 결과 비교\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\n# K-Means\nscatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_kmeans,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[0].set_title(f'K-Means (K=3)\\nSilhouette: {silhouette_score(X_scaled, labels_kmeans):.3f}')\naxes[0].set_xlabel('PC1')\naxes[0].set_ylabel('PC2')\n\n# DBSCAN\nscatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_dbscan,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\nn_clusters_db = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\naxes[1].set_title(f'DBSCAN (eps=0.8)\\nClusters: {n_clusters_db}')\naxes[1].set_xlabel('PC1')\naxes[1].set_ylabel('PC2')\n\n# GMM\nscatter3 = axes[2].scatter(X_pca[:, 0], X_pca[:, 1], c=labels_gmm,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[2].set_title(f'GMM (n=3)\\nSilhouette: {silhouette_score(X_scaled, labels_gmm):.3f}')\naxes[2].set_xlabel('PC1')\naxes[2].set_ylabel('PC2')\n\n# 실제 종\nscatter4 = axes[3].scatter(X_pca[:, 0], X_pca[:, 1],\n                           c=y_true.astype('category').cat.codes,\n                           cmap='viridis', alpha=0.6, edgecolors='k')\naxes[3].set_title('True Species')\naxes[3].set_xlabel('PC1')\naxes[3].set_ylabel('PC2')\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#군집-결과-평가",
    "href": "part3/06. 군집 분석.html#군집-결과-평가",
    "title": "24  군집 분석",
    "section": "24.7 군집 결과 평가",
    "text": "24.7 군집 결과 평가\n군집 분석은 정답이 없으므로, 평가 방법이 분류와 다르다.\n평가 지표 종류\n\n\n\n\n\n\n\n\n\n\n지표\n타입\n설명\n범위\n해석\n\n\n\n\nSilhouette Score\n내재적\n군집 응집도와 분리도\n-1 ~ 1\n높을수록 좋음\n\n\nDavies-Bouldin Index\n내재적\n군집 간 거리와 군집 내 분산\n0 ~ ∞\n낮을수록 좋음\n\n\nCalinski-Harabasz\n내재적\n군집 간/내 분산 비율\n0 ~ ∞\n높을수록 좋음\n\n\nAdjusted Rand Index\n외재적\n실제 라벨과 비교\n-1 ~ 1\n1에 가까울수록 좋음\n\n\n\n예제: 평가 지표 계산\n\nfrom sklearn.metrics import (silhouette_score, davies_bouldin_score, \n                              calinski_harabasz_score, adjusted_rand_score)\n\n# 각 알고리즘별 평가\nalgorithms = {\n    'K-Means': labels_kmeans,\n    'DBSCAN': labels_dbscan,\n    'GMM': labels_gmm\n}\n\nresults = []\nfor name, labels in algorithms.items():\n    # DBSCAN 노이즈 제거 (평가 시)\n    if name == 'DBSCAN':\n        mask = labels != -1\n        X_eval = X_scaled[mask]\n        labels_eval = labels[mask]\n    else:\n        X_eval = X_scaled\n        labels_eval = labels\n    \n    # 군집이 1개 이하면 평가 불가\n    if len(set(labels_eval)) &lt;= 1:\n        continue\n    \n    silhouette = silhouette_score(X_eval, labels_eval)\n    davies_bouldin = davies_bouldin_score(X_eval, labels_eval)\n    calinski = calinski_harabasz_score(X_eval, labels_eval)\n    \n    # 실제 라벨과 비교 (참고용)\n    if name == 'DBSCAN':\n        ari = adjusted_rand_score(y_true[mask], labels_eval)\n    else:\n        ari = adjusted_rand_score(y_true, labels_eval)\n    \n    results.append({\n        'Algorithm': name,\n        'Silhouette': silhouette,\n        'Davies-Bouldin': davies_bouldin,\n        'Calinski-Harabasz': calinski,\n        'ARI (vs True)': ari\n    })\n\nresults_df = pd.DataFrame(results)\nprint(\"=== 군집 평가 결과 ===\")\nprint(results_df.round(3))\n\n=== 군집 평가 결과 ===\n  Algorithm  Silhouette  Davies-Bouldin  Calinski-Harabasz  ARI (vs True)\n0   K-Means       0.446           0.942            427.773          0.799\n1    DBSCAN       0.535           0.706            472.374          0.651\n2       GMM       0.453           0.899            413.159          0.959",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#모델-선택-가이드",
    "href": "part3/06. 군집 분석.html#모델-선택-가이드",
    "title": "24  군집 분석",
    "section": "24.8 모델 선택 가이드",
    "text": "24.8 모델 선택 가이드\n상황별 알고리즘 선택\n\n\n\n상황\n권장 알고리즘\n이유\n\n\n\n\n빠른 탐색, 기준선\nK-Means\n빠르고 간단\n\n\n군집 수 모름\nDBSCAN\n자동 결정\n\n\n이상치 탐지 필요\nDBSCAN\n노이즈 분리\n\n\n비구형 군집\nDBSCAN\n형태 자유\n\n\n확률 필요\nGMM\n소프트 할당\n\n\n타원형 군집\nGMM\n공분산 고려\n\n\n대용량 데이터\nK-Means, Mini-Batch K-Means\n속도\n\n\n\n의사결정 흐름\n군집 수를 알고 있는가?\n├─ Yes → 확률이 필요한가?\n│         ├─ Yes → GMM\n│         └─ No → K-Means\n└─ No → 이상치 탐지가 중요한가?\n          ├─ Yes → DBSCAN\n          └─ No → Elbow/Silhouette로 K 찾기 → K-Means",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#실무-체크리스트",
    "href": "part3/06. 군집 분석.html#실무-체크리스트",
    "title": "24  군집 분석",
    "section": "24.9 실무 체크리스트",
    "text": "24.9 실무 체크리스트\n군집 분석 수행 시 확인사항\n\n데이터 표준화 수행\n적절한 알고리즘 선택 (목적에 맞게)\n하이퍼파라미터 튜닝 (K, eps 등)\n여러 평가 지표로 검증\n시각화로 결과 확인 (PCA, t-SNE)\n실무적 해석 가능성 검토\n다른 알고리즘과 비교\n군집 프로파일링 (각 군집 특성 분석)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/06. 군집 분석.html#요약",
    "href": "part3/06. 군집 분석.html#요약",
    "title": "24  군집 분석",
    "section": "24.10 요약",
    "text": "24.10 요약\n이 장에서는 비지도 학습의 핵심인 군집 분석을 학습했다. 주요 내용은 다음과 같다.\n군집 알고리즘 핵심\n\nK-Means: 빠르고 간단, 구형 군집, K 사전 지정\nDBSCAN: 밀도 기반, 이상치 탐지, 비구형 가능\nGMM: 확률 기반, 소프트 할당, 타원형 군집\n\n평가 및 선택\n\n평가: Silhouette, Davies-Bouldin 등 내재적 지표\n시각화: PCA/t-SNE로 2D 투영 후 확인\n비교: 여러 알고리즘 결과 종합 판단\n\n주의사항\n\n군집에는 절대적 정답이 없음\n수치 평가와 시각적 해석 모두 중요\n실무적 의미 있는 군집인지 확인 필수\n하이퍼파라미터에 결과가 크게 좌우됨\n\n군집 분석은 데이터의 숨겨진 구조를 발견하는 강력한 도구이다. 알고리즘의 가정과 특성을 이해하고, 목적에 맞는 방법을 선택하며, 다양한 평가 방법으로 검증하는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>군집 분석</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html",
    "href": "part3/07. 서포트 벡터 머신.html",
    "title": "25  서포트 벡터 머신",
    "section": "",
    "text": "25.1 SVM의 핵심 개념\n서포트 벡터 머신(Support Vector Machine, SVM)은 클래스를 가장 잘 구분하는 최적의 결정 경계(초평면)를 찾는 강력한 분류 모델이다. 핵심 아이디어는 클래스 간 마진(margin)을 최대화하는 것이며, 경계에 가장 가까운 일부 데이터(서포트 벡터)만 사용하여 모델을 결정한다. SVM은 특히 고차원 데이터와 비선형 분류에서 우수한 성능을 보인다. 이 장에서는 선형 SVM부터 커널 트릭을 사용한 비선형 SVM까지 원리와 실무 활용법을 학습한다.\n예제: 데이터 로드\nSVM은 클래스 간의 마진을 최대화하는 최적의 결정 경계를 찾는다.\nSVM의 핵심 아이디어\nSVM의 장점\nSVM의 단점",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#svm의-핵심-개념",
    "href": "part3/07. 서포트 벡터 머신.html#svm의-핵심-개념",
    "title": "25  서포트 벡터 머신",
    "section": "",
    "text": "개념\n설명\n\n\n\n\n결정 경계 (Decision Boundary)\n클래스를 구분하는 초평면\n\n\n마진 (Margin)\n결정 경계와 가장 가까운 데이터 사이의 거리\n\n\n서포트 벡터 (Support Vectors)\n마진 경계에 위치한 핵심 데이터\n\n\n마진 최대화\n일반화 성능 향상 목표\n\n\n\n\n\n일반화 성능 우수 (마진 최대화)\n고차원 데이터에 효과적\n커널 트릭으로 비선형 문제 해결\n소수의 서포트 벡터만 사용 (메모리 효율적)\n이상치에 비교적 강건\n\n\n\n대규모 데이터셋에서 학습 시간 오래 걸림 (O(n²~n³))\n하이퍼파라미터 튜닝 필요 (C, gamma)\n확률 출력 기본 제공 안 함\n모델 해석 어려움\n다중 클래스 분류는 내부적으로 여러 이진 분류기 조합",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#마진과-서포트-벡터",
    "href": "part3/07. 서포트 벡터 머신.html#마진과-서포트-벡터",
    "title": "25  서포트 벡터 머신",
    "section": "25.2 마진과 서포트 벡터",
    "text": "25.2 마진과 서포트 벡터\n마진(Margin)\n\n결정 경계와 가장 가까운 데이터 사이의 거리\n마진이 클수록 일반화 성능 향상\nSVM은 마진을 최대화하는 초평면 찾기\n\n서포트 벡터(Support Vectors)\n\n마진 경계에 위치한 핵심 데이터 포인트\n결정 경계를 실제로 결정하는 데이터\n전체 데이터의 일부만 사용 (효율적)\n나머지 데이터는 모델에 영향 없음\n\nSVM vs 로지스틱 회귀\n\n\n\n구분\n로지스틱 회귀\nSVM\n\n\n\n\n목적 함수\n확률 최대화 (로그 우도)\n마진 최대화\n\n\n출력\n확률 (0~1)\n클래스 (확률은 옵션)\n\n\n결정 경계\n확률 기반\n마진 기반\n\n\n영향 데이터\n모든 데이터\n서포트 벡터만\n\n\n해석\n쉬움 (계수)\n어려움\n\n\n비선형\n다항식 특성 추가\n커널 트릭\n\n\n대규모 데이터\n적합\n부적합",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#하드-마진-vs-소프트-마진",
    "href": "part3/07. 서포트 벡터 머신.html#하드-마진-vs-소프트-마진",
    "title": "25  서포트 벡터 머신",
    "section": "25.3 하드 마진 vs 소프트 마진",
    "text": "25.3 하드 마진 vs 소프트 마진\n하드 마진 SVM\n\n모든 데이터를 완벽히 분리\n오분류 허용 안 함\n현실 데이터에서는 거의 불가능\n노이즈나 이상치에 매우 민감\n\n소프트 마진 SVM\n\n일부 오분류 허용\nC 파라미터로 허용 정도 조절\n실무에서 주로 사용\n과적합 방지\n\nC 파라미터의 의미\n\n\n\nC 값\n효과\n마진\n과적합 위험\n\n\n\n\nC ↑ (큼)\n오분류 엄격히 금지\n좁음\n높음\n\n\nC = 1\n균형 (기본값)\n중간\n중간\n\n\nC ↓ (작음)\n오분류 허용 많음\n넓음\n낮음 (과소적합 가능)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#선형-svm",
    "href": "part3/07. 서포트 벡터 머신.html#선형-svm",
    "title": "25  서포트 벡터 머신",
    "section": "25.4 선형 SVM",
    "text": "25.4 선형 SVM\n선형 SVM은 선형 결정 경계로 클래스를 구분한다.\n선형 SVM 특징\n\n선형 초평면으로 분리\n고차원 데이터에 효과적\n계산 효율적\n로지스틱 회귀의 강력한 대안\n\n\n25.4.1 선형 SVM 실습\n예제: 선형 SVM 학습\n\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n\n# 선형 SVM 파이프라인 (스케일링 필수)\npipe_svm_linear = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', SVC(kernel='linear', C=1.0, random_state=42))\n])\n\n# 학습\npipe_svm_linear.fit(X_train, y_train)\n\n# 예측\ny_pred_linear = pipe_svm_linear.predict(X_test)\n\n# 평가\naccuracy_linear = accuracy_score(y_test, y_pred_linear)\nprint(\"=== 선형 SVM 성능 ===\")\nprint(f\"정확도: {accuracy_linear:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_linear))\n\n# 서포트 벡터 개수\nsvm_model = pipe_svm_linear.named_steps['model']\nprint(f\"\\n서포트 벡터 개수: {svm_model.n_support_}\")\nprint(f\"전체 학습 데이터 대비: {svm_model.n_support_.sum() / len(X_train) * 100:.1f}%\")\n\n=== 선형 SVM 성능 ===\n정확도: 1.0000\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        29\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n서포트 벡터 개수: [12 10  4]\n전체 학습 데이터 대비: 9.8%\n\n\n예제: C 값에 따른 성능 변화\n\n# 다양한 C 값으로 실험\nC_values = [0.01, 0.1, 1.0, 10.0, 100.0]\naccuracies = []\nn_support_vectors = []\n\nfor C in C_values:\n    svm_temp = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', SVC(kernel='linear', C=C, random_state=42))\n    ])\n    svm_temp.fit(X_train, y_train)\n    acc = svm_temp.score(X_test, y_test)\n    n_sv = svm_temp.named_steps['model'].n_support_.sum()\n    \n    accuracies.append(acc)\n    n_support_vectors.append(n_sv)\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 정확도\naxes[0].semilogx(C_values, accuracies, marker='o', linewidth=2)\naxes[0].set_xlabel('C (Regularization)')\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Linear SVM: C vs Accuracy')\naxes[0].grid(True, alpha=0.3)\n\n# 서포트 벡터 수\naxes[1].semilogx(C_values, n_support_vectors, marker='o', linewidth=2, color='orange')\naxes[1].set_xlabel('C (Regularization)')\naxes[1].set_ylabel('Number of Support Vectors')\naxes[1].set_title('Linear SVM: C vs Support Vectors')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n=== C 값별 결과 ===\")\nfor C, acc, n_sv in zip(C_values, accuracies, n_support_vectors):\n    print(f\"C={C:6.2f}: Accuracy={acc:.4f}, Support Vectors={n_sv}\")\n\n\n\n\n\n\n\n\n\n=== C 값별 결과 ===\nC=  0.01: Accuracy=0.9254, Support Vectors=149\nC=  0.10: Accuracy=1.0000, Support Vectors=61\nC=  1.00: Accuracy=1.0000, Support Vectors=26\nC= 10.00: Accuracy=1.0000, Support Vectors=18\nC=100.00: Accuracy=1.0000, Support Vectors=12",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#비선형-svm과-커널-트릭",
    "href": "part3/07. 서포트 벡터 머신.html#비선형-svm과-커널-트릭",
    "title": "25  서포트 벡터 머신",
    "section": "25.5 비선형 SVM과 커널 트릭",
    "text": "25.5 비선형 SVM과 커널 트릭\n현실 데이터는 선형으로 분리되지 않는 경우가 많다. 커널 함수를 사용하면 비선형 경계를 만들 수 있다.\n커널 트릭의 개념\n\n원래 공간에서는 선형 분리 불가능\n고차원 공간으로 매핑하면 선형 분리 가능\n실제로 고차원 변환을 계산하지 않음\n내적만 계산 (커널 함수) → 효율적\n\n주요 커널 함수\n\n\n\n\n\n\n\n\n\n커널\n수식\n특징\n사용 상황\n\n\n\n\nLinear\nK(x, y) = x·y\n선형 경계\n선형 분리 가능\n\n\nPolynomial\nK(x, y) = (γx·y + r)ᵈ\nd차 다항식 경계\n중간 복잡도\n\n\nRBF (Gaussian)\nK(x, y) = exp(-γ‖x-y‖²)\n무한 차원, 유연\n일반적 상황 (가장 많이 사용)\n\n\nSigmoid\nK(x, y) = tanh(γx·y + r)\n신경망 유사\n특수 상황",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#rbf-커널과-하이퍼파라미터",
    "href": "part3/07. 서포트 벡터 머신.html#rbf-커널과-하이퍼파라미터",
    "title": "25  서포트 벡터 머신",
    "section": "25.6 RBF 커널과 하이퍼파라미터",
    "text": "25.6 RBF 커널과 하이퍼파라미터\nRBF(Radial Basis Function) 커널은 가장 널리 사용되는 비선형 커널이다.\nRBF 커널 특징\n\n가우시안 함수 기반\n무한 차원 특성 공간으로 매핑\n매우 유연한 결정 경계\n대부분의 비선형 문제에 효과적\n\ngamma (γ) 파라미터\n\n\n\ngamma 값\n영향 범위\n결정 경계\n과적합 위험\n\n\n\n\ngamma ↑ (큼)\n좁음\n복잡, 구불구불\n높음\n\n\ngamma = ‘scale’ (기본)\n적절\n균형\n중간\n\n\ngamma ↓ (작음)\n넓음\n단순, 부드러움\n낮음 (과소적합 가능)\n\n\n\nC와 gamma의 조합 효과\n\n\n\nC  gamma\ngamma 큼\ngamma 작음\n\n\n\n\nC 큼\n과적합 (복잡한 경계)\n적절\n\n\nC 작음\n적절\n과소적합 (단순한 경계)\n\n\n\n\n25.6.1 RBF SVM 실습\n예제: RBF SVM 학습\n\n# RBF SVM 파이프라인\npipe_svm_rbf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42))\n])\n\n# 학습\npipe_svm_rbf.fit(X_train, y_train)\n\n# 예측\ny_pred_rbf = pipe_svm_rbf.predict(X_test)\n\n# 평가\naccuracy_rbf = accuracy_score(y_test, y_pred_rbf)\nprint(\"=== RBF SVM 성능 ===\")\nprint(f\"정확도: {accuracy_rbf:.4f}\")\nprint(f\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred_rbf))\n\n=== RBF SVM 성능 ===\n정확도: 1.0000\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        29\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\n예제: gamma 값에 따른 성능 변화\n\n# 다양한 gamma 값으로 실험\ngamma_values = [0.001, 0.01, 0.1, 1.0, 10.0]\naccuracies_gamma = []\n\nfor gamma in gamma_values:\n    svm_temp = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', SVC(kernel='rbf', C=1.0, gamma=gamma, random_state=42))\n    ])\n    svm_temp.fit(X_train, y_train)\n    acc = svm_temp.score(X_test, y_test)\n    accuracies_gamma.append(acc)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.semilogx(gamma_values, accuracies_gamma, marker='o', linewidth=2)\nplt.xlabel('gamma')\nplt.ylabel('Accuracy')\nplt.title('RBF SVM: gamma vs Accuracy (C=1.0)')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n=== gamma 값별 결과 ===\")\nfor gamma, acc in zip(gamma_values, accuracies_gamma):\n    print(f\"gamma={gamma:6.3f}: Accuracy={acc:.4f}\")\n\n\n\n\n\n\n\n\n\n=== gamma 값별 결과 ===\ngamma= 0.001: Accuracy=0.7910\ngamma= 0.010: Accuracy=0.9701\ngamma= 0.100: Accuracy=1.0000\ngamma= 1.000: Accuracy=1.0000\ngamma=10.000: Accuracy=0.9104\n\n\n예제: GridSearchCV로 최적 하이퍼파라미터 찾기\n\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 그리드\nparam_grid = {\n    'model__C': [0.1, 1.0, 10.0, 100.0],\n    'model__gamma': [0.001, 0.01, 0.1, 1.0]\n}\n\n# GridSearchCV\ngrid_search = GridSearchCV(\n    pipe_svm_rbf,\n    param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1\n)\n\ngrid_search.fit(X_train, y_train)\n\nprint(\"=== GridSearchCV 결과 ===\")\nprint(f\"최적 파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\nprint(f\"테스트 점수: {grid_search.score(X_test, y_test):.4f}\")\n\n# 최적 모델\nbest_svm = grid_search.best_estimator_\n\nFitting 5 folds for each of 16 candidates, totalling 80 fits\n=== GridSearchCV 결과 ===\n최적 파라미터: {'model__C': 100.0, 'model__gamma': 0.01}\n최적 교차 검증 점수: 0.9774\n테스트 점수: 1.0000\n\n\n예제: C-gamma 히트맵\n\n# C-gamma 조합별 성능\nresults = pd.DataFrame(grid_search.cv_results_)\npivot_table = results.pivot_table(\n    values='mean_test_score',\n    index='param_model__gamma',\n    columns='param_model__C'\n)\n\n# 히트맵\nplt.figure(figsize=(10, 8))\nsns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlGnBu')\nplt.xlabel('C')\nplt.ylabel('gamma')\nplt.title('RBF SVM: C vs gamma (Cross-Validation Accuracy)')\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#다중-클래스-분류",
    "href": "part3/07. 서포트 벡터 머신.html#다중-클래스-분류",
    "title": "25  서포트 벡터 머신",
    "section": "25.7 다중 클래스 분류",
    "text": "25.7 다중 클래스 분류\nSVM은 본래 이진 분류 모델이지만, 다중 클래스에도 적용 가능하다.\n다중 클래스 전략\n\n\n\n\n\n\n\n\n\n전략\n설명\n분류기 수\nscikit-learn 기본값\n\n\n\n\nOne-vs-Rest (OvR)\n각 클래스 vs 나머지\nk개 (k=클래스 수)\nLinearSVC\n\n\nOne-vs-One (OvO)\n모든 클래스 쌍 비교\nk(k-1)/2개\nSVC\n\n\n\n예제: 다중 클래스 확인\n\n# 다중 클래스 SVM (자동 처리)\nsvm_multiclass = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\nscaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nsvm_multiclass.fit(X_train_scaled, y_train)\n\nprint(\"=== 다중 클래스 SVM ===\")\nprint(f\"클래스 수: {len(svm_multiclass.classes_)}\")\nprint(f\"클래스: {svm_multiclass.classes_}\")\nprint(f\"이진 분류기 수 (OvO): {len(svm_multiclass.classes_) * (len(svm_multiclass.classes_) - 1) // 2}\")\n\n# 혼동 행렬\ny_pred = svm_multiclass.predict(X_test_scaled)\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=svm_multiclass.classes_,\n            yticklabels=svm_multiclass.classes_)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('SVM Multiclass: Confusion Matrix')\nplt.tight_layout()\nplt.show()\n\n=== 다중 클래스 SVM ===\n클래스 수: 3\n클래스: ['Adelie' 'Chinstrap' 'Gentoo']\n이진 분류기 수 (OvO): 3",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#커널-비교",
    "href": "part3/07. 서포트 벡터 머신.html#커널-비교",
    "title": "25  서포트 벡터 머신",
    "section": "25.8 커널 비교",
    "text": "25.8 커널 비교\n예제: 다양한 커널 비교\n\n# 다양한 커널로 학습\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nresults_kernel = []\n\nfor kernel in kernels:\n    pipe_temp = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', SVC(kernel=kernel, random_state=42))\n    ])\n    pipe_temp.fit(X_train, y_train)\n    acc = pipe_temp.score(X_test, y_test)\n    n_sv = pipe_temp.named_steps['model'].n_support_.sum()\n    \n    results_kernel.append({\n        'Kernel': kernel,\n        'Accuracy': acc,\n        'Support Vectors': n_sv\n    })\n\nresults_kernel_df = pd.DataFrame(results_kernel)\nprint(\"=== 커널별 성능 비교 ===\")\nprint(results_kernel_df)\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].bar(results_kernel_df['Kernel'], results_kernel_df['Accuracy'])\naxes[0].set_ylabel('Accuracy')\naxes[0].set_title('Kernel Comparison: Accuracy')\naxes[0].set_ylim([0.8, 1.0])\n\naxes[1].bar(results_kernel_df['Kernel'], results_kernel_df['Support Vectors'])\naxes[1].set_ylabel('Number of Support Vectors')\naxes[1].set_title('Kernel Comparison: Support Vectors')\n\nplt.tight_layout()\nplt.show()\n\n=== 커널별 성능 비교 ===\n    Kernel  Accuracy  Support Vectors\n0   linear  1.000000               26\n1     poly  0.985075               75\n2      rbf  1.000000               46\n3  sigmoid  1.000000               52",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#모델-비교",
    "href": "part3/07. 서포트 벡터 머신.html#모델-비교",
    "title": "25  서포트 벡터 머신",
    "section": "25.9 모델 비교",
    "text": "25.9 모델 비교\n예제: 로지스틱 회귀 vs SVM\n\nfrom sklearn.linear_model import LogisticRegression\n\n# 로지스틱 회귀\npipe_lr = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression(max_iter=1000, random_state=42))\n])\npipe_lr.fit(X_train, y_train)\n\n# 성능 비교\nmodels = {\n    'Logistic Regression': pipe_lr,\n    'Linear SVM': pipe_svm_linear,\n    'RBF SVM': best_svm\n}\n\ncomparison_results = []\nfor name, model in models.items():\n    acc = model.score(X_test, y_test)\n    comparison_results.append({'Model': name, 'Accuracy': acc})\n\ncomparison_df = pd.DataFrame(comparison_results)\nprint(\"\\n=== 모델 성능 비교 ===\")\nprint(comparison_df)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.bar(comparison_df['Model'], comparison_df['Accuracy'])\nplt.ylabel('Accuracy')\nplt.title('Model Comparison')\nplt.ylim([0.9, 1.0])\nplt.xticks(rotation=15, ha='right')\nplt.tight_layout()\nplt.show()\n\n\n=== 모델 성능 비교 ===\n                 Model  Accuracy\n0  Logistic Regression       1.0\n1           Linear SVM       1.0\n2              RBF SVM       1.0",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#활용-가이드",
    "href": "part3/07. 서포트 벡터 머신.html#활용-가이드",
    "title": "25  서포트 벡터 머신",
    "section": "25.10 활용 가이드",
    "text": "25.10 활용 가이드\nSVM 사용이 적합한 경우\n\n\n\n상황\n이유\n\n\n\n\n고차원 데이터\n차원의 저주에 강함\n\n\n명확한 마진\n클래스 간 분리가 뚜렷\n\n\n소~중규모 데이터\n효율적 학습 가능\n\n\n비선형 경계 필요\n커널 트릭 활용\n\n\n이상치 존재\n서포트 벡터만 사용\n\n\n\nSVM 사용이 부적합한 경우\n\n\n\n상황\n대안\n\n\n\n\n대규모 데이터 (100만+ 샘플)\n로지스틱 회귀, SGDClassifier\n\n\n확률 출력 필수\n로지스틱 회귀, RandomForest\n\n\n모델 해석 중요\n로지스틱 회귀, 결정 트리\n\n\n실시간 예측 중요\n경량 모델 (나이브 베이즈)\n\n\n\n하이퍼파라미터 튜닝 가이드\n\n커널 선택: linear → RBF 순서로 시도\nC 튜닝: 0.1, 1, 10, 100 범위에서 시작\ngamma 튜닝 (RBF): ‘scale’, ‘auto’, 또는 0.001~10 범위\nGridSearchCV: 교차 검증으로 최적값 찾기\n스케일링: 항상 표준화 수행",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/07. 서포트 벡터 머신.html#요약",
    "href": "part3/07. 서포트 벡터 머신.html#요약",
    "title": "25  서포트 벡터 머신",
    "section": "25.11 요약",
    "text": "25.11 요약\n이 장에서는 강력한 분류 모델인 SVM을 학습했다. 주요 내용은 다음과 같다.\nSVM 핵심 개념\n\n마진 최대화: 일반화 성능 향상\n서포트 벡터: 일부 핵심 데이터만 사용\n소프트 마진: C로 오분류 허용 조절\n커널 트릭: 비선형 문제 해결\n\n주요 하이퍼파라미터\n\n\n\n파라미터\n역할\n조정 방향\n\n\n\n\nC\n오분류 허용 정도\n큼 → 엄격, 작음 → 관대\n\n\ngamma (RBF)\n영향 범위\n큼 → 좁음, 작음 → 넓음\n\n\nkernel\n결정 경계 형태\nlinear, rbf, poly\n\n\n\n실무 체크리스트\n\n데이터 스케일링 수행 (필수)\n적절한 커널 선택 (linear → rbf)\nGridSearchCV로 C, gamma 튜닝\n교차 검증으로 성능 평가\n서포트 벡터 수 확인\n다른 모델과 비교\n계산 시간 고려 (대규모 데이터)\n\nSVM은 마진 최대화를 통해 우수한 일반화 성능을 달성하는 강력한 분류 모델이다. 특히 고차원 데이터와 비선형 문제에서 효과적이며, 적절한 하이퍼파라미터 튜닝을 통해 최적의 성능을 얻을 수 있다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>서포트 벡터 머신</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html",
    "href": "part3/08. 모델 성능 평가.html",
    "title": "26  모델 성능 평가",
    "section": "",
    "text": "26.1 모델 성능 평가의 목적\n모델 성능 평가(Model Evaluation)는 머신러닝 모델이 얼마나 잘 작동하는지 측정하고 판단하는 과정이다. 단순히 “정확한가?”를 묻는 것이 아니라, “새로운 데이터에서도 잘 작동하는가?”, “어떤 유형의 오류를 범하는가?”, “모델 간 공정한 비교가 가능한가?”를 파악하는 것이 목적이다. 문제 유형(회귀/분류/군집)과 비즈니스 맥락에 따라 적절한 평가 지표를 선택하는 것이 중요하다. 이 장에서는 회귀, 분류, 군집 문제별 주요 평가 지표의 의미와 활용법을 학습한다.\n예제: 데이터 로드\n모델 평가는 숫자를 계산하는 것이 아니라 올바른 판단을 내리기 위한 근거를 제공하는 과정이다.\n핵심 질문\n평가의 원칙",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#모델-성능-평가의-목적",
    "href": "part3/08. 모델 성능 평가.html#모델-성능-평가의-목적",
    "title": "26  모델 성능 평가",
    "section": "",
    "text": "질문\n의미\n평가 방법\n\n\n\n\n일반화 성능\n새 데이터에서도 잘 작동하는가?\n테스트셋 평가, 교차 검증\n\n\n오류 유형\n어떤 실수를 하는가?\n혼동 행렬, 잔차 분석\n\n\n비교 가능성\n모델 간 공정한 비교가 가능한가?\n동일한 데이터셋, 동일한 지표\n\n\n실무 적합성\n비즈니스 목표에 부합하는가?\n맥락에 맞는 지표 선택\n\n\n\n\n\n단일 지표 금지: 여러 지표를 종합적으로 고려\n맥락 고려: 문제 상황에 맞는 지표 선택\n테스트 기준: 항상 테스트 데이터로 평가\n교차 검증: 안정적인 성능 추정\n편향 방지: 데이터 누수 주의",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#회귀-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#회귀-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "26.2 회귀 성능 평가",
    "text": "26.2 회귀 성능 평가\n회귀 문제는 예측값이 실제값과 얼마나 다른지를 측정한다.\n회귀 평가 지표 종류\n\n\n\n\n\n\n\n\n\n\n지표\n수식\n범위\n단위\n특징\n\n\n\n\nMAE\n\\(\\frac{1}{n}\\sum\\|y_i - \\hat{y}_i\\|\\)\n0 ~ ∞\n원본\n직관적, 이상치 영향 적음\n\n\nMSE\n\\(\\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2\\)\n0 ~ ∞\n제곱\n큰 오차 강조\n\n\nRMSE\n\\(\\sqrt{\\text{MSE}}\\)\n0 ~ ∞\n원본\nMSE의 해석 가능 버전\n\n\nR²\n\\(1 - \\frac{\\text{SS}_{\\text{res}}}{\\text{SS}_{\\text{tot}}}\\)\n-∞ ~ 1\n없음\n상대적 성능\n\n\n\n\n26.2.1 회귀 실습\n예제: 데이터 준비 및 모델 학습\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# 데이터 분할\nX_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n    X_reg, y_reg, test_size=0.2, random_state=42\n)\n\n# 선형 회귀 모델\nlr = LinearRegression()\nlr.fit(X_train_reg, y_train_reg)\ny_pred_lr = lr.predict(X_test_reg)\n\n# 랜덤 포레스트 모델\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_reg, y_train_reg)\ny_pred_rf = rf.predict(X_test_reg)\n\nprint(\"모델 학습 완료\")\n\n모델 학습 완료\n\n\n예제: 회귀 평가 지표 계산\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndef evaluate_regression(y_true, y_pred, model_name):\n    \"\"\"회귀 모델 평가\"\"\"\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"\\n=== {model_name} 평가 ===\")\n    print(f\"MAE:  {mae:.2f}g\")\n    print(f\"MSE:  {mse:.2f}g²\")\n    print(f\"RMSE: {rmse:.2f}g\")\n    print(f\"R²:   {r2:.4f}\")\n    \n    return {'Model': model_name, 'MAE': mae, 'RMSE': rmse, 'R²': r2}\n\n# 평가\nresults_reg = []\nresults_reg.append(evaluate_regression(y_test_reg, y_pred_lr, 'Linear Regression'))\nresults_reg.append(evaluate_regression(y_test_reg, y_pred_rf, 'Random Forest'))\n\nresults_reg_df = pd.DataFrame(results_reg)\nprint(\"\\n=== 모델 비교 ===\")\nprint(results_reg_df)\n\n\n=== Linear Regression 평가 ===\nMAE:  289.69g\nMSE:  127200.47g²\nRMSE: 356.65g\nR²:   0.7981\n\n=== Random Forest 평가 ===\nMAE:  262.99g\nMSE:  106814.96g²\nRMSE: 326.83g\nR²:   0.8304\n\n=== 모델 비교 ===\n               Model         MAE        RMSE        R²\n0  Linear Regression  289.689016  356.651752  0.798076\n1      Random Forest  262.985075  326.825579  0.830437\n\n\n예제: 잔차 분석\n\n# 잔차 계산\nresiduals_lr = y_test_reg - y_pred_lr\nresiduals_rf = y_test_reg - y_pred_rf\n\n# 시각화\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Linear Regression 잔차\naxes[0, 0].scatter(y_pred_lr, residuals_lr, alpha=0.6, edgecolors='k')\naxes[0, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[0, 0].set_xlabel('Predicted')\naxes[0, 0].set_ylabel('Residuals')\naxes[0, 0].set_title('Linear Regression: Residual Plot')\naxes[0, 0].grid(True, alpha=0.3)\n\n# Linear Regression 예측 vs 실제\naxes[0, 1].scatter(y_test_reg, y_pred_lr, alpha=0.6, edgecolors='k')\naxes[0, 1].plot([y_test_reg.min(), y_test_reg.max()], \n                [y_test_reg.min(), y_test_reg.max()], \n                'r--', linewidth=2, label='Perfect Prediction')\naxes[0, 1].set_xlabel('Actual')\naxes[0, 1].set_ylabel('Predicted')\naxes[0, 1].set_title(f'Linear Regression: R²={r2_score(y_test_reg, y_pred_lr):.3f}')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Random Forest 잔차\naxes[1, 0].scatter(y_pred_rf, residuals_rf, alpha=0.6, edgecolors='k')\naxes[1, 0].axhline(y=0, color='r', linestyle='--', linewidth=2)\naxes[1, 0].set_xlabel('Predicted')\naxes[1, 0].set_ylabel('Residuals')\naxes[1, 0].set_title('Random Forest: Residual Plot')\naxes[1, 0].grid(True, alpha=0.3)\n\n# Random Forest 예측 vs 실제\naxes[1, 1].scatter(y_test_reg, y_pred_rf, alpha=0.6, edgecolors='k')\naxes[1, 1].plot([y_test_reg.min(), y_test_reg.max()], \n                [y_test_reg.min(), y_test_reg.max()], \n                'r--', linewidth=2, label='Perfect Prediction')\naxes[1, 1].set_xlabel('Actual')\naxes[1, 1].set_ylabel('Predicted')\naxes[1, 1].set_title(f'Random Forest: R²={r2_score(y_test_reg, y_pred_rf):.3f}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n26.2.2 회귀 지표 선택 가이드\n상황별 지표 선택\n\n\n\n상황\n권장 지표\n이유\n\n\n\n\n직관적 오차 크기\nMAE\n원본 단위, 이해 쉬움\n\n\n큰 오차 중요\nRMSE\n큰 오차에 패널티\n\n\n모델 비교\nR²\n스케일 독립적\n\n\n비율 오차\nMAPE\n상대적 오차\n\n\n\n지표 해석\n\nMAE: 평균적으로 얼마나 틀리는가?\nRMSE: 큰 오차를 고려하면 얼마나 틀리는가?\nR²: 모델이 분산의 몇 %를 설명하는가?",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#분류-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#분류-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "26.3 분류 성능 평가",
    "text": "26.3 분류 성능 평가\n분류 문제는 얼마나 정확히 구분했는지뿐만 아니라 어떤 실수를 했는지가 중요하다.\n\n26.3.1 혼동 행렬 (Confusion Matrix)\n혼동 행렬은 분류 결과를 한눈에 파악할 수 있는 기본 도구이다.\n이진 분류 혼동 행렬\n\n\n\n\n예측 Positive\n예측 Negative\n\n\n\n\n실제 Positive\nTP (True Positive)\nFN (False Negative)\n\n\n실제 Negative\nFP (False Positive)\nTN (True Negative)\n\n\n\n용어 설명\n\nTP: 양성을 양성으로 (정답)\nTN: 음성을 음성으로 (정답)\nFP: 음성을 양성으로 (오탐, Type I Error)\nFN: 양성을 음성으로 (미탐, Type II Error)\n\n\n\n26.3.2 분류 실습\n예제: 데이터 준비 및 모델 학습\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\n\n# 데이터 분할\nX_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n)\n\n# 로지스틱 회귀\npipe_lr_clf = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression(max_iter=1000, random_state=42))\n])\npipe_lr_clf.fit(X_train_clf, y_train_clf)\ny_pred_lr_clf = pipe_lr_clf.predict(X_test_clf)\ny_proba_lr_clf = pipe_lr_clf.predict_proba(X_test_clf)\n\n# 랜덤 포레스트\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_clf.fit(X_train_clf, y_train_clf)\ny_pred_rf_clf = rf_clf.predict(X_test_clf)\ny_proba_rf_clf = rf_clf.predict_proba(X_test_clf)\n\nprint(\"모델 학습 완료\")\n\n모델 학습 완료\n\n\n예제: 혼동 행렬 시각화\n\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n# 혼동 행렬\ncm_lr = confusion_matrix(y_test_clf, y_pred_lr_clf)\ncm_rf = confusion_matrix(y_test_clf, y_pred_rf_clf)\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Logistic Regression\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n            xticklabels=pipe_lr_clf.classes_, yticklabels=pipe_lr_clf.classes_)\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('Actual')\naxes[0].set_title('Logistic Regression: Confusion Matrix')\n\n# Random Forest\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n            xticklabels=rf_clf.classes_, yticklabels=rf_clf.classes_)\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('Actual')\naxes[1].set_title('Random Forest: Confusion Matrix')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n26.3.3 주요 분류 지표\n분류 평가 지표 정의\n\n\n\n\n\n\n\n\n\n지표\n수식\n의미\n언제 중요한가?\n\n\n\n\nAccuracy\n\\(\\frac{TP+TN}{TP+TN+FP+FN}\\)\n전체 정확도\n클래스 균형\n\n\nPrecision\n\\(\\frac{TP}{TP+FP}\\)\n양성 예측의 정확도\n오탐 비용 높음\n\n\nRecall (Sensitivity)\n\\(\\frac{TP}{TP+FN}\\)\n실제 양성 탐지율\n미탐 비용 높음\n\n\nF1-Score\n\\(2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\nPrecision과 Recall의 조화평균\n불균형 데이터\n\n\nSpecificity\n\\(\\frac{TN}{TN+FP}\\)\n실제 음성 탐지율\n음성 클래스 중요\n\n\n\n예제: 분류 평가 지표 계산\n\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score, \n                              f1_score, classification_report)\n\ndef evaluate_classification(y_true, y_pred, model_name):\n    \"\"\"분류 모델 평가\"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    \n    print(f\"\\n=== {model_name} 평가 ===\")\n    print(f\"Accuracy:  {accuracy:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall:    {recall:.4f}\")\n    print(f\"F1-Score:  {f1:.4f}\")\n    \n    return {'Model': model_name, 'Accuracy': accuracy, \n            'Precision': precision, 'Recall': recall, 'F1': f1}\n\n# 평가\nresults_clf = []\nresults_clf.append(evaluate_classification(y_test_clf, y_pred_lr_clf, 'Logistic Regression'))\nresults_clf.append(evaluate_classification(y_test_clf, y_pred_rf_clf, 'Random Forest'))\n\nresults_clf_df = pd.DataFrame(results_clf)\nprint(\"\\n=== 모델 비교 ===\")\nprint(results_clf_df)\n\n\n=== Logistic Regression 평가 ===\nAccuracy:  1.0000\nPrecision: 1.0000\nRecall:    1.0000\nF1-Score:  1.0000\n\n=== Random Forest 평가 ===\nAccuracy:  0.9701\nPrecision: 0.9739\nRecall:    0.9701\nF1-Score:  0.9709\n\n=== 모델 비교 ===\n                 Model  Accuracy  Precision    Recall        F1\n0  Logistic Regression  1.000000   1.000000  1.000000  1.000000\n1        Random Forest  0.970149   0.973881  0.970149  0.970855\n\n\n예제: 분류 리포트\n\n# 상세 분류 리포트\nprint(\"\\n=== Logistic Regression 상세 리포트 ===\")\nprint(classification_report(y_test_clf, y_pred_lr_clf))\n\nprint(\"\\n=== Random Forest 상세 리포트 ===\")\nprint(classification_report(y_test_clf, y_pred_rf_clf))\n\n\n=== Logistic Regression 상세 리포트 ===\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        29\n   Chinstrap       1.00      1.00      1.00        14\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n=== Random Forest 상세 리포트 ===\n              precision    recall  f1-score   support\n\n      Adelie       1.00      0.97      0.98        29\n   Chinstrap       0.88      1.00      0.93        14\n      Gentoo       1.00      0.96      0.98        24\n\n    accuracy                           0.97        67\n   macro avg       0.96      0.97      0.96        67\nweighted avg       0.97      0.97      0.97        67\n\n\n\n\n\n26.3.4 ROC Curve와 AUC\nROC(Receiver Operating Characteristic) 곡선은 분류 임계값 변화에 따른 성능을 시각화한다.\n예제: ROC Curve (이진 분류)\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.preprocessing import label_binarize\n\n# 다중 클래스를 One-vs-Rest로 변환\ny_test_bin = label_binarize(y_test_clf, classes=pipe_lr_clf.classes_)\nn_classes = y_test_bin.shape[1]\n\n# 각 클래스별 ROC Curve\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nfor i, class_name in enumerate(pipe_lr_clf.classes_):\n    # Logistic Regression\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba_lr_clf[:, i])\n    auc = roc_auc_score(y_test_bin[:, i], y_proba_lr_clf[:, i])\n    axes[0].plot(fpr, tpr, linewidth=2, label=f'{class_name} (AUC={auc:.3f})')\n\naxes[0].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\naxes[0].set_xlabel('False Positive Rate')\naxes[0].set_ylabel('True Positive Rate')\naxes[0].set_title('Logistic Regression: ROC Curves')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\nfor i, class_name in enumerate(rf_clf.classes_):\n    # Random Forest\n    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba_rf_clf[:, i])\n    auc = roc_auc_score(y_test_bin[:, i], y_proba_rf_clf[:, i])\n    axes[1].plot(fpr, tpr, linewidth=2, label=f'{class_name} (AUC={auc:.3f})')\n\naxes[1].plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\naxes[1].set_xlabel('False Positive Rate')\naxes[1].set_ylabel('True Positive Rate')\naxes[1].set_title('Random Forest: ROC Curves')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n26.3.5 다중 클래스 평가\n다중 클래스 분류에서는 평균 방식이 중요하다.\n평균 방식\n\n\n\n방식\n설명\n사용 상황\n\n\n\n\nmacro\n각 클래스 평균 (동일 가중치)\n클래스 중요도 동일\n\n\nweighted\n클래스 크기로 가중 평균\n불균형 데이터\n\n\nmicro\n전체 TP, FP, FN 합산 후 계산\n샘플 중심 평가\n\n\n\n예제: 평균 방식 비교\n\n# 다양한 평균 방식\nfor avg in ['macro', 'weighted', 'micro']:\n    f1 = f1_score(y_test_clf, y_pred_rf_clf, average=avg)\n    print(f\"F1-Score ({avg:8s}): {f1:.4f}\")\n\nF1-Score (macro   ): 0.9648\nF1-Score (weighted): 0.9709\nF1-Score (micro   ): 0.9701",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#군집-성능-평가",
    "href": "part3/08. 모델 성능 평가.html#군집-성능-평가",
    "title": "26  모델 성능 평가",
    "section": "26.4 군집 성능 평가",
    "text": "26.4 군집 성능 평가\n군집 분석은 정답 라벨이 없는 경우가 많아 평가 방식이 다르다.\n군집 평가 유형\n\n\n\n유형\n필요 정보\n지표\n\n\n\n\n내부 평가\n데이터만\nSilhouette, Davies-Bouldin, Calinski-Harabasz\n\n\n외부 평가\n정답 라벨\nARI, NMI, Completeness, Homogeneity\n\n\n\n\n26.4.1 내부 평가 지표\n예제: 군집 생성 및 내부 평가\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import (silhouette_score, davies_bouldin_score, \n                              calinski_harabasz_score)\n\n# 데이터 표준화\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_clf)\n\n# K-Means 군집\nkmeans = KMeans(n_clusters=3, random_state=42)\nlabels_kmeans = kmeans.fit_predict(X_scaled)\n\n# 내부 평가 지표\nsilhouette = silhouette_score(X_scaled, labels_kmeans)\ndavies_bouldin = davies_bouldin_score(X_scaled, labels_kmeans)\ncalinski = calinski_harabasz_score(X_scaled, labels_kmeans)\n\nprint(\"=== 군집 내부 평가 ===\")\nprint(f\"Silhouette Score:        {silhouette:.4f} (높을수록 좋음, -1~1)\")\nprint(f\"Davies-Bouldin Index:    {davies_bouldin:.4f} (낮을수록 좋음)\")\nprint(f\"Calinski-Harabasz Score: {calinski:.2f} (높을수록 좋음)\")\n\n=== 군집 내부 평가 ===\nSilhouette Score:        0.4462 (높을수록 좋음, -1~1)\nDavies-Bouldin Index:    0.9420 (낮을수록 좋음)\nCalinski-Harabasz Score: 427.77 (높을수록 좋음)\n\n\n\n\n26.4.2 외부 평가 지표\n정답 라벨이 있는 경우(예: penguins의 실제 종)에만 사용 가능하다.\n예제: 외부 평가\n\nfrom sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score,\n                              completeness_score, homogeneity_score, v_measure_score)\n\n# 외부 평가 지표 (실제 종과 비교)\nari = adjusted_rand_score(y_clf, labels_kmeans)\nnmi = normalized_mutual_info_score(y_clf, labels_kmeans)\ncompleteness = completeness_score(y_clf, labels_kmeans)\nhomogeneity = homogeneity_score(y_clf, labels_kmeans)\nv_measure = v_measure_score(y_clf, labels_kmeans)\n\nprint(\"\\n=== 군집 외부 평가 (vs 실제 종) ===\")\nprint(f\"Adjusted Rand Index (ARI): {ari:.4f} (높을수록 좋음, -1~1)\")\nprint(f\"Normalized Mutual Info:    {nmi:.4f} (높을수록 좋음, 0~1)\")\nprint(f\"Completeness:              {completeness:.4f} (높을수록 좋음, 0~1)\")\nprint(f\"Homogeneity:               {homogeneity:.4f} (높을수록 좋음, 0~1)\")\nprint(f\"V-measure:                 {v_measure:.4f} (높을수록 좋음, 0~1)\")\n\n\n=== 군집 외부 평가 (vs 실제 종) ===\nAdjusted Rand Index (ARI): 0.7994 (높을수록 좋음, -1~1)\nNormalized Mutual Info:    0.7899 (높을수록 좋음, 0~1)\nCompleteness:              0.7790 (높을수록 좋음, 0~1)\nHomogeneity:               0.8012 (높을수록 좋음, 0~1)\nV-measure:                 0.7899 (높을수록 좋음, 0~1)",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#평가-지표-종합-비교",
    "href": "part3/08. 모델 성능 평가.html#평가-지표-종합-비교",
    "title": "26  모델 성능 평가",
    "section": "26.5 평가 지표 종합 비교",
    "text": "26.5 평가 지표 종합 비교\n문제 유형별 평가 지표\n\n\n\n문제 유형\n주요 지표\n보조 지표\n\n\n\n\n회귀\nRMSE, R²\nMAE, MAPE\n\n\n이진 분류 (균형)\nF1-Score, ROC-AUC\nAccuracy\n\n\n이진 분류 (불균형)\nPrecision, Recall\nF1, ROC-AUC\n\n\n다중 분류\nF1 (macro/weighted)\nAccuracy, Confusion Matrix\n\n\n군집 (라벨 없음)\nSilhouette\nDavies-Bouldin\n\n\n군집 (라벨 있음)\nARI\nNMI, V-measure\n\n\n\n상황별 중요 지표\n\n\n\n상황\n중요 지표\n예시\n\n\n\n\n오탐 비용 높음\nPrecision\n스팸 필터 (정상 메일 차단 최소화)\n\n\n미탐 비용 높음\nRecall\n질병 진단 (환자 놓치지 않기)\n\n\n클래스 불균형\nF1, ROC-AUC\n이상 거래 탐지\n\n\n해석 중요\nR², Confusion Matrix\n비즈니스 보고",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#교차-검증",
    "href": "part3/08. 모델 성능 평가.html#교차-검증",
    "title": "26  모델 성능 평가",
    "section": "26.6 교차 검증",
    "text": "26.6 교차 검증\n단일 테스트셋 평가는 운에 좌우될 수 있으므로 교차 검증으로 안정적인 성능을 추정한다.\n예제: 교차 검증\n\nfrom sklearn.model_selection import cross_val_score\n\n# 5-Fold 교차 검증\ncv_scores = cross_val_score(pipe_lr_clf, X_clf, y_clf, cv=5, scoring='accuracy')\n\nprint(\"=== 교차 검증 결과 ===\")\nprint(f\"각 폴드 점수: {cv_scores}\")\nprint(f\"평균: {cv_scores.mean():.4f}\")\nprint(f\"표준편차: {cv_scores.std():.4f}\")\nprint(f\"95% 신뢰구간: {cv_scores.mean():.4f} ± {1.96 * cv_scores.std():.4f}\")\n\n=== 교차 검증 결과 ===\n각 폴드 점수: [1.         0.98507463 0.97014925 1.         1.        ]\n평균: 0.9910\n표준편차: 0.0119\n95% 신뢰구간: 0.9910 ± 0.0234",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#평가-체크리스트",
    "href": "part3/08. 모델 성능 평가.html#평가-체크리스트",
    "title": "26  모델 성능 평가",
    "section": "26.7 평가 체크리스트",
    "text": "26.7 평가 체크리스트\n모델 평가 시 확인사항\n\n적절한 평가 지표 선택 (문제 유형에 맞게)\n테스트 데이터로 평가 (데이터 누수 방지)\n여러 지표 종합 검토 (단일 지표 금지)\n교차 검증으로 안정성 확인\n혼동 행렬로 오류 패턴 분석\n잔차 분석 (회귀)\nROC Curve 확인 (분류)\n비즈니스 맥락 고려",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/08. 모델 성능 평가.html#요약",
    "href": "part3/08. 모델 성능 평가.html#요약",
    "title": "26  모델 성능 평가",
    "section": "26.8 요약",
    "text": "26.8 요약\n이 장에서는 머신러닝 모델의 성능을 평가하는 다양한 지표와 방법을 학습했다. 주요 내용은 다음과 같다.\n핵심 원칙\n\n맥락 고려: 문제와 비즈니스 상황에 맞는 지표 선택\n종합 평가: 단일 지표가 아닌 다각도 검토\n테스트 기준: 항상 테스트 데이터로 평가\n안정성: 교차 검증으로 신뢰도 확보\n\n주요 지표 요약\n\n\n\n문제\n균형 데이터\n불균형 데이터\n추가 고려\n\n\n\n\n회귀\nRMSE, R²\nMAE\n잔차 분석\n\n\n분류\nAccuracy, F1\nPrecision/Recall, F1\n혼동 행렬, ROC\n\n\n군집\nSilhouette\nARI (라벨 있는 경우)\n시각화\n\n\n\n실무 가이드\n\n회귀: RMSE로 시작, R²로 비교, 잔차로 진단\n분류: Confusion Matrix → F1 → ROC (순서대로)\n군집: Silhouette → 시각화 → ARI (가능 시)\n\n모델 성능 평가는 숫자를 계산하는 것이 아니라, 올바른 판단을 내리기 위한 근거를 제공하는 과정이다. 문제의 맥락을 이해하고 적절한 지표를 선택하는 것이 성공적인 머신러닝 프로젝트의 핵심이다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>모델 성능 평가</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html",
    "href": "part3/09. 파이프라인 및 자동화.html",
    "title": "27  파이프라인 및 자동화",
    "section": "",
    "text": "27.1 파이프라인의 필요성\n파이프라인(Pipeline)은 머신러닝 워크플로우의 전처리, 모델링, 평가 단계를 하나의 객체로 묶어 자동화하는 강력한 도구이다. 실무에서 가장 흔한 실수인 데이터 누수, 전처리 불일치, 재현 불가능한 실험을 구조적으로 방지한다. 파이프라인과 하이퍼파라미터 최적화(GridSearchCV, RandomizedSearchCV)를 결합하면 효율적이고 신뢰할 수 있는 머신러닝 워크플로우를 구축할 수 있다. 이 장에서는 파이프라인의 개념부터 실무 활용까지 체계적으로 학습한다.\n예제: 데이터 로드\n머신러닝 실무에서 흔히 발생하는 문제들을 파이프라인이 해결한다.\n파이프라인 없이 발생하는 문제\n파이프라인의 장점\n예제: 파이프라인 없는 경우 vs 있는 경우\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n# ❌ 잘못된 예: 파이프라인 없음\n# 전체 데이터 스케일링 (데이터 누수!)\nscaler_wrong = StandardScaler()\n\nX_scaled_wrong = scaler_wrong.fit_transform(X)  # 테스트 데이터 정보 유출!\n\n# 데이터 분할\nX_train_wrong, X_test_wrong, y_train, y_test = train_test_split(\n    X_scaled_wrong, y, test_size=0.2, random_state=42\n)\n\n# 모델 학습\nmodel_wrong = LogisticRegression(max_iter=1000)\nmodel_wrong.fit(X_train_wrong, y_train)\nacc_wrong = model_wrong.score(X_test_wrong, y_test)\n\nprint(\"=== 잘못된 방법 (데이터 누수) ===\")\nprint(f\"정확도: {acc_wrong:.4f}\")\nprint(\"⚠️ 과대평가 가능!\")\n\n# ✓ 올바른 예: 파이프라인 사용\nfrom sklearn.pipeline import Pipeline\n\npipe_correct = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression(max_iter=1000))\n])\n\n# 학습 (scaler는 X_train으로만 fit)\npipe_correct.fit(X_train.select_dtypes(include=[np.number]), y_train)\nacc_correct = pipe_correct.score(X_test.select_dtypes(include=[np.number]), y_test)\n\nprint(\"\\n=== 올바른 방법 (파이프라인) ===\")\nprint(f\"정확도: {acc_correct:.4f}\")\nprint(\"✓ 정확한 평가!\")\n\n=== 잘못된 방법 (데이터 누수) ===\n정확도: 1.0000\n⚠️ 과대평가 가능!\n\n=== 올바른 방법 (파이프라인) ===\n정확도: 0.3731\n✓ 정확한 평가!",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#파이프라인의-필요성",
    "href": "part3/09. 파이프라인 및 자동화.html#파이프라인의-필요성",
    "title": "27  파이프라인 및 자동화",
    "section": "",
    "text": "문제\n설명\n결과\n\n\n\n\n데이터 누수\n전처리 시 테스트 데이터 정보 유출\n과대평가된 성능\n\n\n전처리 불일치\n학습과 테스트에 다른 전처리 적용\n예측 오류\n\n\n재현 불가\n실험 단계와 순서 추적 안 됨\n결과 재현 실패\n\n\n코드 복잡성\n각 단계를 개별 관리\n유지보수 어려움\n\n\n교차 검증 오류\nCV 전에 전처리하여 데이터 누수\n부정확한 평가\n\n\n\n\n\n\n\n장점\n설명\n\n\n\n\n데이터 누수 방지\n각 폴드마다 독립적 전처리\n\n\n코드 단순화\n한 줄로 fit + predict\n\n\n재현 가능성\n전체 워크플로우 저장 가능\n\n\n자동화\n하이퍼파라미터 튜닝 통합\n\n\n프로덕션 배포\n전처리 + 모델을 하나로 저장",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#기본-파이프라인-구성",
    "href": "part3/09. 파이프라인 및 자동화.html#기본-파이프라인-구성",
    "title": "27  파이프라인 및 자동화",
    "section": "27.2 기본 파이프라인 구성",
    "text": "27.2 기본 파이프라인 구성\n파이프라인은 순차적으로 실행되는 단계들의 리스트이다.\n파이프라인 구조\nPipeline([\n    ('step1_name', Transformer1()),\n    ('step2_name', Transformer2()),\n    ...\n    ('final_step', Estimator())\n])\n파이프라인 실행 흐름\n\nfit() 호출 시:\n\nStep 1: fit_transform\nStep 2: fit_transform\n…\nFinal step: fit (마지막은 transform 안 함)\n\npredict() 호출 시:\n\nStep 1: transform\nStep 2: transform\n…\nFinal step: predict\n\n\n\n27.2.1 기본 파이프라인 실습\n예제: 간단한 파이프라인\n\n# 수치형 변수만 사용\nX_num = X.select_dtypes(include=[np.number])\nX_train_num, X_test_num, y_train, y_test = train_test_split(\n    X_num, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 파이프라인 구성\npipe_basic = Pipeline([\n    ('scaler', StandardScaler()),\n    ('model', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# 학습\npipe_basic.fit(X_train_num, y_train)\n\n# 평가\ntrain_score = pipe_basic.score(X_train_num, y_train)\ntest_score = pipe_basic.score(X_test_num, y_test)\n\nprint(\"=== 기본 파이프라인 성능 ===\")\nprint(f\"학습 정확도: {train_score:.4f}\")\nprint(f\"테스트 정확도: {test_score:.4f}\")\n\n# 파이프라인 단계 확인\nprint(\"\\n=== 파이프라인 단계 ===\")\nfor name, step in pipe_basic.named_steps.items():\n    print(f\"{name}: {step}\")\n\n=== 기본 파이프라인 성능 ===\n학습 정확도: 0.9962\n테스트 정확도: 1.0000\n\n=== 파이프라인 단계 ===\nscaler: StandardScaler()\nmodel: LogisticRegression(max_iter=1000, random_state=42)\n\n\n예제: 파이프라인 내부 접근\n\n# 특정 단계 접근\nscaler = pipe_basic.named_steps['scaler']\nmodel = pipe_basic.named_steps['model']\n\nprint(\"\\n=== Scaler 정보 ===\")\nprint(f\"평균: {scaler.mean_[:3]}\")\nprint(f\"표준편차: {scaler.scale_[:3]}\")\n\nprint(\"\\n=== 모델 정보 ===\")\nprint(f\"계수 shape: {model.coef_.shape}\")\nprint(f\"클래스: {model.classes_}\")\n\n\n=== Scaler 정보 ===\n평균: [ 43.98421053  17.22593985 201.30075188]\n표준편차: [ 5.47311738  1.97054534 14.01475322]\n\n=== 모델 정보 ===\n계수 shape: (3, 5)\n클래스: ['Adelie' 'Chinstrap' 'Gentoo']",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#columntransformer-혼합-데이터-처리",
    "href": "part3/09. 파이프라인 및 자동화.html#columntransformer-혼합-데이터-처리",
    "title": "27  파이프라인 및 자동화",
    "section": "27.3 ColumnTransformer: 혼합 데이터 처리",
    "text": "27.3 ColumnTransformer: 혼합 데이터 처리\n실제 데이터는 수치형과 범주형이 섞여 있어 각 타입별로 다른 전처리가 필요하다.\nColumnTransformer 개념\n\n컬럼별로 다른 변환 적용\n수치형: 스케일링\n범주형: 인코딩\n결과를 하나로 병합\n\n\n27.3.1 ColumnTransformer 실습\n예제: 수치형 + 범주형 전처리\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 컬럼 타입 정의\nnum_cols = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\ncat_cols = [\"sex\"]\n\n# ColumnTransformer 구성\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), num_cols),\n        ('cat', OneHotEncoder(drop='first'), cat_cols)  # drop='first'로 다중공선성 방지\n    ],\n    remainder='drop'  # 명시되지 않은 컬럼 제거\n)\n\n# 전체 파이프라인\npipe_full = Pipeline([\n    ('preprocessor', preprocessor),\n    ('model', LogisticRegression(max_iter=1000, random_state=42))\n])\n\n# 학습 및 평가\npipe_full.fit(X_train, y_train)\ntrain_score_full = pipe_full.score(X_train, y_train)\ntest_score_full = pipe_full.score(X_test, y_test)\n\nprint(\"=== ColumnTransformer 파이프라인 성능 ===\")\nprint(f\"학습 정확도: {train_score_full:.4f}\")\nprint(f\"테스트 정확도: {test_score_full:.4f}\")\n\n=== ColumnTransformer 파이프라인 성능 ===\n학습 정확도: 0.9925\n테스트 정확도: 1.0000\n\n\n예제: 변환된 특성 이름 확인\n\n# 변환 후 특성 이름\npreprocessor_fitted = pipe_full.named_steps['preprocessor']\n\n# 수치형 특성 이름\nnum_feature_names = num_cols\n\n# 범주형 특성 이름 (OneHotEncoder 후)\ncat_feature_names = preprocessor_fitted.named_transformers_['cat'].get_feature_names_out(cat_cols)\n\n# 전체 특성 이름\nall_feature_names = list(num_feature_names) + list(cat_feature_names)\n\nprint(\"\\n=== 변환 후 특성 이름 ===\")\nprint(f\"수치형 ({len(num_feature_names)}개): {num_feature_names}\")\nprint(f\"범주형 ({len(cat_feature_names)}개): {list(cat_feature_names)}\")\nprint(f\"전체 ({len(all_feature_names)}개): {all_feature_names}\")\n\n\n=== 변환 후 특성 이름 ===\n수치형 (4개): ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n범주형 (1개): ['sex_1']\n전체 (5개): ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex_1']",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#하이퍼파라미터-최적화",
    "href": "part3/09. 파이프라인 및 자동화.html#하이퍼파라미터-최적화",
    "title": "27  파이프라인 및 자동화",
    "section": "27.4 하이퍼파라미터 최적화",
    "text": "27.4 하이퍼파라미터 최적화\n모델 성능은 하이퍼파라미터 설정에 크게 좌우된다. 자동 탐색이 필수이다.\n하이퍼파라미터 탐색 방법 비교\n\n\n\n\n\n\n\n\n\n\n방법\n전략\n탐색 범위\n계산 비용\n사용 상황\n\n\n\n\n수동 조정\n사람이 직접\n제한적\n낮음\n소규모 실험\n\n\nGridSearchCV\n격자 탐색 (모든 조합)\n지정한 값만\n높음\n작은 파라미터 공간\n\n\nRandomizedSearchCV\n무작위 샘플링\n연속 분포 가능\n중간\n큰 파라미터 공간\n\n\nBayesian Optimization\n확률 모델 기반\n효율적 탐색\n중간\n복잡한 탐색",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#gridsearchcv-격자-탐색",
    "href": "part3/09. 파이프라인 및 자동화.html#gridsearchcv-격자-탐색",
    "title": "27  파이프라인 및 자동화",
    "section": "27.5 GridSearchCV: 격자 탐색",
    "text": "27.5 GridSearchCV: 격자 탐색\nGridSearchCV는 지정한 모든 파라미터 조합을 빠짐없이 탐색한다.\nGridSearchCV 특징\n\n완전 탐색 (Exhaustive Search)\n모든 조합 시도\n최적값 보장 (탐색 범위 내)\n계산 비용 높음\n\n\n27.5.1 GridSearchCV 실습\n예제: GridSearchCV로 최적 파라미터 찾기\n\nfrom sklearn.model_selection import GridSearchCV\n\n# 파라미터 그리드 정의\nparam_grid = {\n    'model__C': [0.01, 0.1, 1.0, 10.0],\n    'model__penalty': ['l2'],\n    'model__solver': ['lbfgs']\n}\n\n# GridSearchCV 설정\ngrid_search = GridSearchCV(\n    estimator=pipe_full,\n    param_grid=param_grid,\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,  # 병렬 처리\n    verbose=1,\n    return_train_score=True\n)\n\n# 탐색 실행\nprint(\"=== GridSearchCV 실행 중 ===\")\ngrid_search.fit(X_train, y_train)\n\n# 최적 파라미터\nprint(\"\\n=== GridSearchCV 결과 ===\")\nprint(f\"최적 파라미터: {grid_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {grid_search.best_score_:.4f}\")\nprint(f\"테스트 점수: {grid_search.score(X_test, y_test):.4f}\")\n\n# 탐색한 조합 수\nprint(f\"\\n탐색한 조합 수: {len(grid_search.cv_results_['params'])}\")\n\n=== GridSearchCV 실행 중 ===\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\n\n=== GridSearchCV 결과 ===\n최적 파라미터: {'model__C': 1.0, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n최적 교차 검증 점수: 0.9925\n테스트 점수: 1.0000\n\n탐색한 조합 수: 4\n\n\n예제: GridSearchCV 결과 시각화\n\n# 결과를 DataFrame으로 변환\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# C 값에 따른 성능\nplt.figure(figsize=(12, 5))\n\nfor penalty in ['l1', 'l2']:\n    mask = results_df['param_model__penalty'] == penalty\n    subset = results_df[mask].sort_values('param_model__C')\n    \n    plt.plot(subset['param_model__C'], subset['mean_test_score'], \n             marker='o', label=f'{penalty}', linewidth=2)\n    plt.fill_between(subset['param_model__C'],\n                     subset['mean_test_score'] - subset['std_test_score'],\n                     subset['mean_test_score'] + subset['std_test_score'],\n                     alpha=0.2)\n\nplt.xscale('log')\nplt.xlabel('C (Regularization)')\nplt.ylabel('Cross-Validation Accuracy')\nplt.title('GridSearchCV: C vs Accuracy')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 상위 5개 조합 확인\n\n# 성능 상위 5개 조합\ntop5 = results_df.nsmallest(5, 'rank_test_score')[\n    ['params', 'mean_test_score', 'std_test_score', 'rank_test_score']\n]\n\nprint(\"\\n=== 성능 상위 5개 조합 ===\")\nfor idx, row in top5.iterrows():\n    print(f\"순위 {int(row['rank_test_score'])}: {row['params']}\")\n    print(f\"  평균 점수: {row['mean_test_score']:.4f} ± {row['std_test_score']:.4f}\\n\")\n\n\n=== 성능 상위 5개 조합 ===\n순위 1: {'model__C': 1.0, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n  평균 점수: 0.9925 ± 0.0092\n\n순위 1: {'model__C': 10.0, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n  평균 점수: 0.9925 ± 0.0092\n\n순위 3: {'model__C': 0.1, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n  평균 점수: 0.9811 ± 0.0119\n\n순위 4: {'model__C': 0.01, 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n  평균 점수: 0.9099 ± 0.0318",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#randomizedsearchcv-무작위-탐색",
    "href": "part3/09. 파이프라인 및 자동화.html#randomizedsearchcv-무작위-탐색",
    "title": "27  파이프라인 및 자동화",
    "section": "27.6 RandomizedSearchCV: 무작위 탐색",
    "text": "27.6 RandomizedSearchCV: 무작위 탐색\nRandomizedSearchCV는 파라미터 공간에서 무작위로 샘플링하여 효율적으로 탐색한다.\nRandomizedSearchCV 특징\n\n무작위 샘플링\nn_iter로 탐색 횟수 제어\n연속 분포 사용 가능\nGridSearchCV보다 빠름\n\n\n27.6.1 RandomizedSearchCV 실습\n예제: RandomizedSearchCV\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform, loguniform\n\n# 파라미터 분포 정의\nparam_distributions = {\n    'model__C': loguniform(1e-3, 1e2),  # 로그 스케일\n    'model__penalty': ['l2'],\n    'model__solver': ['lbfgs']\n}\n\n# RandomizedSearchCV 설정\nrandom_search = RandomizedSearchCV(\n    estimator=pipe_full,\n    param_distributions=param_distributions,\n    n_iter=20,  # 20번 무작위 샘플링\n    cv=5,\n    scoring='accuracy',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42,\n    return_train_score=True\n)\n\n# 탐색 실행\nprint(\"=== RandomizedSearchCV 실행 중 ===\")\nrandom_search.fit(X_train, y_train)\n\n# 결과\nprint(\"\\n=== RandomizedSearchCV 결과 ===\")\nprint(f\"최적 파라미터: {random_search.best_params_}\")\nprint(f\"최적 교차 검증 점수: {random_search.best_score_:.4f}\")\nprint(f\"테스트 점수: {random_search.score(X_test, y_test):.4f}\")\n\n# Grid vs Random 비교\nprint(\"\\n=== GridSearchCV vs RandomizedSearchCV ===\")\nprint(f\"Grid - 탐색 조합: {len(grid_search.cv_results_['params'])}, 최고 점수: {grid_search.best_score_:.4f}\")\nprint(f\"Random - 탐색 조합: {len(random_search.cv_results_['params'])}, 최고 점수: {random_search.best_score_:.4f}\")\n\n=== RandomizedSearchCV 실행 중 ===\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\n\n=== RandomizedSearchCV 결과 ===\n최적 파라미터: {'model__C': np.float64(4.5705630998014515), 'model__penalty': 'l2', 'model__solver': 'lbfgs'}\n최적 교차 검증 점수: 0.9925\n테스트 점수: 1.0000\n\n=== GridSearchCV vs RandomizedSearchCV ===\nGrid - 탐색 조합: 4, 최고 점수: 0.9925\nRandom - 탐색 조합: 20, 최고 점수: 0.9925",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#교차-검증과-파이프라인의-결합",
    "href": "part3/09. 파이프라인 및 자동화.html#교차-검증과-파이프라인의-결합",
    "title": "27  파이프라인 및 자동화",
    "section": "27.7 교차 검증과 파이프라인의 결합",
    "text": "27.7 교차 검증과 파이프라인의 결합\n교차 검증 시 파이프라인을 사용하면 각 폴드마다 독립적으로 전처리가 수행되어 데이터 누수를 완전히 차단한다.\n올바른 교차 검증 흐름\n각 Fold에서:\n1. 학습 데이터로 전처리 fit\n2. 학습 데이터 transform\n3. 검증 데이터 transform (학습 데이터의 파라미터 사용)\n4. 모델 학습\n5. 검증 데이터로 평가\n예제: 교차 검증 with Pipeline\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\n\n# 교차 검증\ncv_scores = cross_val_score(\n    pipe_full, X, y, \n    cv=5, \n    scoring='accuracy'\n)\n\nprint(\"=== 교차 검증 결과 ===\")\nprint(f\"각 폴드 점수: {cv_scores}\")\nprint(f\"평균 점수: {cv_scores.mean():.4f}\")\nprint(f\"표준편차: {cv_scores.std():.4f}\")\n\n# 여러 지표로 평가\nscoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\ncv_results = cross_validate(\n    pipe_full, X, y,\n    cv=5,\n    scoring=scoring,\n    return_train_score=True\n)\n\nprint(\"\\n=== 다중 지표 교차 검증 ===\")\nfor metric in scoring:\n    test_scores = cv_results[f'test_{metric}']\n    print(f\"{metric:20s}: {test_scores.mean():.4f} ± {test_scores.std():.4f}\")\n\n=== 교차 검증 결과 ===\n각 폴드 점수: [1.         0.98507463 0.98507463 1.         1.        ]\n평균 점수: 0.9940\n표준편차: 0.0073\n\n=== 다중 지표 교차 검증 ===\naccuracy            : 0.9940 ± 0.0073\nprecision_macro     : 0.9956 ± 0.0054\nrecall_macro        : 0.9905 ± 0.0117\nf1_macro            : 0.9928 ± 0.0088",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#완전한-머신러닝-워크플로우",
    "href": "part3/09. 파이프라인 및 자동화.html#완전한-머신러닝-워크플로우",
    "title": "27  파이프라인 및 자동화",
    "section": "27.8 완전한 머신러닝 워크플로우",
    "text": "27.8 완전한 머신러닝 워크플로우\n예제: 전체 워크플로우 통합\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 1. 파이프라인 정의\npipe_workflow = Pipeline([\n    ('preprocessor', ColumnTransformer([\n        ('num', StandardScaler(), num_cols),\n        ('cat', OneHotEncoder(drop='first'), cat_cols)\n    ])),\n    ('model', RandomForestClassifier(random_state=42))\n])\n\n# 2. 하이퍼파라미터 그리드\nparam_grid_rf = {\n    'model__n_estimators': [50, 100, 200],\n    'model__max_depth': [5, 10, None],\n    'model__min_samples_split': [2, 5]\n}\n\n# 3. GridSearchCV\ngrid_rf = GridSearchCV(\n    pipe_workflow,\n    param_grid_rf,\n    cv=5,\n    scoring='f1_macro',\n    n_jobs=-1,\n    verbose=1\n)\n\n# 4. 학습\nprint(\"=== 전체 워크플로우 실행 ===\")\ngrid_rf.fit(X_train, y_train)\n\n# 5. 최종 평가\nprint(\"\\n=== 최종 결과 ===\")\nprint(f\"최적 파라미터: {grid_rf.best_params_}\")\nprint(f\"교차 검증 F1 (macro): {grid_rf.best_score_:.4f}\")\nprint(f\"테스트 F1 (macro): \", end=\"\")\n\nfrom sklearn.metrics import f1_score\ny_pred_final = grid_rf.predict(X_test)\ntest_f1 = f1_score(y_test, y_pred_final, average='macro')\nprint(f\"{test_f1:.4f}\")\n\n=== 전체 워크플로우 실행 ===\nFitting 5 folds for each of 18 candidates, totalling 90 fits\n\n=== 최종 결과 ===\n최적 파라미터: {'model__max_depth': 5, 'model__min_samples_split': 2, 'model__n_estimators': 50}\n교차 검증 F1 (macro): 0.9816\n테스트 F1 (macro): 0.9827",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#모델-저장-및-로드",
    "href": "part3/09. 파이프라인 및 자동화.html#모델-저장-및-로드",
    "title": "27  파이프라인 및 자동화",
    "section": "27.9 모델 저장 및 로드",
    "text": "27.9 모델 저장 및 로드\n학습된 파이프라인은 저장하여 재사용할 수 있다.\n예제: 모델 저장 및 로드\n\nimport joblib\n\n# 모델 저장\nmodel_filename = 'best_pipeline.pkl'\njoblib.dump(grid_rf.best_estimator_, model_filename)\nprint(f\"\\n모델 저장 완료: {model_filename}\")\n\n# 모델 로드\nloaded_pipeline = joblib.load(model_filename)\n\n# 로드한 모델로 예측\ny_pred_loaded = loaded_pipeline.predict(X_test)\naccuracy_loaded = (y_pred_loaded == y_test).mean()\n\nprint(f\"\\n=== 로드한 모델 성능 ===\")\nprint(f\"정확도: {accuracy_loaded:.4f}\")\nprint(\"✓ 저장 전과 동일한 결과\")\n\n\n모델 저장 완료: best_pipeline.pkl\n\n=== 로드한 모델 성능 ===\n정확도: 0.9851\n✓ 저장 전과 동일한 결과",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#실무-권장사항",
    "href": "part3/09. 파이프라인 및 자동화.html#실무-권장사항",
    "title": "27  파이프라인 및 자동화",
    "section": "27.10 실무 권장사항",
    "text": "27.10 실무 권장사항\n파이프라인 사용 원칙\n\n\n\n원칙\n설명\n이유\n\n\n\n\n항상 파이프라인 사용\n모든 실험을 파이프라인으로\n데이터 누수 방지\n\n\n전처리 통합\n전처리를 파이프라인 안에\n일관성 보장\n\n\nCV 내부 전처리\n교차 검증 전에 fit 안 함\n독립성 유지\n\n\n재현성 확보\nrandom_state 고정\n실험 재현 가능\n\n\n적절한 scoring\n문제에 맞는 지표\n올바른 최적화\n\n\n\n하이퍼파라미터 탐색 가이드\n\n\n\n상황\n방법\n이유\n\n\n\n\n파라미터 공간 작음\nGridSearchCV\n완전 탐색\n\n\n파라미터 공간 큼\nRandomizedSearchCV\n효율성\n\n\n시간 제약 있음\nRandomizedSearchCV (적은 n_iter)\n속도\n\n\n연속 값 탐색\nRandomizedSearchCV (분포 사용)\n유연성",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#체크리스트",
    "href": "part3/09. 파이프라인 및 자동화.html#체크리스트",
    "title": "27  파이프라인 및 자동화",
    "section": "27.11 체크리스트",
    "text": "27.11 체크리스트\n파이프라인 및 자동화 체크리스트\n\n파이프라인에 모든 전처리 포함\nColumnTransformer로 수치형/범주형 분리\n교차 검증 전에 fit 안 함 (파이프라인 사용)\n적절한 scoring 지표 선택\nrandom_state 고정 (재현성)\n하이퍼파라미터 탐색 수행\n테스트 데이터는 마지막에만 사용\n최종 모델 저장",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/09. 파이프라인 및 자동화.html#요약",
    "href": "part3/09. 파이프라인 및 자동화.html#요약",
    "title": "27  파이프라인 및 자동화",
    "section": "27.12 요약",
    "text": "27.12 요약\n이 장에서는 머신러닝 워크플로우를 자동화하는 파이프라인과 하이퍼파라미터 최적화를 학습했다. 주요 내용은 다음과 같다.\n파이프라인 핵심\n\n목적: 데이터 누수 방지, 코드 단순화, 재현성\n구조: 전처리 → 모델 순차 실행\nColumnTransformer: 수치형/범주형 별도 처리\n교차 검증: 각 폴드마다 독립적 전처리\n\n하이퍼파라미터 최적화\n\nGridSearchCV: 모든 조합 탐색 (작은 공간)\nRandomizedSearchCV: 무작위 샘플링 (큰 공간)\n교차 검증 통합: 과적합 방지\nscoring: 문제에 맞는 지표 선택\n\n실무 워크플로우\n\n파이프라인 정의 (전처리 + 모델)\n하이퍼파라미터 그리드 설정\nGridSearchCV/RandomizedSearchCV 실행\n최적 모델 선택\n테스트 데이터로 최종 평가\n모델 저장\n\n파이프라인과 자동화는 신뢰할 수 있고 재현 가능한 머신러닝 프로젝트의 필수 요소이다. 모든 실험을 파이프라인으로 구성하고, 체계적인 하이퍼파라미터 탐색을 통해 최적의 모델을 찾는 것이 중요하다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>파이프라인 및 자동화</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html",
    "href": "part3/10. 모델 해석.html",
    "title": "28  모델 해석",
    "section": "",
    "text": "28.1 모델 해석의 필요성\n모델 해석(Model Interpretation)은 머신러닝 모델이 어떻게 예측하는지 이해하고 설명하는 과정이다. 모델이 복잡해질수록 블랙박스처럼 작동하지만, 실무에서는 “어떤 변수가 중요한가?”, “왜 이런 예측이 나왔는가?”, “모델이 편향되지 않았는가?”를 반드시 답해야 한다. 모델 해석은 선택이 아닌 필수이며, 신뢰성 있는 의사결정을 위해 반드시 필요하다. 이 장에서는 특성 중요도, Permutation Importance, SHAP, PDP 등 주요 모델 해석 기법을 학습한다.\n예제: 데이터 로드\n모델 해석은 단순한 학술적 관심이 아니라 실무적 필수 요소이다.\n모델 해석이 필요한 이유\n해석 가능성 vs 성능",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#모델-해석의-필요성",
    "href": "part3/10. 모델 해석.html#모델-해석의-필요성",
    "title": "28  모델 해석",
    "section": "",
    "text": "이유\n설명\n예시\n\n\n\n\n신뢰성 확보\n모델 결정 근거 제시\n의료 진단, 대출 심사\n\n\n편향 탐지\n불공정한 예측 발견\n성별/인종 차별 방지\n\n\n디버깅\n모델 오류 원인 파악\n잘못된 변수 사용 발견\n\n\n규제 준수\n설명 가능성 요구 충족\nGDPR, 금융 규제\n\n\n도메인 지식 결합\n전문가 검증 가능\n의학적 타당성 확인\n\n\n개선 방향\n모델 성능 향상 힌트\n중요 변수 추가 수집\n\n\n\n\n\n\n\n모델\n해석 가능성\n성능\n사용 상황\n\n\n\n\n선형 회귀\n매우 높음\n낮음\n규제 산업, 보고서\n\n\n결정 트리\n높음\n중간\n비즈니스 룰\n\n\n랜덤 포레스트\n중간\n높음\n일반적 상황\n\n\nXGBoost\n낮음\n매우 높음\n경진대회\n\n\n딥러닝\n매우 낮음\n매우 높음\n이미지, 텍스트",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#모델-해석의-범주",
    "href": "part3/10. 모델 해석.html#모델-해석의-범주",
    "title": "28  모델 해석",
    "section": "28.2 모델 해석의 범주",
    "text": "28.2 모델 해석의 범주\n모델 해석은 전역적 해석과 국소적 해석으로 나뉜다.\n전역 해석 vs 국소 해석\n\n\n\n\n\n\n\n\n구분\n전역 해석 (Global)\n국소 해석 (Local)\n\n\n\n\n질문\n모델이 전반적으로 무엇을 중요하게 보는가?\n이 특정 예측은 왜 이렇게 나왔는가?\n\n\n대상\n전체 데이터셋\n개별 샘플\n\n\n방법\nFeature Importance, PDP\nSHAP Force Plot, LIME\n\n\n용도\n모델 이해, 변수 선택\n개별 결정 설명",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#특성-중요도-feature-importance",
    "href": "part3/10. 모델 해석.html#특성-중요도-feature-importance",
    "title": "28  모델 해석",
    "section": "28.3 특성 중요도 (Feature Importance)",
    "text": "28.3 특성 중요도 (Feature Importance)\n특성 중요도는 모델이 예측에 각 변수를 얼마나 활용했는지 수치로 표현한 것이다.\n\n28.3.1 트리 기반 모델의 특성 중요도\n트리 기반 모델(Decision Tree, Random Forest, XGBoost)은 내장 중요도를 제공한다.\n예제: 모델 학습 및 특성 중요도\n\n# RandomForest 학습\nrf = RandomForestClassifier(n_estimators=200, random_state=42)\nrf.fit(X_train, y_train)\n\n# 특성 중요도\nfeature_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': rf.feature_importances_\n}).sort_values('Importance', ascending=False)\n\nprint(\"=== RandomForest 특성 중요도 ===\")\nprint(feature_importance)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.barh(feature_importance['Feature'], feature_importance['Importance'])\nplt.xlabel('Importance')\nplt.title('Feature Importance (Random Forest)')\nplt.tight_layout()\nplt.show()\n\n=== RandomForest 특성 중요도 ===\n             Feature  Importance\n0     bill_length_mm    0.415594\n2  flipper_length_mm    0.330553\n1      bill_depth_mm    0.168720\n3        body_mass_g    0.085133\n\n\n\n\n\n\n\n\n\n특성 중요도 계산 원리\n\n불순도 감소: 각 특성이 분기 시 불순도를 얼마나 감소시켰는가\n가중 평균: 모든 트리에서의 기여도 평균\n정규화: 합이 1이 되도록 조정\n\n해석 시 주의사항\n\n\n\n주의점\n설명\n\n\n\n\n인과관계 아님\n상관관계일 뿐, 인과관계 아님\n\n\n상관변수 왜곡\n상관성 높은 변수들 사이에서 중요도 분산\n\n\n편향 가능\n카디널리티 높은 변수 선호\n\n\n학습 데이터 의존\n테스트 성능과 무관",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#permutation-importance",
    "href": "part3/10. 모델 해석.html#permutation-importance",
    "title": "28  모델 해석",
    "section": "28.4 Permutation Importance",
    "text": "28.4 Permutation Importance\nPermutation Importance는 특정 변수를 무작위로 섞었을 때 모델 성능이 얼마나 감소하는지로 중요도를 측정한다.\nPermutation Importance 장점\n\n모델 종류 무관 (모든 모델에 적용 가능)\n실제 성능 기반 (과적합 변수 탐지)\n상관변수 영향 적음\n\n작동 원리\n\n기준 성능 측정 (원본 데이터)\n특성 하나를 무작위로 섞기\n성능 재측정\n성능 감소량 = 중요도\n여러 번 반복하여 평균\n\n\n28.4.1 Permutation Importance 실습\n예제: Permutation Importance 계산\n\nfrom sklearn.inspection import permutation_importance\n\n# Permutation Importance (테스트 데이터 기준)\nresult = permutation_importance(\n    rf, X_test, y_test,\n    n_repeats=10,\n    random_state=42,\n    n_jobs=-1\n)\n\n# 결과 정리\nperm_importance = pd.DataFrame({\n    'Feature': X.columns,\n    'Importance': result.importances_mean,\n    'Std': result.importances_std\n}).sort_values('Importance', ascending=False)\n\nprint(\"\\n=== Permutation Importance ===\")\nprint(perm_importance)\n\n# 시각화\nplt.figure(figsize=(10, 6))\nplt.barh(perm_importance['Feature'], perm_importance['Importance'], \n         xerr=perm_importance['Std'])\nplt.xlabel('Importance (Decrease in Accuracy)')\nplt.title('Permutation Importance')\nplt.tight_layout()\nplt.show()\n\n\n=== Permutation Importance ===\n             Feature  Importance       Std\n0     bill_length_mm    0.295522  0.060737\n2  flipper_length_mm    0.240299  0.039742\n1      bill_depth_mm    0.091045  0.023552\n3        body_mass_g    0.014925  0.017660\n\n\n\n\n\n\n\n\n\n예제: Feature Importance vs Permutation Importance 비교\n\n# 두 방법 비교\ncomparison = pd.DataFrame({\n    'Feature': X.columns,\n    'Built-in': rf.feature_importances_,\n    'Permutation': result.importances_mean\n})\n\n# 순위 비교\ncomparison['Built-in Rank'] = comparison['Built-in'].rank(ascending=False)\ncomparison['Permutation Rank'] = comparison['Permutation'].rank(ascending=False)\n\nprint(\"\\n=== 중요도 비교 ===\")\nprint(comparison.sort_values('Permutation', ascending=False))\n\n# 시각화\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].barh(comparison['Feature'], comparison['Built-in'])\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Built-in Feature Importance')\n\naxes[1].barh(comparison['Feature'], comparison['Permutation'])\naxes[1].set_xlabel('Importance')\naxes[1].set_title('Permutation Importance')\n\nplt.tight_layout()\nplt.show()\n\n\n=== 중요도 비교 ===\n             Feature  Built-in  Permutation  Built-in Rank  Permutation Rank\n0     bill_length_mm  0.415594     0.295522            1.0               1.0\n2  flipper_length_mm  0.330553     0.240299            2.0               2.0\n1      bill_depth_mm  0.168720     0.091045            3.0               3.0\n3        body_mass_g  0.085133     0.014925            4.0               4.0",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#partial-dependence-plot-pdp",
    "href": "part3/10. 모델 해석.html#partial-dependence-plot-pdp",
    "title": "28  모델 해석",
    "section": "28.5 Partial Dependence Plot (PDP)",
    "text": "28.5 Partial Dependence Plot (PDP)\nPDP는 특정 변수가 변할 때 예측값이 평균적으로 어떻게 변하는지 시각화한다.\nPDP의 특징\n\n변수의 영향 방향(+/−) 확인\n전역적 경향 파악\n비선형 관계 시각화\n다른 변수는 고정\n\n\n28.5.1 PDP 실습\n예제: Partial Dependence Plot\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\n# PDP 생성\nfeatures = ['flipper_length_mm', 'body_mass_g']\nfig, ax = plt.subplots(figsize=(14, 5))\n\nPartialDependenceDisplay.from_estimator(\n    rf, X_train, features,\n    target='Adelie',\n    ax=ax,\n    n_jobs=-1\n)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n예제: 2D PDP (변수 간 상호작용)\n\n# 2D PDP로 상호작용 확인\nfig, ax = plt.subplots(figsize=(10, 8))\n\nPartialDependenceDisplay.from_estimator(\n    rf, X_train,\n    features=[(0, 2)],  # bill_length_mm와 flipper_length_mm\n    target='Adelie',\n    ax=ax,\n    n_jobs=-1\n)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nPDP 해석\n\n상승 패턴: 변수 증가 → 예측값 증가\n하강 패턴: 변수 증가 → 예측값 감소\n평탄: 변수가 예측에 거의 영향 없음\n비선형: 특정 구간에서만 영향",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#shap-shapley-additive-explanations",
    "href": "part3/10. 모델 해석.html#shap-shapley-additive-explanations",
    "title": "28  모델 해석",
    "section": "28.6 SHAP (SHapley Additive exPlanations)",
    "text": "28.6 SHAP (SHapley Additive exPlanations)\nSHAP는 게임 이론의 Shapley Value를 기반으로 각 특성의 기여도를 계산하는 현재 가장 표준적인 모델 해석 방법이다.\nSHAP의 특징\n\n\n\n특징\n설명\n\n\n\n\n이론적 보장\n공정한 기여도 계산 (Shapley Value)\n\n\n전역 + 국소\n모델 전체와 개별 예측 모두 설명\n\n\n방향성\n양수(증가)/음수(감소) 기여도\n\n\n일관성\n모든 예측의 합 = 최종 예측\n\n\n모델 독립적\n다양한 모델에 적용 가능\n\n\n\nSHAP 값의 핵심 개념\n최종 예측 = 기준값(base value) + Σ(각 특성의 SHAP 값)\n\n28.6.1 SHAP 실습 (Python 3.6-3.10)\nSHAP는 Python 3.6~3.10에서 지원된다. Python 3.11+ 환경에서는 Permutation Importance + PDP 조합을 사용한다.\n예제: SHAP 기본 사용 (Python 3.6-3.10)\n# SHAP 설치 필요: pip install shap\ntry:\n    import shap\n    \n    # TreeExplainer (트리 기반 모델용)\n    explainer = shap.TreeExplainer(rf)\n    shap_values = explainer.shap_values(X_test)\n    \n    # Summary Plot (전역 해석)\n    print(\"=== SHAP Summary Plot ===\")\n    shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n    \n    # 상세 Summary Plot (분포 포함)\n    shap.summary_plot(shap_values, X_test)\n    \n    # Force Plot (개별 예측 해석)\n    sample_idx = 0\n    shap.force_plot(\n        explainer.expected_value[0],\n        shap_values[0][sample_idx],\n        X_test.iloc[sample_idx]\n    )\n    \nexcept ImportError:\n    print(\"SHAP이 설치되지 않았거나 Python 버전이 호환되지 않습니다.\")\n    print(\"대신 Permutation Importance + PDP를 사용하세요.\")\nSHAP Summary Plot 해석\n\n막대형 (bar): 전체 모델에서 중요한 변수 순위\n분포형 (beeswarm):\n\n색상: 특성 값 크기 (빨강=높음, 파랑=낮음)\nX축: SHAP 값 (양수=증가 기여, 음수=감소 기여)\nY축: 특성",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#모델-해석-방법-종합-비교",
    "href": "part3/10. 모델 해석.html#모델-해석-방법-종합-비교",
    "title": "28  모델 해석",
    "section": "28.7 모델 해석 방법 종합 비교",
    "text": "28.7 모델 해석 방법 종합 비교\n해석 방법 비교\n\n\n\n방법\n전역/국소\n방향성\n모델 독립\n계산 비용\n장점\n단점\n\n\n\n\nFeature Importance\n전역\n✗\n트리만\n낮음\n빠름\n편향 가능\n\n\nPermutation Importance\n전역\n✗\n✓\n중간\n신뢰\n느림\n\n\nPDP\n전역\n✓\n✓\n중간\n직관적\n상호작용 미반영\n\n\nSHAP\n전역+국소\n✓\n✓\n높음\n이론적 보장\n매우 느림\n\n\n\n상황별 권장 방법\n\n\n\n상황\n권장 방법\n이유\n\n\n\n\n빠른 탐색\nFeature Importance\n즉시 확인 가능\n\n\n신뢰할 수 있는 전역 해석\nPermutation Importance\n실제 성능 기반\n\n\n영향 방향 확인\nPDP\n비선형 관계 파악\n\n\n개별 예측 설명\nSHAP\n완전한 설명\n\n\n보고서/프레젠테이션\nPDP + SHAP\n시각적으로 명확",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#실무-활용-예제",
    "href": "part3/10. 모델 해석.html#실무-활용-예제",
    "title": "28  모델 해석",
    "section": "28.8 실무 활용 예제",
    "text": "28.8 실무 활용 예제\n예제: 종합 해석 리포트\n\n# 1. Permutation Importance로 중요 변수 파악\nprint(\"=== 1. 중요 변수 파악 (Permutation Importance) ===\")\nprint(perm_importance.head(3))\n\n# 2. PDP로 상위 2개 변수 영향 확인\nprint(\"\\n=== 2. 변수 영향 방향 확인 (PDP) ===\")\ntop2_features = perm_importance.head(2)['Feature'].tolist()\n\nfig, ax = plt.subplots(figsize=(14, 5))\nPartialDependenceDisplay.from_estimator(\n    rf, X_train, top2_features, target='Adelie', ax=ax\n)\nplt.suptitle('Top 2 Features: Partial Dependence')\nplt.tight_layout()\nplt.show()\n\n# 3. 모델 성능\nfrom sklearn.metrics import accuracy_score, classification_report\n\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"\\n=== 3. 모델 성능 ===\")\nprint(f\"정확도: {accuracy:.4f}\")\nprint(\"\\n분류 리포트:\")\nprint(classification_report(y_test, y_pred))\n\n=== 1. 중요 변수 파악 (Permutation Importance) ===\n             Feature  Importance       Std\n0     bill_length_mm    0.295522  0.060737\n2  flipper_length_mm    0.240299  0.039742\n1      bill_depth_mm    0.091045  0.023552\n\n=== 2. 변수 영향 방향 확인 (PDP) ===\n\n\n\n\n\n\n\n\n\n\n=== 3. 모델 성능 ===\n정확도: 0.9701\n\n분류 리포트:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      0.97      0.98        29\n   Chinstrap       0.88      1.00      0.93        14\n      Gentoo       1.00      0.96      0.98        24\n\n    accuracy                           0.97        67\n   macro avg       0.96      0.97      0.96        67\nweighted avg       0.97      0.97      0.97        67",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#모델-해석-체크리스트",
    "href": "part3/10. 모델 해석.html#모델-해석-체크리스트",
    "title": "28  모델 해석",
    "section": "28.9 모델 해석 체크리스트",
    "text": "28.9 모델 해석 체크리스트\n모델 해석 수행 시 확인사항\n\nPermutation Importance로 중요 변수 확인\nPDP로 상위 변수 영향 방향 파악\n도메인 전문가와 결과 검토\n예상치 못한 중요 변수 원인 분석\n편향 가능성 검토\n개별 예측 설명 (필요 시 SHAP)\n시각화 자료 준비 (보고서용)\n결과 문서화",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  },
  {
    "objectID": "part3/10. 모델 해석.html#요약",
    "href": "part3/10. 모델 해석.html#요약",
    "title": "28  모델 해석",
    "section": "28.10 요약",
    "text": "28.10 요약\n이 장에서는 머신러닝 모델을 해석하는 다양한 방법을 학습했다. 주요 내용은 다음과 같다.\n핵심 원칙\n\n모델 성능만으로 부족: 해석이 필수\n전역 + 국소: 전체 이해 + 개별 설명\n여러 방법 조합: 한 가지 방법만 사용 금지\n도메인 지식 결합: 통계적 중요도 + 실무 타당성\n\n주요 방법 요약\n\n\n\n목적\n방법\n출력\n\n\n\n\n어떤 변수가 중요한가?\nPermutation Importance\n중요도 순위\n\n\n어떻게 영향을 주는가?\nPDP\n영향 방향 및 형태\n\n\n이 예측은 왜?\nSHAP Force Plot\n개별 기여도\n\n\n전체적으로 어떻게?\nSHAP Summary\n전역 기여도 분포\n\n\n\n실무 권장 워크플로우\n\n모델 학습 및 평가\nPermutation Importance: 중요 변수 3-5개 선정\nPDP: 상위 변수의 영향 방향 확인\n도메인 검토: 전문가와 타당성 논의\nSHAP (선택): 개별 예측 설명 필요 시\n문서화: 시각화 + 해석 정리\n\n모델 해석은 머신러닝 프로젝트의 완성도를 높이고 신뢰성을 확보하는 필수 과정이다. 성능 높은 블랙박스 모델보다 설명 가능한 모델이 실무에서 더 가치 있다는 점을 명심해야 한다.",
    "crumbs": [
      "Ⅲ. 머신러닝 모델링 및 평가",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>모델 해석</span>"
    ]
  }
]