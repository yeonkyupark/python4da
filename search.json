[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "파이썬 데이터 분석",
    "section": "",
    "text": "들어가기\n기계학습으로 데이터를 분석한다. 분석 도구는 파이썬으로 데이터 전처리, 통계 분석, 기계학습 모델링 및 평가까지 코드 중심으로 구성된다. 실무에 필요한 내용을 정리하여 이론적, 학문적으로 미흡한 부분이 있다. 부족한 부분은 향후 성능 개선과 향상된 알고리즘 등으로 수정키로 한다.\n파이썬 버전은 3.12 기반으로 numpy, pandas, sklearn, scipy, stats와 같은 기본적인 라이브러리를 사용한다.\n전체 목차는 다음과 같다.",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-데이터-전처리-분포-이해",
    "href": "index.html#부.-데이터-전처리-분포-이해",
    "title": "파이썬 데이터 분석",
    "section": "1부. 데이터 전처리 & 분포 이해",
    "text": "1부. 데이터 전처리 & 분포 이해\n\n1.1 데이터 로드 및 구조 점검\n\n데이터 로드 (CSV, Excel, SQL)\n데이터 구조 및 타입 확인\n\n\n\n1.2 탐색적 데이터 분석(EDA)\n\n기술통계량 요약\n분포 시각화 (히스토그램, KDE, 박스플롯)\n상관관계 탐색\n\n\n\n1.3 데이터 분포 이해\n\n연속형·이산형 데이터 분포\n왜도와 첨도\n분포 해석을 통한 전처리 전략\n\n\n\n1.4 결측치 처리\n\n결측치 탐지 및 요약\n단순 대치 기법\n고급 대치 기법 (KNN, Iterative)\n\n\n\n1.5 이상치 탐지\n\n정규분포 기반 이상치\nIQR 기반 이상치\n밀도 기반 이상치 (LOF, DBSCAN)\n트리 기반 이상치 (IsolationForest)\n\n\n\n1.6 스케일링\n\n정규화 (Min-Max)\n표준화 (Standard, Robust, MaxAbs)\n\n\n\n1.7 데이터 분포 변환\n\n로그 변환\nBox-Cox 변환\nYeo-Johnson 변환\n분위수 변환\n변환 전·후 분포 비교\n\n\n\n1.8 범주형 데이터 처리\n\n명목형 인코딩\n순서형 인코딩\n\n\n\n1.9 연속형 데이터 범주화\n\n구간 분할 (cut, qcut, KBins)\n\n\n\n1.10 불균형 데이터 처리\n\n오버샘플링\n언더샘플링\n\n\n\n1.11 피처 엔지니어링\n\n다항 특성 생성\n집계 및 롤링 특성",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "href": "index.html#부.-통계-기반-데이터-분석-가설검정",
    "title": "파이썬 데이터 분석",
    "section": "2부. 통계 기반 데이터 분석 & 가설검정",
    "text": "2부. 통계 기반 데이터 분석 & 가설검정\n\n2.1 확률분포와 표본\n\n연속형 확률분포\n이산형 확률분포\n표본 분포 개념\n\n\n\n2.2 정규성 검정\n\nShapiro-Wilk 검정\nKolmogorov-Smirnov 검정\nAnderson-Darling 검정\nQ-Q plot 해석\n\n\n\n2.3 등분산성 검정\n\nLevene 검정\nBartlett 검정\nFligner-Killeen 검정\n\n\n\n2.4 적합성 검정 & 독립성 검정\n\n카이제곱 적합성 검정\n분포 적합성 검정\n카이제곱 독립성 검정\nF 검정 (분산 비교)\n\n\n\n2.5 평균 비교 검정\n\n단일 표본 t-검정\n독립 표본 t-검정\n대응 표본 t-검정\n\n\n\n2.6 분산분석\n\n일원 분산 분석 (One-way ANOVA)\n이원 분산 분석 (Two-way ANOVA)\n사후 검정\n\n\n\n2.7 비모수 검정\n\nMann-Whitney U 검정\nWilcoxon 순위합 검정\nKruskal-Wallis 검정\nFriedman 검정\n\n\n\n2.8 상관 분석\n\nPearson 상관 분석\nSpearman 순위 상관",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "index.html#부.-머신러닝-모델링-평가",
    "href": "index.html#부.-머신러닝-모델링-평가",
    "title": "파이썬 데이터 분석",
    "section": "3부. 머신러닝 모델링 & 평가",
    "text": "3부. 머신러닝 모델링 & 평가\n\n3.1 데이터 분할 및 검증\n\n학습·검증·테스트 분할\n교차 검증 기법\n\n\n\n3.2 특성 선택\n\n필터 방법\n래퍼 방법\n임베디드 방법\n\n\n\n3.3 차원 축소\n\n선형 차원 축소 (PCA)\n비선형 차원 축소 (t-SNE, UMAP)\n\n\n\n3.4 회귀 모델\n\n선형 회귀\n정규화 회귀 (Ridge, Lasso, ElasticNet)\n\n\n\n3.5 분류 모델\n\n선형 분류 모델\n거리 기반 모델\n트리 및 앙상블 모델\n\n\n\n3.6 서포트 벡터 머신\n\nSVM 분류\n\n\n\n3.7 군집 분석\n\n분할 기반 군집\n밀도 기반 군집\n혼합 모델 군집\n\n\n\n3.8 모델 성능 평가\n\n분류 성능 평가\n회귀 성능 평가\n\n\n\n3.9 파이프라인 & 자동화\n\n파이프라인 구성\n하이퍼파라미터 최적화\n\n\n\n3.10 모델 해석\n\n특성 중요도\nSHAP 기반 모델 해석",
    "crumbs": [
      "들어가기"
    ]
  },
  {
    "objectID": "part1/데이터 로드 및 구조 점검.html",
    "href": "part1/데이터 로드 및 구조 점검.html",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1 데이터 로드\n데이터 분석에 있어 가장 먼저 하는 작업이 데이터를 수집하는 것이다. 데이터 수집 방법에는 여러 가지가 있을 수 있고 대부분 로컬 파일이나 데이터베이스 또는 다른 시스템의 산출물에서 확보한다. 또한 데이터 원천에서 수집되는 데이터 형태는 시스템이나 상황에 따라 다양할 수 있지만 일반적으로 CSV나 Excel 형태로 처리하게 된다. 물론 API나 데이터베이스에 직접 Query해서 취합할 수도 있다.\n가장 일반적인 자료 형태인 CSV 파일과 Excel 파일을 메모리에 적재하는 방법을 알아 본다. 아래는 예제로 사용할 palmerpenguins 데이터셋이다.\nimport pandas as pd\nfrom palmerpenguins import load_penguins\n\ndf = load_penguins()\n\ndf.head()\n\ndf.to_csv(\"penguins.csv\", index=False) # CSV 파일로 저장\ndf.to_excel(\"./penguins.xlsx\", index=False) # Excel 파일로 저장\n\n# sqlite DB\nimport sqlite3\nconn = sqlite3.connect(\"penguins.db\")\nresult = df.to_sql(\"penguins\", conn, if_exists=\"replace\", index=False)",
    "crumbs": [
      "데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/데이터 로드 및 구조 점검.html#데이터-로드",
    "href": "part1/데이터 로드 및 구조 점검.html#데이터-로드",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "",
    "text": "1.1.1 CSV 파일 로드\n\nimport pandas as pd\n\ndf_from_csv = pd.read_csv(\"penguins.csv\")\n\ndf_from_csv.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.2 Excel 파일 로드\n\nimport pandas as pd\n\ndf_from_excel = pd.read_excel(\"penguins.xlsx\")\n\ndf_from_excel.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n1.1.3 SQLite 데이터베이스 로드\n\nimport sqlite3\n\nconn = sqlite3.connect(\"penguins.db\")\n\nquery = \"SELECT * FROM penguins\"\ndf_from_sql = pd.read_sql(query, conn)\n\ndf_from_sql.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007",
    "crumbs": [
      "데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  },
  {
    "objectID": "part1/데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "href": "part1/데이터 로드 및 구조 점검.html#데이터-구조-및-타입-확인",
    "title": "1  데이터 로드 및 구조 점검",
    "section": "1.2 데이터 구조 및 타입 확인",
    "text": "1.2 데이터 구조 및 타입 확인\n수집된 데이터에서 가장 먼저 확인하는 것은 데이터의 크기와 컬럼에 대한 정보이다.\n\n1.2.1 데이터 크기와 기본 정보\n\nimport pandas as pd\n\ndf = pd.read_csv(\"penguins.csv\")\n\ndf.shape\n\n(344, 8)\n\n\n\ndf.info()\n\n&lt;class 'pandas.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 8 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   species            344 non-null    str    \n 1   island             344 non-null    str    \n 2   bill_length_mm     342 non-null    float64\n 3   bill_depth_mm      342 non-null    float64\n 4   flipper_length_mm  342 non-null    float64\n 5   body_mass_g        342 non-null    float64\n 6   sex                333 non-null    str    \n 7   year               344 non-null    int64  \ndtypes: float64(4), int64(1), str(3)\nmemory usage: 21.6 KB",
    "crumbs": [
      "데이터 전처리 및 변환",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>데이터 로드 및 구조 점검</span>"
    ]
  }
]